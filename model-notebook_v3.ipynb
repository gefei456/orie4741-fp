{
 "cells": [
  {
   "cell_type": "code",
   "id": "6e504281",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:16.379257Z",
     "start_time": "2025-02-04T03:21:16.332672Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "import joblib"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "id": "a341b52b",
   "metadata": {},
   "source": [
    "## 0. DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "id": "e9469f20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:18.330555Z",
     "start_time": "2025-02-04T03:21:18.180064Z"
    }
   },
   "source": [
    "football_df = pd.read_csv('data/all_data_with_elo.csv', low_memory = False)\n",
    "football_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Unnamed: 0  Div       Date       HomeTeam       AwayTeam  FTHG  FTAG  \\\n",
       "0               0   D1   2003/8/1  Bayern Munich  Ein Frankfurt   3.0   1.0   \n",
       "1               1   F1   2003/8/1          Lille           Lyon   1.0   0.0   \n",
       "2               2   F1   2003/8/2        Auxerre           Nice   1.0   2.0   \n",
       "3               3   F1   2003/8/2       Guingamp      Marseille   0.0   1.0   \n",
       "4               4   D1   2003/8/2        Hamburg       Hannover   0.0   3.0   \n",
       "...           ...  ...        ...            ...            ...   ...   ...   \n",
       "38428       38428   I1  2025/1/26        Udinese           Roma   1.0   2.0   \n",
       "38429       38429  SP1  2025/1/26      Vallecano         Girona   2.0   1.0   \n",
       "38430       38430  SP1  2025/1/27         Alaves          Celta   1.0   1.0   \n",
       "38431       38431   I1  2025/1/27          Genoa          Monza   2.0   0.0   \n",
       "38432       38432   I1  2025/1/27        Venezia         Verona   1.0   1.0   \n",
       "\n",
       "      FTR    HS    AS  ...  WHD    WHA   AHh  B365AHH  B365AHA  AHCh  \\\n",
       "0       H  17.0   6.0  ...  4.5  10.00   NaN      NaN      NaN   NaN   \n",
       "1       H   NaN   NaN  ...  3.0   2.20   NaN      NaN      NaN   NaN   \n",
       "2       A   NaN   NaN  ...  3.1   5.00 -0.75    2.050    1.850   NaN   \n",
       "3       A   NaN   NaN  ...  3.1   2.40  0.00    1.925    1.975   NaN   \n",
       "4       A  10.0  16.0  ...  3.5   5.00 -0.75    1.800    2.100   NaN   \n",
       "...    ..   ...   ...  ...  ...    ...   ...      ...      ...   ...   \n",
       "38428   A  10.0  16.0  ...  3.2   2.20  0.25    1.990    1.940  0.25   \n",
       "38429   H  17.0  12.0  ...  3.0   2.80  0.00    1.910    1.990  0.00   \n",
       "38430   D   3.0   5.0  ...  3.2   2.75  0.00    1.890    2.010 -0.25   \n",
       "38431   H  18.0   5.0  ...  3.0   3.40 -0.25    1.950    1.980 -0.25   \n",
       "38432   D   9.0  21.0  ...  3.3   3.25 -0.25    1.920    2.010 -0.25   \n",
       "\n",
       "       B365CAHH  B365CAHA  HomeTeamELO  AwayTeamELO  \n",
       "0           NaN       NaN  1859.379272  1593.249268  \n",
       "1           NaN       NaN  1612.968018  1726.539795  \n",
       "2           NaN       NaN  1702.604858  1611.196045  \n",
       "3           NaN       NaN  1685.016113  1665.625732  \n",
       "4           NaN       NaN  1718.566284  1649.805298  \n",
       "...         ...       ...          ...          ...  \n",
       "38428      1.73      2.08  1667.065796  1772.048584  \n",
       "38429      1.82      2.11  1635.277222  1704.093872  \n",
       "38430      1.98      1.95  1621.804321  1637.579956  \n",
       "38431      1.98      1.95  1682.297119  1628.680542  \n",
       "38432      1.98      1.95  1575.471802  1615.984619  \n",
       "\n",
       "[38433 rows x 29 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Div</th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HS</th>\n",
       "      <th>AS</th>\n",
       "      <th>...</th>\n",
       "      <th>WHD</th>\n",
       "      <th>WHA</th>\n",
       "      <th>AHh</th>\n",
       "      <th>B365AHH</th>\n",
       "      <th>B365AHA</th>\n",
       "      <th>AHCh</th>\n",
       "      <th>B365CAHH</th>\n",
       "      <th>B365CAHA</th>\n",
       "      <th>HomeTeamELO</th>\n",
       "      <th>AwayTeamELO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>D1</td>\n",
       "      <td>2003/8/1</td>\n",
       "      <td>Bayern Munich</td>\n",
       "      <td>Ein Frankfurt</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>10.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1859.379272</td>\n",
       "      <td>1593.249268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003/8/1</td>\n",
       "      <td>Lille</td>\n",
       "      <td>Lyon</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1612.968018</td>\n",
       "      <td>1726.539795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003/8/2</td>\n",
       "      <td>Auxerre</td>\n",
       "      <td>Nice</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>2.050</td>\n",
       "      <td>1.850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1702.604858</td>\n",
       "      <td>1611.196045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003/8/2</td>\n",
       "      <td>Guingamp</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.925</td>\n",
       "      <td>1.975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1685.016113</td>\n",
       "      <td>1665.625732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>D1</td>\n",
       "      <td>2003/8/2</td>\n",
       "      <td>Hamburg</td>\n",
       "      <td>Hannover</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.800</td>\n",
       "      <td>2.100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1718.566284</td>\n",
       "      <td>1649.805298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38428</th>\n",
       "      <td>38428</td>\n",
       "      <td>I1</td>\n",
       "      <td>2025/1/26</td>\n",
       "      <td>Udinese</td>\n",
       "      <td>Roma</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.990</td>\n",
       "      <td>1.940</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1667.065796</td>\n",
       "      <td>1772.048584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38429</th>\n",
       "      <td>38429</td>\n",
       "      <td>SP1</td>\n",
       "      <td>2025/1/26</td>\n",
       "      <td>Vallecano</td>\n",
       "      <td>Girona</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.910</td>\n",
       "      <td>1.990</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.82</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1635.277222</td>\n",
       "      <td>1704.093872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38430</th>\n",
       "      <td>38430</td>\n",
       "      <td>SP1</td>\n",
       "      <td>2025/1/27</td>\n",
       "      <td>Alaves</td>\n",
       "      <td>Celta</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.890</td>\n",
       "      <td>2.010</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1621.804321</td>\n",
       "      <td>1637.579956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38431</th>\n",
       "      <td>38431</td>\n",
       "      <td>I1</td>\n",
       "      <td>2025/1/27</td>\n",
       "      <td>Genoa</td>\n",
       "      <td>Monza</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.950</td>\n",
       "      <td>1.980</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1682.297119</td>\n",
       "      <td>1628.680542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38432</th>\n",
       "      <td>38432</td>\n",
       "      <td>I1</td>\n",
       "      <td>2025/1/27</td>\n",
       "      <td>Venezia</td>\n",
       "      <td>Verona</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.920</td>\n",
       "      <td>2.010</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1575.471802</td>\n",
       "      <td>1615.984619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38433 rows × 29 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:18.392547Z",
     "start_time": "2025-02-04T03:21:18.332065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 提前6个月，用来划分赛季\n",
    "# 将字符串转换为日期类型，指定日期格式\n",
    "football_df['Date'] = pd.to_datetime(football_df['Date'], format='%Y/%m/%d')\n",
    "football_df['Date'] = football_df['Date'] - pd.DateOffset(months=6)\n",
    "football_df"
   ],
   "id": "91c6f576d106bfec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Unnamed: 0  Div       Date       HomeTeam       AwayTeam  FTHG  FTAG  \\\n",
       "0               0   D1 2003-02-01  Bayern Munich  Ein Frankfurt   3.0   1.0   \n",
       "1               1   F1 2003-02-01          Lille           Lyon   1.0   0.0   \n",
       "2               2   F1 2003-02-02        Auxerre           Nice   1.0   2.0   \n",
       "3               3   F1 2003-02-02       Guingamp      Marseille   0.0   1.0   \n",
       "4               4   D1 2003-02-02        Hamburg       Hannover   0.0   3.0   \n",
       "...           ...  ...        ...            ...            ...   ...   ...   \n",
       "38428       38428   I1 2024-07-26        Udinese           Roma   1.0   2.0   \n",
       "38429       38429  SP1 2024-07-26      Vallecano         Girona   2.0   1.0   \n",
       "38430       38430  SP1 2024-07-27         Alaves          Celta   1.0   1.0   \n",
       "38431       38431   I1 2024-07-27          Genoa          Monza   2.0   0.0   \n",
       "38432       38432   I1 2024-07-27        Venezia         Verona   1.0   1.0   \n",
       "\n",
       "      FTR    HS    AS  ...  WHD    WHA   AHh  B365AHH  B365AHA  AHCh  \\\n",
       "0       H  17.0   6.0  ...  4.5  10.00   NaN      NaN      NaN   NaN   \n",
       "1       H   NaN   NaN  ...  3.0   2.20   NaN      NaN      NaN   NaN   \n",
       "2       A   NaN   NaN  ...  3.1   5.00 -0.75    2.050    1.850   NaN   \n",
       "3       A   NaN   NaN  ...  3.1   2.40  0.00    1.925    1.975   NaN   \n",
       "4       A  10.0  16.0  ...  3.5   5.00 -0.75    1.800    2.100   NaN   \n",
       "...    ..   ...   ...  ...  ...    ...   ...      ...      ...   ...   \n",
       "38428   A  10.0  16.0  ...  3.2   2.20  0.25    1.990    1.940  0.25   \n",
       "38429   H  17.0  12.0  ...  3.0   2.80  0.00    1.910    1.990  0.00   \n",
       "38430   D   3.0   5.0  ...  3.2   2.75  0.00    1.890    2.010 -0.25   \n",
       "38431   H  18.0   5.0  ...  3.0   3.40 -0.25    1.950    1.980 -0.25   \n",
       "38432   D   9.0  21.0  ...  3.3   3.25 -0.25    1.920    2.010 -0.25   \n",
       "\n",
       "       B365CAHH  B365CAHA  HomeTeamELO  AwayTeamELO  \n",
       "0           NaN       NaN  1859.379272  1593.249268  \n",
       "1           NaN       NaN  1612.968018  1726.539795  \n",
       "2           NaN       NaN  1702.604858  1611.196045  \n",
       "3           NaN       NaN  1685.016113  1665.625732  \n",
       "4           NaN       NaN  1718.566284  1649.805298  \n",
       "...         ...       ...          ...          ...  \n",
       "38428      1.73      2.08  1667.065796  1772.048584  \n",
       "38429      1.82      2.11  1635.277222  1704.093872  \n",
       "38430      1.98      1.95  1621.804321  1637.579956  \n",
       "38431      1.98      1.95  1682.297119  1628.680542  \n",
       "38432      1.98      1.95  1575.471802  1615.984619  \n",
       "\n",
       "[38433 rows x 29 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Div</th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HS</th>\n",
       "      <th>AS</th>\n",
       "      <th>...</th>\n",
       "      <th>WHD</th>\n",
       "      <th>WHA</th>\n",
       "      <th>AHh</th>\n",
       "      <th>B365AHH</th>\n",
       "      <th>B365AHA</th>\n",
       "      <th>AHCh</th>\n",
       "      <th>B365CAHH</th>\n",
       "      <th>B365CAHA</th>\n",
       "      <th>HomeTeamELO</th>\n",
       "      <th>AwayTeamELO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>D1</td>\n",
       "      <td>2003-02-01</td>\n",
       "      <td>Bayern Munich</td>\n",
       "      <td>Ein Frankfurt</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>10.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1859.379272</td>\n",
       "      <td>1593.249268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-01</td>\n",
       "      <td>Lille</td>\n",
       "      <td>Lyon</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1612.968018</td>\n",
       "      <td>1726.539795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Auxerre</td>\n",
       "      <td>Nice</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>2.050</td>\n",
       "      <td>1.850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1702.604858</td>\n",
       "      <td>1611.196045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Guingamp</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.925</td>\n",
       "      <td>1.975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1685.016113</td>\n",
       "      <td>1665.625732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>D1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Hamburg</td>\n",
       "      <td>Hannover</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.800</td>\n",
       "      <td>2.100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1718.566284</td>\n",
       "      <td>1649.805298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38428</th>\n",
       "      <td>38428</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Udinese</td>\n",
       "      <td>Roma</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.990</td>\n",
       "      <td>1.940</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1667.065796</td>\n",
       "      <td>1772.048584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38429</th>\n",
       "      <td>38429</td>\n",
       "      <td>SP1</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Vallecano</td>\n",
       "      <td>Girona</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.910</td>\n",
       "      <td>1.990</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.82</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1635.277222</td>\n",
       "      <td>1704.093872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38430</th>\n",
       "      <td>38430</td>\n",
       "      <td>SP1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Alaves</td>\n",
       "      <td>Celta</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.890</td>\n",
       "      <td>2.010</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1621.804321</td>\n",
       "      <td>1637.579956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38431</th>\n",
       "      <td>38431</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Genoa</td>\n",
       "      <td>Monza</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.950</td>\n",
       "      <td>1.980</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1682.297119</td>\n",
       "      <td>1628.680542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38432</th>\n",
       "      <td>38432</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Venezia</td>\n",
       "      <td>Verona</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.920</td>\n",
       "      <td>2.010</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1575.471802</td>\n",
       "      <td>1615.984619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38433 rows × 29 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "id": "fcbbc3ce",
   "metadata": {},
   "source": [
    "## 1. Descriptive Statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b828a5",
   "metadata": {},
   "source": [
    "**1.1 DataFrame Shape**"
   ]
  },
  {
   "cell_type": "code",
   "id": "15427373",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:18.518941Z",
     "start_time": "2025-02-04T03:21:18.469782Z"
    }
   },
   "source": [
    "# no. rows and no. cols\n",
    "football_df.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38433, 29)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "id": "7b649b14",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:18.635117Z",
     "start_time": "2025-02-04T03:21:18.579519Z"
    }
   },
   "source": [
    "# feature names\n",
    "print(football_df.columns.tolist())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'Div', 'Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HS', 'AS', 'HST', 'AST', 'B365H', 'B365D', 'B365A', 'IWH', 'IWD', 'IWA', 'WHH', 'WHD', 'WHA', 'AHh', 'B365AHH', 'B365AHA', 'AHCh', 'B365CAHH', 'B365CAHA', 'HomeTeamELO', 'AwayTeamELO']\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "id": "fdb560ee",
   "metadata": {},
   "source": [
    "**1.2 NaN Values**"
   ]
  },
  {
   "cell_type": "code",
   "id": "d3377302",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:18.851059Z",
     "start_time": "2025-02-04T03:21:18.803321Z"
    }
   },
   "source": [
    "football_df.isnull().sum()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0         0\n",
       "Div                0\n",
       "Date               0\n",
       "HomeTeam           0\n",
       "AwayTeam           0\n",
       "FTHG               0\n",
       "FTAG               0\n",
       "FTR                0\n",
       "HS              1762\n",
       "AS              1762\n",
       "HST             2568\n",
       "AST             2568\n",
       "B365H             49\n",
       "B365D             49\n",
       "B365A             49\n",
       "IWH             1976\n",
       "IWD             1976\n",
       "IWA             1976\n",
       "WHH              573\n",
       "WHD              573\n",
       "WHA              573\n",
       "AHh              262\n",
       "B365AHH          276\n",
       "B365AHA          276\n",
       "AHCh           28479\n",
       "B365CAHH       28481\n",
       "B365CAHA       28481\n",
       "HomeTeamELO      125\n",
       "AwayTeamELO      126\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "id": "7eae5438",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:19.172600Z",
     "start_time": "2025-02-04T03:21:19.128158Z"
    }
   },
   "source": [
    "# total elements in \n",
    "football_df.size"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1114557"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "id": "2afbc469",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:19.493719Z",
     "start_time": "2025-02-04T03:21:19.443993Z"
    }
   },
   "source": [
    "# total number of NaN\n",
    "football_df.size - football_df.count().sum()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102960"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "id": "a9b3446d",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:19.711114Z",
     "start_time": "2025-02-04T03:21:19.653240Z"
    }
   },
   "source": [
    "# total number of NaN rows\n",
    "football_df.isnull().any(axis = 1).sum()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30386"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "id": "12fcf2ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:19.836880Z",
     "start_time": "2025-02-04T03:21:19.791920Z"
    }
   },
   "source": [
    "# total number of NaN columns\n",
    "football_df.isnull().any(axis = 0).sum()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "id": "da2853e2",
   "metadata": {},
   "source": [
    "## 2. Data Wrangling and Feature Transformation/Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cde42d8",
   "metadata": {},
   "source": [
    "### 2.1 NaN Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a34fc35",
   "metadata": {},
   "source": [
    "`TODO`: drop NaN values along columns: {Date, Home Team, Away Team, FTR} <br>\n",
    "`TODO`: identify betting odds w/ most available data"
   ]
  },
  {
   "cell_type": "code",
   "id": "7f75f7d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:19.927980Z",
     "start_time": "2025-02-04T03:21:19.897449Z"
    }
   },
   "source": [
    "# 当前方法仅提取这几个字段 分区 日期 主队 客队 full-time-result 三家机构的胜平负 主队ELO评分 客队ELO评分\n",
    "# nan_mask = ['Div', 'Date', 'HomeTeam', 'AwayTeam', 'FTR', 'B365H', 'B365D', 'B365A', \n",
    "#             'IWH', 'IWD', 'IWA', 'WHH', 'WHD', 'WHA', 'AHh', 'B365AHH', 'B365AHA', 'HomeTeamELO', 'AwayTeamELO']\n",
    "nan_mask = ['Div', 'Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'B365H', 'B365D', 'B365A', 'WHH', 'WHD', 'WHA', 'AHh', 'B365AHH', 'B365AHA', 'HomeTeamELO', 'AwayTeamELO']"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:20.151904Z",
     "start_time": "2025-02-04T03:21:20.079034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nan_football_df = football_df.dropna(subset = nan_mask)\n",
    "nan_football_df"
   ],
   "id": "23e46d7ed335b538",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Unnamed: 0  Div       Date   HomeTeam       AwayTeam  FTHG  FTAG FTR  \\\n",
       "2               2   F1 2003-02-02    Auxerre           Nice   1.0   2.0   A   \n",
       "3               3   F1 2003-02-02   Guingamp      Marseille   0.0   1.0   A   \n",
       "4               4   D1 2003-02-02    Hamburg       Hannover   0.0   3.0   A   \n",
       "5               5   D1 2003-02-02     Hertha  Werder Bremen   0.0   3.0   A   \n",
       "7               7   F1 2003-02-02       Lens        Le Mans   0.0   0.0   D   \n",
       "...           ...  ...        ...        ...            ...   ...   ...  ..   \n",
       "38428       38428   I1 2024-07-26    Udinese           Roma   1.0   2.0   A   \n",
       "38429       38429  SP1 2024-07-26  Vallecano         Girona   2.0   1.0   H   \n",
       "38430       38430  SP1 2024-07-27     Alaves          Celta   1.0   1.0   D   \n",
       "38431       38431   I1 2024-07-27      Genoa          Monza   2.0   0.0   H   \n",
       "38432       38432   I1 2024-07-27    Venezia         Verona   1.0   1.0   D   \n",
       "\n",
       "         HS    AS  ...  WHD   WHA   AHh  B365AHH  B365AHA  AHCh  B365CAHH  \\\n",
       "2       NaN   NaN  ...  3.1  5.00 -0.75    2.050    1.850   NaN       NaN   \n",
       "3       NaN   NaN  ...  3.1  2.40  0.00    1.925    1.975   NaN       NaN   \n",
       "4      10.0  16.0  ...  3.5  5.00 -0.75    1.800    2.100   NaN       NaN   \n",
       "5      23.0  19.0  ...  3.4  4.00 -0.75    2.025    1.875   NaN       NaN   \n",
       "7       NaN   NaN  ...  3.3  4.50 -0.75    1.900    2.000   NaN       NaN   \n",
       "...     ...   ...  ...  ...   ...   ...      ...      ...   ...       ...   \n",
       "38428  10.0  16.0  ...  3.2  2.20  0.25    1.990    1.940  0.25      1.73   \n",
       "38429  17.0  12.0  ...  3.0  2.80  0.00    1.910    1.990  0.00      1.82   \n",
       "38430   3.0   5.0  ...  3.2  2.75  0.00    1.890    2.010 -0.25      1.98   \n",
       "38431  18.0   5.0  ...  3.0  3.40 -0.25    1.950    1.980 -0.25      1.98   \n",
       "38432   9.0  21.0  ...  3.3  3.25 -0.25    1.920    2.010 -0.25      1.98   \n",
       "\n",
       "       B365CAHA  HomeTeamELO  AwayTeamELO  \n",
       "2           NaN  1702.604858  1611.196045  \n",
       "3           NaN  1685.016113  1665.625732  \n",
       "4           NaN  1718.566284  1649.805298  \n",
       "5           NaN  1719.916748  1692.120972  \n",
       "7           NaN  1697.354004  1539.958130  \n",
       "...         ...          ...          ...  \n",
       "38428      2.08  1667.065796  1772.048584  \n",
       "38429      2.11  1635.277222  1704.093872  \n",
       "38430      1.95  1621.804321  1637.579956  \n",
       "38431      1.95  1682.297119  1628.680542  \n",
       "38432      1.95  1575.471802  1615.984619  \n",
       "\n",
       "[37346 rows x 29 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Div</th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HS</th>\n",
       "      <th>AS</th>\n",
       "      <th>...</th>\n",
       "      <th>WHD</th>\n",
       "      <th>WHA</th>\n",
       "      <th>AHh</th>\n",
       "      <th>B365AHH</th>\n",
       "      <th>B365AHA</th>\n",
       "      <th>AHCh</th>\n",
       "      <th>B365CAHH</th>\n",
       "      <th>B365CAHA</th>\n",
       "      <th>HomeTeamELO</th>\n",
       "      <th>AwayTeamELO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Auxerre</td>\n",
       "      <td>Nice</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>2.050</td>\n",
       "      <td>1.850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1702.604858</td>\n",
       "      <td>1611.196045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Guingamp</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.925</td>\n",
       "      <td>1.975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1685.016113</td>\n",
       "      <td>1665.625732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>D1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Hamburg</td>\n",
       "      <td>Hannover</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.800</td>\n",
       "      <td>2.100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1718.566284</td>\n",
       "      <td>1649.805298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>D1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Hertha</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>23.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.00</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>2.025</td>\n",
       "      <td>1.875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1719.916748</td>\n",
       "      <td>1692.120972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Lens</td>\n",
       "      <td>Le Mans</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.50</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.900</td>\n",
       "      <td>2.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1697.354004</td>\n",
       "      <td>1539.958130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38428</th>\n",
       "      <td>38428</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Udinese</td>\n",
       "      <td>Roma</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.990</td>\n",
       "      <td>1.940</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1667.065796</td>\n",
       "      <td>1772.048584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38429</th>\n",
       "      <td>38429</td>\n",
       "      <td>SP1</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Vallecano</td>\n",
       "      <td>Girona</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.910</td>\n",
       "      <td>1.990</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.82</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1635.277222</td>\n",
       "      <td>1704.093872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38430</th>\n",
       "      <td>38430</td>\n",
       "      <td>SP1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Alaves</td>\n",
       "      <td>Celta</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.890</td>\n",
       "      <td>2.010</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1621.804321</td>\n",
       "      <td>1637.579956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38431</th>\n",
       "      <td>38431</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Genoa</td>\n",
       "      <td>Monza</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.950</td>\n",
       "      <td>1.980</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1682.297119</td>\n",
       "      <td>1628.680542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38432</th>\n",
       "      <td>38432</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Venezia</td>\n",
       "      <td>Verona</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.920</td>\n",
       "      <td>2.010</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1575.471802</td>\n",
       "      <td>1615.984619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37346 rows × 29 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:20.339934Z",
     "start_time": "2025-02-04T03:21:20.276604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conditions = [\n",
    "    nan_football_df['AHh'] >= 2.5,         # AHh >= 2.75\n",
    "    (nan_football_df['AHh'] >= 1.5) & (nan_football_df['AHh'] <= 2.25),  # 1.75 < AHh <= 2.75\n",
    "    (nan_football_df['AHh'] >= 0.25) & (nan_football_df['AHh'] <= 1.25),  # 0.25 < AHh <= 1.75\n",
    "    nan_football_df['AHh'] == 0,            # AHh == 0\n",
    "    (nan_football_df['AHh'] >= -1.25) & (nan_football_df['AHh'] <= -0.25),  # -1.75 < AHh <= -0.25\n",
    "    (nan_football_df['AHh'] >= -2.25) & (nan_football_df['AHh'] <= -1.5),  # -2.75 < AHh <= -1.75\n",
    "    nan_football_df['AHh'] <= -2.5\n",
    "]\n",
    "# easy_conditions = [\n",
    "#     nan_football_df['AHh'] <= -0.25,\n",
    "#     nan_football_df['AHh'] == 0,\n",
    "#     nan_football_df['AHh'] >= 0.25,\n",
    "# ]\n",
    "labels = [3, 2, 1, 0, -1, -2, -3]\n",
    "# easy_labels = [-1, 0, 1]\n",
    "\n",
    "nan_football_df['balance_val'] = np.select(conditions, labels)\n",
    "nan_football_df"
   ],
   "id": "b9dbb5d1679a1e6b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Unnamed: 0  Div       Date   HomeTeam       AwayTeam  FTHG  FTAG FTR  \\\n",
       "2               2   F1 2003-02-02    Auxerre           Nice   1.0   2.0   A   \n",
       "3               3   F1 2003-02-02   Guingamp      Marseille   0.0   1.0   A   \n",
       "4               4   D1 2003-02-02    Hamburg       Hannover   0.0   3.0   A   \n",
       "5               5   D1 2003-02-02     Hertha  Werder Bremen   0.0   3.0   A   \n",
       "7               7   F1 2003-02-02       Lens        Le Mans   0.0   0.0   D   \n",
       "...           ...  ...        ...        ...            ...   ...   ...  ..   \n",
       "38428       38428   I1 2024-07-26    Udinese           Roma   1.0   2.0   A   \n",
       "38429       38429  SP1 2024-07-26  Vallecano         Girona   2.0   1.0   H   \n",
       "38430       38430  SP1 2024-07-27     Alaves          Celta   1.0   1.0   D   \n",
       "38431       38431   I1 2024-07-27      Genoa          Monza   2.0   0.0   H   \n",
       "38432       38432   I1 2024-07-27    Venezia         Verona   1.0   1.0   D   \n",
       "\n",
       "         HS    AS  ...   WHA   AHh  B365AHH  B365AHA  AHCh  B365CAHH  \\\n",
       "2       NaN   NaN  ...  5.00 -0.75    2.050    1.850   NaN       NaN   \n",
       "3       NaN   NaN  ...  2.40  0.00    1.925    1.975   NaN       NaN   \n",
       "4      10.0  16.0  ...  5.00 -0.75    1.800    2.100   NaN       NaN   \n",
       "5      23.0  19.0  ...  4.00 -0.75    2.025    1.875   NaN       NaN   \n",
       "7       NaN   NaN  ...  4.50 -0.75    1.900    2.000   NaN       NaN   \n",
       "...     ...   ...  ...   ...   ...      ...      ...   ...       ...   \n",
       "38428  10.0  16.0  ...  2.20  0.25    1.990    1.940  0.25      1.73   \n",
       "38429  17.0  12.0  ...  2.80  0.00    1.910    1.990  0.00      1.82   \n",
       "38430   3.0   5.0  ...  2.75  0.00    1.890    2.010 -0.25      1.98   \n",
       "38431  18.0   5.0  ...  3.40 -0.25    1.950    1.980 -0.25      1.98   \n",
       "38432   9.0  21.0  ...  3.25 -0.25    1.920    2.010 -0.25      1.98   \n",
       "\n",
       "       B365CAHA  HomeTeamELO  AwayTeamELO  balance_val  \n",
       "2           NaN  1702.604858  1611.196045           -1  \n",
       "3           NaN  1685.016113  1665.625732            0  \n",
       "4           NaN  1718.566284  1649.805298           -1  \n",
       "5           NaN  1719.916748  1692.120972           -1  \n",
       "7           NaN  1697.354004  1539.958130           -1  \n",
       "...         ...          ...          ...          ...  \n",
       "38428      2.08  1667.065796  1772.048584            1  \n",
       "38429      2.11  1635.277222  1704.093872            0  \n",
       "38430      1.95  1621.804321  1637.579956            0  \n",
       "38431      1.95  1682.297119  1628.680542           -1  \n",
       "38432      1.95  1575.471802  1615.984619           -1  \n",
       "\n",
       "[37346 rows x 30 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Div</th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HS</th>\n",
       "      <th>AS</th>\n",
       "      <th>...</th>\n",
       "      <th>WHA</th>\n",
       "      <th>AHh</th>\n",
       "      <th>B365AHH</th>\n",
       "      <th>B365AHA</th>\n",
       "      <th>AHCh</th>\n",
       "      <th>B365CAHH</th>\n",
       "      <th>B365CAHA</th>\n",
       "      <th>HomeTeamELO</th>\n",
       "      <th>AwayTeamELO</th>\n",
       "      <th>balance_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Auxerre</td>\n",
       "      <td>Nice</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>2.050</td>\n",
       "      <td>1.850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1702.604858</td>\n",
       "      <td>1611.196045</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Guingamp</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.925</td>\n",
       "      <td>1.975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1685.016113</td>\n",
       "      <td>1665.625732</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>D1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Hamburg</td>\n",
       "      <td>Hannover</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.800</td>\n",
       "      <td>2.100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1718.566284</td>\n",
       "      <td>1649.805298</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>D1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Hertha</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>23.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.00</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>2.025</td>\n",
       "      <td>1.875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1719.916748</td>\n",
       "      <td>1692.120972</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Lens</td>\n",
       "      <td>Le Mans</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.50</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.900</td>\n",
       "      <td>2.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1697.354004</td>\n",
       "      <td>1539.958130</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38428</th>\n",
       "      <td>38428</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Udinese</td>\n",
       "      <td>Roma</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.990</td>\n",
       "      <td>1.940</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1667.065796</td>\n",
       "      <td>1772.048584</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38429</th>\n",
       "      <td>38429</td>\n",
       "      <td>SP1</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Vallecano</td>\n",
       "      <td>Girona</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.910</td>\n",
       "      <td>1.990</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.82</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1635.277222</td>\n",
       "      <td>1704.093872</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38430</th>\n",
       "      <td>38430</td>\n",
       "      <td>SP1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Alaves</td>\n",
       "      <td>Celta</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.890</td>\n",
       "      <td>2.010</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1621.804321</td>\n",
       "      <td>1637.579956</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38431</th>\n",
       "      <td>38431</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Genoa</td>\n",
       "      <td>Monza</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.40</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.950</td>\n",
       "      <td>1.980</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1682.297119</td>\n",
       "      <td>1628.680542</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38432</th>\n",
       "      <td>38432</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Venezia</td>\n",
       "      <td>Verona</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.920</td>\n",
       "      <td>2.010</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1575.471802</td>\n",
       "      <td>1615.984619</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37346 rows × 30 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:20.439078Z",
     "start_time": "2025-02-04T03:21:20.372980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nan_football_df['asia_final_result'] = nan_football_df['FTHG'] - nan_football_df['FTAG'] + nan_football_df['balance_val']\n",
    "nan_football_df"
   ],
   "id": "11c29f5924e5d7a1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Unnamed: 0  Div       Date   HomeTeam       AwayTeam  FTHG  FTAG FTR  \\\n",
       "2               2   F1 2003-02-02    Auxerre           Nice   1.0   2.0   A   \n",
       "3               3   F1 2003-02-02   Guingamp      Marseille   0.0   1.0   A   \n",
       "4               4   D1 2003-02-02    Hamburg       Hannover   0.0   3.0   A   \n",
       "5               5   D1 2003-02-02     Hertha  Werder Bremen   0.0   3.0   A   \n",
       "7               7   F1 2003-02-02       Lens        Le Mans   0.0   0.0   D   \n",
       "...           ...  ...        ...        ...            ...   ...   ...  ..   \n",
       "38428       38428   I1 2024-07-26    Udinese           Roma   1.0   2.0   A   \n",
       "38429       38429  SP1 2024-07-26  Vallecano         Girona   2.0   1.0   H   \n",
       "38430       38430  SP1 2024-07-27     Alaves          Celta   1.0   1.0   D   \n",
       "38431       38431   I1 2024-07-27      Genoa          Monza   2.0   0.0   H   \n",
       "38432       38432   I1 2024-07-27    Venezia         Verona   1.0   1.0   D   \n",
       "\n",
       "         HS    AS  ...   AHh  B365AHH  B365AHA  AHCh  B365CAHH  B365CAHA  \\\n",
       "2       NaN   NaN  ... -0.75    2.050    1.850   NaN       NaN       NaN   \n",
       "3       NaN   NaN  ...  0.00    1.925    1.975   NaN       NaN       NaN   \n",
       "4      10.0  16.0  ... -0.75    1.800    2.100   NaN       NaN       NaN   \n",
       "5      23.0  19.0  ... -0.75    2.025    1.875   NaN       NaN       NaN   \n",
       "7       NaN   NaN  ... -0.75    1.900    2.000   NaN       NaN       NaN   \n",
       "...     ...   ...  ...   ...      ...      ...   ...       ...       ...   \n",
       "38428  10.0  16.0  ...  0.25    1.990    1.940  0.25      1.73      2.08   \n",
       "38429  17.0  12.0  ...  0.00    1.910    1.990  0.00      1.82      2.11   \n",
       "38430   3.0   5.0  ...  0.00    1.890    2.010 -0.25      1.98      1.95   \n",
       "38431  18.0   5.0  ... -0.25    1.950    1.980 -0.25      1.98      1.95   \n",
       "38432   9.0  21.0  ... -0.25    1.920    2.010 -0.25      1.98      1.95   \n",
       "\n",
       "       HomeTeamELO  AwayTeamELO  balance_val  asia_final_result  \n",
       "2      1702.604858  1611.196045           -1               -2.0  \n",
       "3      1685.016113  1665.625732            0               -1.0  \n",
       "4      1718.566284  1649.805298           -1               -4.0  \n",
       "5      1719.916748  1692.120972           -1               -4.0  \n",
       "7      1697.354004  1539.958130           -1               -1.0  \n",
       "...            ...          ...          ...                ...  \n",
       "38428  1667.065796  1772.048584            1                0.0  \n",
       "38429  1635.277222  1704.093872            0                1.0  \n",
       "38430  1621.804321  1637.579956            0                0.0  \n",
       "38431  1682.297119  1628.680542           -1                1.0  \n",
       "38432  1575.471802  1615.984619           -1               -1.0  \n",
       "\n",
       "[37346 rows x 31 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Div</th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HS</th>\n",
       "      <th>AS</th>\n",
       "      <th>...</th>\n",
       "      <th>AHh</th>\n",
       "      <th>B365AHH</th>\n",
       "      <th>B365AHA</th>\n",
       "      <th>AHCh</th>\n",
       "      <th>B365CAHH</th>\n",
       "      <th>B365CAHA</th>\n",
       "      <th>HomeTeamELO</th>\n",
       "      <th>AwayTeamELO</th>\n",
       "      <th>balance_val</th>\n",
       "      <th>asia_final_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Auxerre</td>\n",
       "      <td>Nice</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>2.050</td>\n",
       "      <td>1.850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1702.604858</td>\n",
       "      <td>1611.196045</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Guingamp</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.925</td>\n",
       "      <td>1.975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1685.016113</td>\n",
       "      <td>1665.625732</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>D1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Hamburg</td>\n",
       "      <td>Hannover</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.800</td>\n",
       "      <td>2.100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1718.566284</td>\n",
       "      <td>1649.805298</td>\n",
       "      <td>-1</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>D1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Hertha</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>23.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>2.025</td>\n",
       "      <td>1.875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1719.916748</td>\n",
       "      <td>1692.120972</td>\n",
       "      <td>-1</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Lens</td>\n",
       "      <td>Le Mans</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.900</td>\n",
       "      <td>2.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1697.354004</td>\n",
       "      <td>1539.958130</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38428</th>\n",
       "      <td>38428</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Udinese</td>\n",
       "      <td>Roma</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.990</td>\n",
       "      <td>1.940</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1667.065796</td>\n",
       "      <td>1772.048584</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38429</th>\n",
       "      <td>38429</td>\n",
       "      <td>SP1</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Vallecano</td>\n",
       "      <td>Girona</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.910</td>\n",
       "      <td>1.990</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.82</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1635.277222</td>\n",
       "      <td>1704.093872</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38430</th>\n",
       "      <td>38430</td>\n",
       "      <td>SP1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Alaves</td>\n",
       "      <td>Celta</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.890</td>\n",
       "      <td>2.010</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1621.804321</td>\n",
       "      <td>1637.579956</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38431</th>\n",
       "      <td>38431</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Genoa</td>\n",
       "      <td>Monza</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.950</td>\n",
       "      <td>1.980</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1682.297119</td>\n",
       "      <td>1628.680542</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38432</th>\n",
       "      <td>38432</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Venezia</td>\n",
       "      <td>Verona</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.920</td>\n",
       "      <td>2.010</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1575.471802</td>\n",
       "      <td>1615.984619</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37346 rows × 31 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:20.612805Z",
     "start_time": "2025-02-04T03:21:20.555230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nan_football_df_noNone = nan_football_df.dropna(subset = nan_mask)\n",
    "nan_football_df_noNone"
   ],
   "id": "d1bdaa50359f4dd6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Unnamed: 0  Div       Date   HomeTeam       AwayTeam  FTHG  FTAG FTR  \\\n",
       "2               2   F1 2003-02-02    Auxerre           Nice   1.0   2.0   A   \n",
       "3               3   F1 2003-02-02   Guingamp      Marseille   0.0   1.0   A   \n",
       "4               4   D1 2003-02-02    Hamburg       Hannover   0.0   3.0   A   \n",
       "5               5   D1 2003-02-02     Hertha  Werder Bremen   0.0   3.0   A   \n",
       "7               7   F1 2003-02-02       Lens        Le Mans   0.0   0.0   D   \n",
       "...           ...  ...        ...        ...            ...   ...   ...  ..   \n",
       "38428       38428   I1 2024-07-26    Udinese           Roma   1.0   2.0   A   \n",
       "38429       38429  SP1 2024-07-26  Vallecano         Girona   2.0   1.0   H   \n",
       "38430       38430  SP1 2024-07-27     Alaves          Celta   1.0   1.0   D   \n",
       "38431       38431   I1 2024-07-27      Genoa          Monza   2.0   0.0   H   \n",
       "38432       38432   I1 2024-07-27    Venezia         Verona   1.0   1.0   D   \n",
       "\n",
       "         HS    AS  ...   AHh  B365AHH  B365AHA  AHCh  B365CAHH  B365CAHA  \\\n",
       "2       NaN   NaN  ... -0.75    2.050    1.850   NaN       NaN       NaN   \n",
       "3       NaN   NaN  ...  0.00    1.925    1.975   NaN       NaN       NaN   \n",
       "4      10.0  16.0  ... -0.75    1.800    2.100   NaN       NaN       NaN   \n",
       "5      23.0  19.0  ... -0.75    2.025    1.875   NaN       NaN       NaN   \n",
       "7       NaN   NaN  ... -0.75    1.900    2.000   NaN       NaN       NaN   \n",
       "...     ...   ...  ...   ...      ...      ...   ...       ...       ...   \n",
       "38428  10.0  16.0  ...  0.25    1.990    1.940  0.25      1.73      2.08   \n",
       "38429  17.0  12.0  ...  0.00    1.910    1.990  0.00      1.82      2.11   \n",
       "38430   3.0   5.0  ...  0.00    1.890    2.010 -0.25      1.98      1.95   \n",
       "38431  18.0   5.0  ... -0.25    1.950    1.980 -0.25      1.98      1.95   \n",
       "38432   9.0  21.0  ... -0.25    1.920    2.010 -0.25      1.98      1.95   \n",
       "\n",
       "       HomeTeamELO  AwayTeamELO  balance_val  asia_final_result  \n",
       "2      1702.604858  1611.196045           -1               -2.0  \n",
       "3      1685.016113  1665.625732            0               -1.0  \n",
       "4      1718.566284  1649.805298           -1               -4.0  \n",
       "5      1719.916748  1692.120972           -1               -4.0  \n",
       "7      1697.354004  1539.958130           -1               -1.0  \n",
       "...            ...          ...          ...                ...  \n",
       "38428  1667.065796  1772.048584            1                0.0  \n",
       "38429  1635.277222  1704.093872            0                1.0  \n",
       "38430  1621.804321  1637.579956            0                0.0  \n",
       "38431  1682.297119  1628.680542           -1                1.0  \n",
       "38432  1575.471802  1615.984619           -1               -1.0  \n",
       "\n",
       "[37346 rows x 31 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Div</th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HS</th>\n",
       "      <th>AS</th>\n",
       "      <th>...</th>\n",
       "      <th>AHh</th>\n",
       "      <th>B365AHH</th>\n",
       "      <th>B365AHA</th>\n",
       "      <th>AHCh</th>\n",
       "      <th>B365CAHH</th>\n",
       "      <th>B365CAHA</th>\n",
       "      <th>HomeTeamELO</th>\n",
       "      <th>AwayTeamELO</th>\n",
       "      <th>balance_val</th>\n",
       "      <th>asia_final_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Auxerre</td>\n",
       "      <td>Nice</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>2.050</td>\n",
       "      <td>1.850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1702.604858</td>\n",
       "      <td>1611.196045</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Guingamp</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.925</td>\n",
       "      <td>1.975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1685.016113</td>\n",
       "      <td>1665.625732</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>D1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Hamburg</td>\n",
       "      <td>Hannover</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.800</td>\n",
       "      <td>2.100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1718.566284</td>\n",
       "      <td>1649.805298</td>\n",
       "      <td>-1</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>D1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Hertha</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>23.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>2.025</td>\n",
       "      <td>1.875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1719.916748</td>\n",
       "      <td>1692.120972</td>\n",
       "      <td>-1</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Lens</td>\n",
       "      <td>Le Mans</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.900</td>\n",
       "      <td>2.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1697.354004</td>\n",
       "      <td>1539.958130</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38428</th>\n",
       "      <td>38428</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Udinese</td>\n",
       "      <td>Roma</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.990</td>\n",
       "      <td>1.940</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1667.065796</td>\n",
       "      <td>1772.048584</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38429</th>\n",
       "      <td>38429</td>\n",
       "      <td>SP1</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Vallecano</td>\n",
       "      <td>Girona</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.910</td>\n",
       "      <td>1.990</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.82</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1635.277222</td>\n",
       "      <td>1704.093872</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38430</th>\n",
       "      <td>38430</td>\n",
       "      <td>SP1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Alaves</td>\n",
       "      <td>Celta</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.890</td>\n",
       "      <td>2.010</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1621.804321</td>\n",
       "      <td>1637.579956</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38431</th>\n",
       "      <td>38431</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Genoa</td>\n",
       "      <td>Monza</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.950</td>\n",
       "      <td>1.980</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1682.297119</td>\n",
       "      <td>1628.680542</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38432</th>\n",
       "      <td>38432</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Venezia</td>\n",
       "      <td>Verona</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.920</td>\n",
       "      <td>2.010</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1575.471802</td>\n",
       "      <td>1615.984619</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37346 rows × 31 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:20.920460Z",
     "start_time": "2025-02-04T03:21:20.843083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nan_football_df_noNone.reset_index(inplace=True, drop=True)\n",
    "nan_football_df_noNone"
   ],
   "id": "fd25a49ce3655cb6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Unnamed: 0  Div       Date   HomeTeam       AwayTeam  FTHG  FTAG FTR  \\\n",
       "0               2   F1 2003-02-02    Auxerre           Nice   1.0   2.0   A   \n",
       "1               3   F1 2003-02-02   Guingamp      Marseille   0.0   1.0   A   \n",
       "2               4   D1 2003-02-02    Hamburg       Hannover   0.0   3.0   A   \n",
       "3               5   D1 2003-02-02     Hertha  Werder Bremen   0.0   3.0   A   \n",
       "4               7   F1 2003-02-02       Lens        Le Mans   0.0   0.0   D   \n",
       "...           ...  ...        ...        ...            ...   ...   ...  ..   \n",
       "37341       38428   I1 2024-07-26    Udinese           Roma   1.0   2.0   A   \n",
       "37342       38429  SP1 2024-07-26  Vallecano         Girona   2.0   1.0   H   \n",
       "37343       38430  SP1 2024-07-27     Alaves          Celta   1.0   1.0   D   \n",
       "37344       38431   I1 2024-07-27      Genoa          Monza   2.0   0.0   H   \n",
       "37345       38432   I1 2024-07-27    Venezia         Verona   1.0   1.0   D   \n",
       "\n",
       "         HS    AS  ...   AHh  B365AHH  B365AHA  AHCh  B365CAHH  B365CAHA  \\\n",
       "0       NaN   NaN  ... -0.75    2.050    1.850   NaN       NaN       NaN   \n",
       "1       NaN   NaN  ...  0.00    1.925    1.975   NaN       NaN       NaN   \n",
       "2      10.0  16.0  ... -0.75    1.800    2.100   NaN       NaN       NaN   \n",
       "3      23.0  19.0  ... -0.75    2.025    1.875   NaN       NaN       NaN   \n",
       "4       NaN   NaN  ... -0.75    1.900    2.000   NaN       NaN       NaN   \n",
       "...     ...   ...  ...   ...      ...      ...   ...       ...       ...   \n",
       "37341  10.0  16.0  ...  0.25    1.990    1.940  0.25      1.73      2.08   \n",
       "37342  17.0  12.0  ...  0.00    1.910    1.990  0.00      1.82      2.11   \n",
       "37343   3.0   5.0  ...  0.00    1.890    2.010 -0.25      1.98      1.95   \n",
       "37344  18.0   5.0  ... -0.25    1.950    1.980 -0.25      1.98      1.95   \n",
       "37345   9.0  21.0  ... -0.25    1.920    2.010 -0.25      1.98      1.95   \n",
       "\n",
       "       HomeTeamELO  AwayTeamELO  balance_val  asia_final_result  \n",
       "0      1702.604858  1611.196045           -1               -2.0  \n",
       "1      1685.016113  1665.625732            0               -1.0  \n",
       "2      1718.566284  1649.805298           -1               -4.0  \n",
       "3      1719.916748  1692.120972           -1               -4.0  \n",
       "4      1697.354004  1539.958130           -1               -1.0  \n",
       "...            ...          ...          ...                ...  \n",
       "37341  1667.065796  1772.048584            1                0.0  \n",
       "37342  1635.277222  1704.093872            0                1.0  \n",
       "37343  1621.804321  1637.579956            0                0.0  \n",
       "37344  1682.297119  1628.680542           -1                1.0  \n",
       "37345  1575.471802  1615.984619           -1               -1.0  \n",
       "\n",
       "[37346 rows x 31 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Div</th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HS</th>\n",
       "      <th>AS</th>\n",
       "      <th>...</th>\n",
       "      <th>AHh</th>\n",
       "      <th>B365AHH</th>\n",
       "      <th>B365AHA</th>\n",
       "      <th>AHCh</th>\n",
       "      <th>B365CAHH</th>\n",
       "      <th>B365CAHA</th>\n",
       "      <th>HomeTeamELO</th>\n",
       "      <th>AwayTeamELO</th>\n",
       "      <th>balance_val</th>\n",
       "      <th>asia_final_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Auxerre</td>\n",
       "      <td>Nice</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>2.050</td>\n",
       "      <td>1.850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1702.604858</td>\n",
       "      <td>1611.196045</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Guingamp</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.925</td>\n",
       "      <td>1.975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1685.016113</td>\n",
       "      <td>1665.625732</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>D1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Hamburg</td>\n",
       "      <td>Hannover</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.800</td>\n",
       "      <td>2.100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1718.566284</td>\n",
       "      <td>1649.805298</td>\n",
       "      <td>-1</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>D1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Hertha</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>23.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>2.025</td>\n",
       "      <td>1.875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1719.916748</td>\n",
       "      <td>1692.120972</td>\n",
       "      <td>-1</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Lens</td>\n",
       "      <td>Le Mans</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.900</td>\n",
       "      <td>2.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1697.354004</td>\n",
       "      <td>1539.958130</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37341</th>\n",
       "      <td>38428</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Udinese</td>\n",
       "      <td>Roma</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.990</td>\n",
       "      <td>1.940</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1667.065796</td>\n",
       "      <td>1772.048584</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37342</th>\n",
       "      <td>38429</td>\n",
       "      <td>SP1</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Vallecano</td>\n",
       "      <td>Girona</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.910</td>\n",
       "      <td>1.990</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.82</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1635.277222</td>\n",
       "      <td>1704.093872</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37343</th>\n",
       "      <td>38430</td>\n",
       "      <td>SP1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Alaves</td>\n",
       "      <td>Celta</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.890</td>\n",
       "      <td>2.010</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1621.804321</td>\n",
       "      <td>1637.579956</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37344</th>\n",
       "      <td>38431</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Genoa</td>\n",
       "      <td>Monza</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.950</td>\n",
       "      <td>1.980</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1682.297119</td>\n",
       "      <td>1628.680542</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37345</th>\n",
       "      <td>38432</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Venezia</td>\n",
       "      <td>Verona</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.920</td>\n",
       "      <td>2.010</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1575.471802</td>\n",
       "      <td>1615.984619</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37346 rows × 31 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:21.433353Z",
     "start_time": "2025-02-04T03:21:21.360568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conditions = [\n",
    "    nan_football_df_noNone['asia_final_result'] < 0,\n",
    "    nan_football_df_noNone['asia_final_result'] == 0,\n",
    "    nan_football_df_noNone['asia_final_result'] > 0,\n",
    "]\n",
    "easy_labels = [0, 1, 2]\n",
    "\n",
    "nan_football_df_noNone['easy_label'] = np.select(conditions, easy_labels)\n",
    "nan_football_df_noNone"
   ],
   "id": "143b539d11ec3c73",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Unnamed: 0  Div       Date   HomeTeam       AwayTeam  FTHG  FTAG FTR  \\\n",
       "0               2   F1 2003-02-02    Auxerre           Nice   1.0   2.0   A   \n",
       "1               3   F1 2003-02-02   Guingamp      Marseille   0.0   1.0   A   \n",
       "2               4   D1 2003-02-02    Hamburg       Hannover   0.0   3.0   A   \n",
       "3               5   D1 2003-02-02     Hertha  Werder Bremen   0.0   3.0   A   \n",
       "4               7   F1 2003-02-02       Lens        Le Mans   0.0   0.0   D   \n",
       "...           ...  ...        ...        ...            ...   ...   ...  ..   \n",
       "37341       38428   I1 2024-07-26    Udinese           Roma   1.0   2.0   A   \n",
       "37342       38429  SP1 2024-07-26  Vallecano         Girona   2.0   1.0   H   \n",
       "37343       38430  SP1 2024-07-27     Alaves          Celta   1.0   1.0   D   \n",
       "37344       38431   I1 2024-07-27      Genoa          Monza   2.0   0.0   H   \n",
       "37345       38432   I1 2024-07-27    Venezia         Verona   1.0   1.0   D   \n",
       "\n",
       "         HS    AS  ...  B365AHH  B365AHA  AHCh  B365CAHH  B365CAHA  \\\n",
       "0       NaN   NaN  ...    2.050    1.850   NaN       NaN       NaN   \n",
       "1       NaN   NaN  ...    1.925    1.975   NaN       NaN       NaN   \n",
       "2      10.0  16.0  ...    1.800    2.100   NaN       NaN       NaN   \n",
       "3      23.0  19.0  ...    2.025    1.875   NaN       NaN       NaN   \n",
       "4       NaN   NaN  ...    1.900    2.000   NaN       NaN       NaN   \n",
       "...     ...   ...  ...      ...      ...   ...       ...       ...   \n",
       "37341  10.0  16.0  ...    1.990    1.940  0.25      1.73      2.08   \n",
       "37342  17.0  12.0  ...    1.910    1.990  0.00      1.82      2.11   \n",
       "37343   3.0   5.0  ...    1.890    2.010 -0.25      1.98      1.95   \n",
       "37344  18.0   5.0  ...    1.950    1.980 -0.25      1.98      1.95   \n",
       "37345   9.0  21.0  ...    1.920    2.010 -0.25      1.98      1.95   \n",
       "\n",
       "       HomeTeamELO  AwayTeamELO  balance_val  asia_final_result  easy_label  \n",
       "0      1702.604858  1611.196045           -1               -2.0           0  \n",
       "1      1685.016113  1665.625732            0               -1.0           0  \n",
       "2      1718.566284  1649.805298           -1               -4.0           0  \n",
       "3      1719.916748  1692.120972           -1               -4.0           0  \n",
       "4      1697.354004  1539.958130           -1               -1.0           0  \n",
       "...            ...          ...          ...                ...         ...  \n",
       "37341  1667.065796  1772.048584            1                0.0           1  \n",
       "37342  1635.277222  1704.093872            0                1.0           2  \n",
       "37343  1621.804321  1637.579956            0                0.0           1  \n",
       "37344  1682.297119  1628.680542           -1                1.0           2  \n",
       "37345  1575.471802  1615.984619           -1               -1.0           0  \n",
       "\n",
       "[37346 rows x 32 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Div</th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HS</th>\n",
       "      <th>AS</th>\n",
       "      <th>...</th>\n",
       "      <th>B365AHH</th>\n",
       "      <th>B365AHA</th>\n",
       "      <th>AHCh</th>\n",
       "      <th>B365CAHH</th>\n",
       "      <th>B365CAHA</th>\n",
       "      <th>HomeTeamELO</th>\n",
       "      <th>AwayTeamELO</th>\n",
       "      <th>balance_val</th>\n",
       "      <th>asia_final_result</th>\n",
       "      <th>easy_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Auxerre</td>\n",
       "      <td>Nice</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.050</td>\n",
       "      <td>1.850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1702.604858</td>\n",
       "      <td>1611.196045</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Guingamp</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.925</td>\n",
       "      <td>1.975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1685.016113</td>\n",
       "      <td>1665.625732</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>D1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Hamburg</td>\n",
       "      <td>Hannover</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.800</td>\n",
       "      <td>2.100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1718.566284</td>\n",
       "      <td>1649.805298</td>\n",
       "      <td>-1</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>D1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Hertha</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>23.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.025</td>\n",
       "      <td>1.875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1719.916748</td>\n",
       "      <td>1692.120972</td>\n",
       "      <td>-1</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Lens</td>\n",
       "      <td>Le Mans</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.900</td>\n",
       "      <td>2.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1697.354004</td>\n",
       "      <td>1539.958130</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37341</th>\n",
       "      <td>38428</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Udinese</td>\n",
       "      <td>Roma</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.990</td>\n",
       "      <td>1.940</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1667.065796</td>\n",
       "      <td>1772.048584</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37342</th>\n",
       "      <td>38429</td>\n",
       "      <td>SP1</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Vallecano</td>\n",
       "      <td>Girona</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.910</td>\n",
       "      <td>1.990</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.82</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1635.277222</td>\n",
       "      <td>1704.093872</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37343</th>\n",
       "      <td>38430</td>\n",
       "      <td>SP1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Alaves</td>\n",
       "      <td>Celta</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.890</td>\n",
       "      <td>2.010</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1621.804321</td>\n",
       "      <td>1637.579956</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37344</th>\n",
       "      <td>38431</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Genoa</td>\n",
       "      <td>Monza</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.950</td>\n",
       "      <td>1.980</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1682.297119</td>\n",
       "      <td>1628.680542</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37345</th>\n",
       "      <td>38432</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Venezia</td>\n",
       "      <td>Verona</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.920</td>\n",
       "      <td>2.010</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1575.471802</td>\n",
       "      <td>1615.984619</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37346 rows × 32 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "id": "ad42e459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:21.909286Z",
     "start_time": "2025-02-04T03:21:21.858293Z"
    }
   },
   "source": [
    "# resize shape\n",
    "football_df.shape[0] - nan_football_df_noNone.shape[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1087"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "id": "42864535",
   "metadata": {},
   "source": [
    "### 2.2 Feature Encoding <br>\n",
    "* $\\phi(Date)$ $\\Rightarrow$ one column for *year*, second column for *month*, third column for *day of year*\n",
    "* One hot encode Division, Home and Away Teams\n",
    "* Label encode Full Time Result (Win/Draw/Loss)"
   ]
  },
  {
   "cell_type": "code",
   "id": "7b5bb61e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:22.275202Z",
     "start_time": "2025-02-04T03:21:22.203506Z"
    }
   },
   "source": [
    "feats = nan_mask\n",
    "feats.append('easy_label')\n",
    "feats.append('balance_val')"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:23.042197Z",
     "start_time": "2025-02-04T03:21:22.957777Z"
    }
   },
   "cell_type": "code",
   "source": "nan_football_df_noNone",
   "id": "9919d74193a28304",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Unnamed: 0  Div       Date   HomeTeam       AwayTeam  FTHG  FTAG FTR  \\\n",
       "0               2   F1 2003-02-02    Auxerre           Nice   1.0   2.0   A   \n",
       "1               3   F1 2003-02-02   Guingamp      Marseille   0.0   1.0   A   \n",
       "2               4   D1 2003-02-02    Hamburg       Hannover   0.0   3.0   A   \n",
       "3               5   D1 2003-02-02     Hertha  Werder Bremen   0.0   3.0   A   \n",
       "4               7   F1 2003-02-02       Lens        Le Mans   0.0   0.0   D   \n",
       "...           ...  ...        ...        ...            ...   ...   ...  ..   \n",
       "37341       38428   I1 2024-07-26    Udinese           Roma   1.0   2.0   A   \n",
       "37342       38429  SP1 2024-07-26  Vallecano         Girona   2.0   1.0   H   \n",
       "37343       38430  SP1 2024-07-27     Alaves          Celta   1.0   1.0   D   \n",
       "37344       38431   I1 2024-07-27      Genoa          Monza   2.0   0.0   H   \n",
       "37345       38432   I1 2024-07-27    Venezia         Verona   1.0   1.0   D   \n",
       "\n",
       "         HS    AS  ...  B365AHH  B365AHA  AHCh  B365CAHH  B365CAHA  \\\n",
       "0       NaN   NaN  ...    2.050    1.850   NaN       NaN       NaN   \n",
       "1       NaN   NaN  ...    1.925    1.975   NaN       NaN       NaN   \n",
       "2      10.0  16.0  ...    1.800    2.100   NaN       NaN       NaN   \n",
       "3      23.0  19.0  ...    2.025    1.875   NaN       NaN       NaN   \n",
       "4       NaN   NaN  ...    1.900    2.000   NaN       NaN       NaN   \n",
       "...     ...   ...  ...      ...      ...   ...       ...       ...   \n",
       "37341  10.0  16.0  ...    1.990    1.940  0.25      1.73      2.08   \n",
       "37342  17.0  12.0  ...    1.910    1.990  0.00      1.82      2.11   \n",
       "37343   3.0   5.0  ...    1.890    2.010 -0.25      1.98      1.95   \n",
       "37344  18.0   5.0  ...    1.950    1.980 -0.25      1.98      1.95   \n",
       "37345   9.0  21.0  ...    1.920    2.010 -0.25      1.98      1.95   \n",
       "\n",
       "       HomeTeamELO  AwayTeamELO  balance_val  asia_final_result  easy_label  \n",
       "0      1702.604858  1611.196045           -1               -2.0           0  \n",
       "1      1685.016113  1665.625732            0               -1.0           0  \n",
       "2      1718.566284  1649.805298           -1               -4.0           0  \n",
       "3      1719.916748  1692.120972           -1               -4.0           0  \n",
       "4      1697.354004  1539.958130           -1               -1.0           0  \n",
       "...            ...          ...          ...                ...         ...  \n",
       "37341  1667.065796  1772.048584            1                0.0           1  \n",
       "37342  1635.277222  1704.093872            0                1.0           2  \n",
       "37343  1621.804321  1637.579956            0                0.0           1  \n",
       "37344  1682.297119  1628.680542           -1                1.0           2  \n",
       "37345  1575.471802  1615.984619           -1               -1.0           0  \n",
       "\n",
       "[37346 rows x 32 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Div</th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HS</th>\n",
       "      <th>AS</th>\n",
       "      <th>...</th>\n",
       "      <th>B365AHH</th>\n",
       "      <th>B365AHA</th>\n",
       "      <th>AHCh</th>\n",
       "      <th>B365CAHH</th>\n",
       "      <th>B365CAHA</th>\n",
       "      <th>HomeTeamELO</th>\n",
       "      <th>AwayTeamELO</th>\n",
       "      <th>balance_val</th>\n",
       "      <th>asia_final_result</th>\n",
       "      <th>easy_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Auxerre</td>\n",
       "      <td>Nice</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.050</td>\n",
       "      <td>1.850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1702.604858</td>\n",
       "      <td>1611.196045</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Guingamp</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.925</td>\n",
       "      <td>1.975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1685.016113</td>\n",
       "      <td>1665.625732</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>D1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Hamburg</td>\n",
       "      <td>Hannover</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.800</td>\n",
       "      <td>2.100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1718.566284</td>\n",
       "      <td>1649.805298</td>\n",
       "      <td>-1</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>D1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Hertha</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>23.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.025</td>\n",
       "      <td>1.875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1719.916748</td>\n",
       "      <td>1692.120972</td>\n",
       "      <td>-1</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Lens</td>\n",
       "      <td>Le Mans</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.900</td>\n",
       "      <td>2.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1697.354004</td>\n",
       "      <td>1539.958130</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37341</th>\n",
       "      <td>38428</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Udinese</td>\n",
       "      <td>Roma</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.990</td>\n",
       "      <td>1.940</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1667.065796</td>\n",
       "      <td>1772.048584</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37342</th>\n",
       "      <td>38429</td>\n",
       "      <td>SP1</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Vallecano</td>\n",
       "      <td>Girona</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.910</td>\n",
       "      <td>1.990</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.82</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1635.277222</td>\n",
       "      <td>1704.093872</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37343</th>\n",
       "      <td>38430</td>\n",
       "      <td>SP1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Alaves</td>\n",
       "      <td>Celta</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.890</td>\n",
       "      <td>2.010</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1621.804321</td>\n",
       "      <td>1637.579956</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37344</th>\n",
       "      <td>38431</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Genoa</td>\n",
       "      <td>Monza</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.950</td>\n",
       "      <td>1.980</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1682.297119</td>\n",
       "      <td>1628.680542</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37345</th>\n",
       "      <td>38432</td>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Venezia</td>\n",
       "      <td>Verona</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.920</td>\n",
       "      <td>2.010</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1575.471802</td>\n",
       "      <td>1615.984619</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37346 rows × 32 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:23.433756Z",
     "start_time": "2025-02-04T03:21:23.391932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_df_feat = nan_football_df_noNone.copy()[feats]\n",
    "learning_df_feat"
   ],
   "id": "5213f7b7293d7cc0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Div       Date   HomeTeam       AwayTeam  FTHG  FTAG FTR  B365H  B365D  \\\n",
       "0       F1 2003-02-02    Auxerre           Nice   1.0   2.0   A  1.727  3.100   \n",
       "1       F1 2003-02-02   Guingamp      Marseille   0.0   1.0   A  2.500  2.875   \n",
       "2       D1 2003-02-02    Hamburg       Hannover   0.0   3.0   A  1.571  3.500   \n",
       "3       D1 2003-02-02     Hertha  Werder Bremen   0.0   3.0   A  1.833  3.200   \n",
       "4       F1 2003-02-02       Lens        Le Mans   0.0   0.0   D  1.571  3.250   \n",
       "...    ...        ...        ...            ...   ...   ...  ..    ...    ...   \n",
       "37341   I1 2024-07-26    Udinese           Roma   1.0   2.0   A  3.300  3.300   \n",
       "37342  SP1 2024-07-26  Vallecano         Girona   2.0   1.0   H  2.750  2.900   \n",
       "37343  SP1 2024-07-27     Alaves          Celta   1.0   1.0   D  2.630  3.000   \n",
       "37344   I1 2024-07-27      Genoa          Monza   2.0   0.0   H  2.250  3.000   \n",
       "37345   I1 2024-07-27    Venezia         Verona   1.0   1.0   D  2.150  3.500   \n",
       "\n",
       "       B365A   WHH  WHD   WHA   AHh  B365AHH  B365AHA  HomeTeamELO  \\\n",
       "0      4.500  1.66  3.1  5.00 -0.75    2.050    1.850  1702.604858   \n",
       "1      2.625  2.60  3.1  2.40  0.00    1.925    1.975  1685.016113   \n",
       "2      5.000  1.57  3.5  5.00 -0.75    1.800    2.100  1718.566284   \n",
       "3      3.750  1.72  3.4  4.00 -0.75    2.025    1.875  1719.916748   \n",
       "4      5.500  1.66  3.3  4.50 -0.75    1.900    2.000  1697.354004   \n",
       "...      ...   ...  ...   ...   ...      ...      ...          ...   \n",
       "37341  2.250  3.30  3.2  2.20  0.25    1.990    1.940  1667.065796   \n",
       "37342  2.880  2.75  3.0  2.80  0.00    1.910    1.990  1635.277222   \n",
       "37343  3.000  2.62  3.2  2.75  0.00    1.890    2.010  1621.804321   \n",
       "37344  3.600  2.25  3.0  3.40 -0.25    1.950    1.980  1682.297119   \n",
       "37345  3.250  2.15  3.3  3.25 -0.25    1.920    2.010  1575.471802   \n",
       "\n",
       "       AwayTeamELO  easy_label  balance_val  \n",
       "0      1611.196045           0           -1  \n",
       "1      1665.625732           0            0  \n",
       "2      1649.805298           0           -1  \n",
       "3      1692.120972           0           -1  \n",
       "4      1539.958130           0           -1  \n",
       "...            ...         ...          ...  \n",
       "37341  1772.048584           1            1  \n",
       "37342  1704.093872           2            0  \n",
       "37343  1637.579956           1            0  \n",
       "37344  1628.680542           2           -1  \n",
       "37345  1615.984619           0           -1  \n",
       "\n",
       "[37346 rows x 20 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Div</th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>WHH</th>\n",
       "      <th>WHD</th>\n",
       "      <th>WHA</th>\n",
       "      <th>AHh</th>\n",
       "      <th>B365AHH</th>\n",
       "      <th>B365AHA</th>\n",
       "      <th>HomeTeamELO</th>\n",
       "      <th>AwayTeamELO</th>\n",
       "      <th>easy_label</th>\n",
       "      <th>balance_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Auxerre</td>\n",
       "      <td>Nice</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>1.727</td>\n",
       "      <td>3.100</td>\n",
       "      <td>4.500</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>2.050</td>\n",
       "      <td>1.850</td>\n",
       "      <td>1702.604858</td>\n",
       "      <td>1611.196045</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Guingamp</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>2.500</td>\n",
       "      <td>2.875</td>\n",
       "      <td>2.625</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.925</td>\n",
       "      <td>1.975</td>\n",
       "      <td>1685.016113</td>\n",
       "      <td>1665.625732</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Hamburg</td>\n",
       "      <td>Hannover</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>1.571</td>\n",
       "      <td>3.500</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.800</td>\n",
       "      <td>2.100</td>\n",
       "      <td>1718.566284</td>\n",
       "      <td>1649.805298</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Hertha</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>1.833</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.750</td>\n",
       "      <td>1.72</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.00</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>2.025</td>\n",
       "      <td>1.875</td>\n",
       "      <td>1719.916748</td>\n",
       "      <td>1692.120972</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Lens</td>\n",
       "      <td>Le Mans</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "      <td>1.571</td>\n",
       "      <td>3.250</td>\n",
       "      <td>5.500</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.50</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.900</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1697.354004</td>\n",
       "      <td>1539.958130</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37341</th>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Udinese</td>\n",
       "      <td>Roma</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>3.300</td>\n",
       "      <td>3.300</td>\n",
       "      <td>2.250</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.990</td>\n",
       "      <td>1.940</td>\n",
       "      <td>1667.065796</td>\n",
       "      <td>1772.048584</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37342</th>\n",
       "      <td>SP1</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Vallecano</td>\n",
       "      <td>Girona</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>2.750</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.880</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.910</td>\n",
       "      <td>1.990</td>\n",
       "      <td>1635.277222</td>\n",
       "      <td>1704.093872</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37343</th>\n",
       "      <td>SP1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Alaves</td>\n",
       "      <td>Celta</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>2.630</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.890</td>\n",
       "      <td>2.010</td>\n",
       "      <td>1621.804321</td>\n",
       "      <td>1637.579956</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37344</th>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Genoa</td>\n",
       "      <td>Monza</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>2.250</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.600</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.950</td>\n",
       "      <td>1.980</td>\n",
       "      <td>1682.297119</td>\n",
       "      <td>1628.680542</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37345</th>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Venezia</td>\n",
       "      <td>Verona</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>2.150</td>\n",
       "      <td>3.500</td>\n",
       "      <td>3.250</td>\n",
       "      <td>2.15</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.920</td>\n",
       "      <td>2.010</td>\n",
       "      <td>1575.471802</td>\n",
       "      <td>1615.984619</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37346 rows × 20 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "id": "fe16f1b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:24.475789Z",
     "start_time": "2025-02-04T03:21:23.571118Z"
    }
   },
   "source": [
    "learning_df_feat.reset_index(inplace=True, drop=True)\n",
    "# 保存文件作为历史文件\n",
    "learning_df_feat.to_csv('.\\prediction_data/history_data_balance.csv', index=False, encoding='utf-8-sig')\n",
    "learning_df_feat"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Div       Date   HomeTeam       AwayTeam  FTHG  FTAG FTR  B365H  B365D  \\\n",
       "0       F1 2003-02-02    Auxerre           Nice   1.0   2.0   A  1.727  3.100   \n",
       "1       F1 2003-02-02   Guingamp      Marseille   0.0   1.0   A  2.500  2.875   \n",
       "2       D1 2003-02-02    Hamburg       Hannover   0.0   3.0   A  1.571  3.500   \n",
       "3       D1 2003-02-02     Hertha  Werder Bremen   0.0   3.0   A  1.833  3.200   \n",
       "4       F1 2003-02-02       Lens        Le Mans   0.0   0.0   D  1.571  3.250   \n",
       "...    ...        ...        ...            ...   ...   ...  ..    ...    ...   \n",
       "37341   I1 2024-07-26    Udinese           Roma   1.0   2.0   A  3.300  3.300   \n",
       "37342  SP1 2024-07-26  Vallecano         Girona   2.0   1.0   H  2.750  2.900   \n",
       "37343  SP1 2024-07-27     Alaves          Celta   1.0   1.0   D  2.630  3.000   \n",
       "37344   I1 2024-07-27      Genoa          Monza   2.0   0.0   H  2.250  3.000   \n",
       "37345   I1 2024-07-27    Venezia         Verona   1.0   1.0   D  2.150  3.500   \n",
       "\n",
       "       B365A   WHH  WHD   WHA   AHh  B365AHH  B365AHA  HomeTeamELO  \\\n",
       "0      4.500  1.66  3.1  5.00 -0.75    2.050    1.850  1702.604858   \n",
       "1      2.625  2.60  3.1  2.40  0.00    1.925    1.975  1685.016113   \n",
       "2      5.000  1.57  3.5  5.00 -0.75    1.800    2.100  1718.566284   \n",
       "3      3.750  1.72  3.4  4.00 -0.75    2.025    1.875  1719.916748   \n",
       "4      5.500  1.66  3.3  4.50 -0.75    1.900    2.000  1697.354004   \n",
       "...      ...   ...  ...   ...   ...      ...      ...          ...   \n",
       "37341  2.250  3.30  3.2  2.20  0.25    1.990    1.940  1667.065796   \n",
       "37342  2.880  2.75  3.0  2.80  0.00    1.910    1.990  1635.277222   \n",
       "37343  3.000  2.62  3.2  2.75  0.00    1.890    2.010  1621.804321   \n",
       "37344  3.600  2.25  3.0  3.40 -0.25    1.950    1.980  1682.297119   \n",
       "37345  3.250  2.15  3.3  3.25 -0.25    1.920    2.010  1575.471802   \n",
       "\n",
       "       AwayTeamELO  easy_label  balance_val  \n",
       "0      1611.196045           0           -1  \n",
       "1      1665.625732           0            0  \n",
       "2      1649.805298           0           -1  \n",
       "3      1692.120972           0           -1  \n",
       "4      1539.958130           0           -1  \n",
       "...            ...         ...          ...  \n",
       "37341  1772.048584           1            1  \n",
       "37342  1704.093872           2            0  \n",
       "37343  1637.579956           1            0  \n",
       "37344  1628.680542           2           -1  \n",
       "37345  1615.984619           0           -1  \n",
       "\n",
       "[37346 rows x 20 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Div</th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>WHH</th>\n",
       "      <th>WHD</th>\n",
       "      <th>WHA</th>\n",
       "      <th>AHh</th>\n",
       "      <th>B365AHH</th>\n",
       "      <th>B365AHA</th>\n",
       "      <th>HomeTeamELO</th>\n",
       "      <th>AwayTeamELO</th>\n",
       "      <th>easy_label</th>\n",
       "      <th>balance_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Auxerre</td>\n",
       "      <td>Nice</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>1.727</td>\n",
       "      <td>3.100</td>\n",
       "      <td>4.500</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>2.050</td>\n",
       "      <td>1.850</td>\n",
       "      <td>1702.604858</td>\n",
       "      <td>1611.196045</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Guingamp</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>2.500</td>\n",
       "      <td>2.875</td>\n",
       "      <td>2.625</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.925</td>\n",
       "      <td>1.975</td>\n",
       "      <td>1685.016113</td>\n",
       "      <td>1665.625732</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Hamburg</td>\n",
       "      <td>Hannover</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>1.571</td>\n",
       "      <td>3.500</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.800</td>\n",
       "      <td>2.100</td>\n",
       "      <td>1718.566284</td>\n",
       "      <td>1649.805298</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Hertha</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>1.833</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.750</td>\n",
       "      <td>1.72</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.00</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>2.025</td>\n",
       "      <td>1.875</td>\n",
       "      <td>1719.916748</td>\n",
       "      <td>1692.120972</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Lens</td>\n",
       "      <td>Le Mans</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "      <td>1.571</td>\n",
       "      <td>3.250</td>\n",
       "      <td>5.500</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.50</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.900</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1697.354004</td>\n",
       "      <td>1539.958130</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37341</th>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Udinese</td>\n",
       "      <td>Roma</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>3.300</td>\n",
       "      <td>3.300</td>\n",
       "      <td>2.250</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.990</td>\n",
       "      <td>1.940</td>\n",
       "      <td>1667.065796</td>\n",
       "      <td>1772.048584</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37342</th>\n",
       "      <td>SP1</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Vallecano</td>\n",
       "      <td>Girona</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>2.750</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.880</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.910</td>\n",
       "      <td>1.990</td>\n",
       "      <td>1635.277222</td>\n",
       "      <td>1704.093872</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37343</th>\n",
       "      <td>SP1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Alaves</td>\n",
       "      <td>Celta</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>2.630</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.890</td>\n",
       "      <td>2.010</td>\n",
       "      <td>1621.804321</td>\n",
       "      <td>1637.579956</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37344</th>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Genoa</td>\n",
       "      <td>Monza</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>2.250</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.600</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.950</td>\n",
       "      <td>1.980</td>\n",
       "      <td>1682.297119</td>\n",
       "      <td>1628.680542</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37345</th>\n",
       "      <td>I1</td>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Venezia</td>\n",
       "      <td>Verona</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>2.150</td>\n",
       "      <td>3.500</td>\n",
       "      <td>3.250</td>\n",
       "      <td>2.15</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.920</td>\n",
       "      <td>2.010</td>\n",
       "      <td>1575.471802</td>\n",
       "      <td>1615.984619</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37346 rows × 20 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "id": "15884417",
   "metadata": {},
   "source": [
    "**2.2.1 Division and Home/Away Team Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "id": "913c0088",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:24.540464Z",
     "start_time": "2025-02-04T03:21:24.477795Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "div_encoder = OneHotEncoder()\n",
    "home_encoder = OneHotEncoder()\n",
    "away_encoder = OneHotEncoder()"
   ],
   "outputs": [],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "id": "bc739d2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:24.676330Z",
     "start_time": "2025-02-04T03:21:24.542463Z"
    }
   },
   "source": [
    "onehot_div = div_encoder.fit_transform(learning_df_feat.Div.values.reshape(-1,1)).toarray().astype(int)\n",
    "onehot_div_df = pd.DataFrame(onehot_div, columns = [\"Div \"+str(int(i)) for i in range(onehot_div.shape[1])])\n",
    "\n",
    "onehot_home = home_encoder.fit_transform(learning_df_feat.HomeTeam.values.reshape(-1,1)).toarray().astype(int)\n",
    "onehot_home_df = pd.DataFrame(onehot_home, columns = ['HomeTeam ' + str(int(i)) for i in np.arange(onehot_home.shape[1])])\n",
    "\n",
    "onehot_away = away_encoder.fit_transform(learning_df_feat.AwayTeam.values.reshape(-1,1)).toarray().astype(int)\n",
    "onehot_away_df = pd.DataFrame(onehot_away, columns = ['AwayTeam ' + str(int(i)) for i in np.arange(onehot_away.shape[1])])"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:24.763929Z",
     "start_time": "2025-02-04T03:21:24.678331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 保存编码器到本地\n",
    "joblib.dump(div_encoder, 'div_encoder.pkl')\n",
    "joblib.dump(home_encoder, 'home_encoder.pkl')\n",
    "joblib.dump(away_encoder, 'away_encoder.pkl')"
   ],
   "id": "afe5dd460928770d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['away_encoder.pkl']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "id": "f8444147",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:24.897020Z",
     "start_time": "2025-02-04T03:21:24.765932Z"
    }
   },
   "source": [
    "learning_df_div = pd.concat([learning_df_feat, onehot_div_df, onehot_home_df, onehot_away_df], axis = 1)\n",
    "learning_df_div.drop(columns = ['Div'], inplace = True)"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "id": "321f2f32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:24.987148Z",
     "start_time": "2025-02-04T03:21:24.898025Z"
    }
   },
   "source": "learning_df_div",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            Date   HomeTeam       AwayTeam  FTHG  FTAG FTR  B365H  B365D  \\\n",
       "0     2003-02-02    Auxerre           Nice   1.0   2.0   A  1.727  3.100   \n",
       "1     2003-02-02   Guingamp      Marseille   0.0   1.0   A  2.500  2.875   \n",
       "2     2003-02-02    Hamburg       Hannover   0.0   3.0   A  1.571  3.500   \n",
       "3     2003-02-02     Hertha  Werder Bremen   0.0   3.0   A  1.833  3.200   \n",
       "4     2003-02-02       Lens        Le Mans   0.0   0.0   D  1.571  3.250   \n",
       "...          ...        ...            ...   ...   ...  ..    ...    ...   \n",
       "37341 2024-07-26    Udinese           Roma   1.0   2.0   A  3.300  3.300   \n",
       "37342 2024-07-26  Vallecano         Girona   2.0   1.0   H  2.750  2.900   \n",
       "37343 2024-07-27     Alaves          Celta   1.0   1.0   D  2.630  3.000   \n",
       "37344 2024-07-27      Genoa          Monza   2.0   0.0   H  2.250  3.000   \n",
       "37345 2024-07-27    Venezia         Verona   1.0   1.0   D  2.150  3.500   \n",
       "\n",
       "       B365A   WHH  ...  AwayTeam 197  AwayTeam 198  AwayTeam 199  \\\n",
       "0      4.500  1.66  ...             0             0             0   \n",
       "1      2.625  2.60  ...             0             0             0   \n",
       "2      5.000  1.57  ...             0             0             0   \n",
       "3      3.750  1.72  ...             0             0             1   \n",
       "4      5.500  1.66  ...             0             0             0   \n",
       "...      ...   ...  ...           ...           ...           ...   \n",
       "37341  2.250  3.30  ...             0             0             0   \n",
       "37342  2.880  2.75  ...             0             0             0   \n",
       "37343  3.000  2.62  ...             0             0             0   \n",
       "37344  3.600  2.25  ...             0             0             0   \n",
       "37345  3.250  2.15  ...             0             0             0   \n",
       "\n",
       "       AwayTeam 200  AwayTeam 201  AwayTeam 202  AwayTeam 203  AwayTeam 204  \\\n",
       "0                 0             0             0             0             0   \n",
       "1                 0             0             0             0             0   \n",
       "2                 0             0             0             0             0   \n",
       "3                 0             0             0             0             0   \n",
       "4                 0             0             0             0             0   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "37341             0             0             0             0             0   \n",
       "37342             0             0             0             0             0   \n",
       "37343             0             0             0             0             0   \n",
       "37344             0             0             0             0             0   \n",
       "37345             0             0             0             0             0   \n",
       "\n",
       "       AwayTeam 205  AwayTeam 206  \n",
       "0                 0             0  \n",
       "1                 0             0  \n",
       "2                 0             0  \n",
       "3                 0             0  \n",
       "4                 0             0  \n",
       "...             ...           ...  \n",
       "37341             0             0  \n",
       "37342             0             0  \n",
       "37343             0             0  \n",
       "37344             0             0  \n",
       "37345             0             0  \n",
       "\n",
       "[37346 rows x 438 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>WHH</th>\n",
       "      <th>...</th>\n",
       "      <th>AwayTeam 197</th>\n",
       "      <th>AwayTeam 198</th>\n",
       "      <th>AwayTeam 199</th>\n",
       "      <th>AwayTeam 200</th>\n",
       "      <th>AwayTeam 201</th>\n",
       "      <th>AwayTeam 202</th>\n",
       "      <th>AwayTeam 203</th>\n",
       "      <th>AwayTeam 204</th>\n",
       "      <th>AwayTeam 205</th>\n",
       "      <th>AwayTeam 206</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Auxerre</td>\n",
       "      <td>Nice</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>1.727</td>\n",
       "      <td>3.100</td>\n",
       "      <td>4.500</td>\n",
       "      <td>1.66</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Guingamp</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>2.500</td>\n",
       "      <td>2.875</td>\n",
       "      <td>2.625</td>\n",
       "      <td>2.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Hamburg</td>\n",
       "      <td>Hannover</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>1.571</td>\n",
       "      <td>3.500</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.57</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Hertha</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>1.833</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.750</td>\n",
       "      <td>1.72</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>Lens</td>\n",
       "      <td>Le Mans</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "      <td>1.571</td>\n",
       "      <td>3.250</td>\n",
       "      <td>5.500</td>\n",
       "      <td>1.66</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37341</th>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Udinese</td>\n",
       "      <td>Roma</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>3.300</td>\n",
       "      <td>3.300</td>\n",
       "      <td>2.250</td>\n",
       "      <td>3.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37342</th>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>Vallecano</td>\n",
       "      <td>Girona</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>2.750</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.880</td>\n",
       "      <td>2.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37343</th>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Alaves</td>\n",
       "      <td>Celta</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>2.630</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37344</th>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Genoa</td>\n",
       "      <td>Monza</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>2.250</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.600</td>\n",
       "      <td>2.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37345</th>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>Venezia</td>\n",
       "      <td>Verona</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>2.150</td>\n",
       "      <td>3.500</td>\n",
       "      <td>3.250</td>\n",
       "      <td>2.15</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37346 rows × 438 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "cell_type": "markdown",
   "id": "1b0fefa3",
   "metadata": {},
   "source": [
    "**2.2.2 Full Time Result Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "id": "f9442a80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:25.052943Z",
     "start_time": "2025-02-04T03:21:24.987653Z"
    }
   },
   "source": [
    "target_encoder = LabelEncoder()\n",
    "# learning_df_div['Result'] = target_encoder.fit_transform(learning_df_div.easy_label) \n",
    "learning_df_div['Result'] = target_encoder.fit_transform(learning_df_div.FTR)"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "markdown",
   "id": "18c1cb41",
   "metadata": {},
   "source": [
    "**2.2.3 Date Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "id": "f5dfc529",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:25.217948Z",
     "start_time": "2025-02-04T03:21:25.157607Z"
    }
   },
   "source": [
    "learning_df_div['Year'] = pd.DatetimeIndex(learning_df_div.Date).year\n",
    "\n",
    "learning_df_div['Month'] = pd.DatetimeIndex(learning_df_div.Date).month\n",
    "learning_df_div['Sin_Month'] = np.sin(2*np.pi*learning_df_div.Month/12)\n",
    "learning_df_div['Cos_Month'] = np.cos(2*np.pi*learning_df_div.Month/12)\n",
    "\n",
    "learning_df_div['DayofYear'] = pd.DatetimeIndex(learning_df_div.Date).dayofyear\n",
    "learning_df_div['Sin_Day'] = np.sin(2*np.pi*learning_df_div.DayofYear/365)\n",
    "learning_df_div['Cos_Day'] = np.cos(2*np.pi*learning_df_div.DayofYear/365)\n",
    "\n",
    "# 注意 inplace是在原始frame修改，返回值是Nonetype\n",
    "# learning_df = learning_df_div.drop(columns = ['Date','Month'], inplace = True)\n",
    "# learning_df = learning_df_div.drop(columns = ['Date','Month'])\n",
    "learning_df = learning_df_div.drop(columns = ['Date'])\n",
    "# learning_df.drop(columns = ['Date'], inplace = True)"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:25.693048Z",
     "start_time": "2025-02-04T03:21:25.616355Z"
    }
   },
   "cell_type": "code",
   "source": "learning_df",
   "id": "7e7bc241",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        HomeTeam       AwayTeam  FTHG  FTAG FTR  B365H  B365D  B365A   WHH  \\\n",
       "0        Auxerre           Nice   1.0   2.0   A  1.727  3.100  4.500  1.66   \n",
       "1       Guingamp      Marseille   0.0   1.0   A  2.500  2.875  2.625  2.60   \n",
       "2        Hamburg       Hannover   0.0   3.0   A  1.571  3.500  5.000  1.57   \n",
       "3         Hertha  Werder Bremen   0.0   3.0   A  1.833  3.200  3.750  1.72   \n",
       "4           Lens        Le Mans   0.0   0.0   D  1.571  3.250  5.500  1.66   \n",
       "...          ...            ...   ...   ...  ..    ...    ...    ...   ...   \n",
       "37341    Udinese           Roma   1.0   2.0   A  3.300  3.300  2.250  3.30   \n",
       "37342  Vallecano         Girona   2.0   1.0   H  2.750  2.900  2.880  2.75   \n",
       "37343     Alaves          Celta   1.0   1.0   D  2.630  3.000  3.000  2.62   \n",
       "37344      Genoa          Monza   2.0   0.0   H  2.250  3.000  3.600  2.25   \n",
       "37345    Venezia         Verona   1.0   1.0   D  2.150  3.500  3.250  2.15   \n",
       "\n",
       "       WHD  ...  AwayTeam 205  AwayTeam 206  Result  Year  Month  Sin_Month  \\\n",
       "0      3.1  ...             0             0       0  2003      2   0.866025   \n",
       "1      3.1  ...             0             0       0  2003      2   0.866025   \n",
       "2      3.5  ...             0             0       0  2003      2   0.866025   \n",
       "3      3.4  ...             0             0       0  2003      2   0.866025   \n",
       "4      3.3  ...             0             0       1  2003      2   0.866025   \n",
       "...    ...  ...           ...           ...     ...   ...    ...        ...   \n",
       "37341  3.2  ...             0             0       0  2024      7  -0.500000   \n",
       "37342  3.0  ...             0             0       2  2024      7  -0.500000   \n",
       "37343  3.2  ...             0             0       1  2024      7  -0.500000   \n",
       "37344  3.0  ...             0             0       2  2024      7  -0.500000   \n",
       "37345  3.3  ...             0             0       1  2024      7  -0.500000   \n",
       "\n",
       "       Cos_Month  DayofYear   Sin_Day   Cos_Day  \n",
       "0       0.500000         33  0.538005  0.842942  \n",
       "1       0.500000         33  0.538005  0.842942  \n",
       "2       0.500000         33  0.538005  0.842942  \n",
       "3       0.500000         33  0.538005  0.842942  \n",
       "4       0.500000         33  0.538005  0.842942  \n",
       "...          ...        ...       ...       ...  \n",
       "37341  -0.866025        208 -0.425000 -0.905193  \n",
       "37342  -0.866025        208 -0.425000 -0.905193  \n",
       "37343  -0.866025        209 -0.440519 -0.897743  \n",
       "37344  -0.866025        209 -0.440519 -0.897743  \n",
       "37345  -0.866025        209 -0.440519 -0.897743  \n",
       "\n",
       "[37346 rows x 445 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>WHH</th>\n",
       "      <th>WHD</th>\n",
       "      <th>...</th>\n",
       "      <th>AwayTeam 205</th>\n",
       "      <th>AwayTeam 206</th>\n",
       "      <th>Result</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Sin_Month</th>\n",
       "      <th>Cos_Month</th>\n",
       "      <th>DayofYear</th>\n",
       "      <th>Sin_Day</th>\n",
       "      <th>Cos_Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Auxerre</td>\n",
       "      <td>Nice</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>1.727</td>\n",
       "      <td>3.100</td>\n",
       "      <td>4.500</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>33</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Guingamp</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>2.500</td>\n",
       "      <td>2.875</td>\n",
       "      <td>2.625</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>33</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hamburg</td>\n",
       "      <td>Hannover</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>1.571</td>\n",
       "      <td>3.500</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>33</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hertha</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>1.833</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.750</td>\n",
       "      <td>1.72</td>\n",
       "      <td>3.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>33</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lens</td>\n",
       "      <td>Le Mans</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "      <td>1.571</td>\n",
       "      <td>3.250</td>\n",
       "      <td>5.500</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>33</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37341</th>\n",
       "      <td>Udinese</td>\n",
       "      <td>Roma</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>3.300</td>\n",
       "      <td>3.300</td>\n",
       "      <td>2.250</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>208</td>\n",
       "      <td>-0.425000</td>\n",
       "      <td>-0.905193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37342</th>\n",
       "      <td>Vallecano</td>\n",
       "      <td>Girona</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>2.750</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.880</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>208</td>\n",
       "      <td>-0.425000</td>\n",
       "      <td>-0.905193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37343</th>\n",
       "      <td>Alaves</td>\n",
       "      <td>Celta</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>2.630</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>209</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37344</th>\n",
       "      <td>Genoa</td>\n",
       "      <td>Monza</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>2.250</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.600</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>209</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37345</th>\n",
       "      <td>Venezia</td>\n",
       "      <td>Verona</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>2.150</td>\n",
       "      <td>3.500</td>\n",
       "      <td>3.250</td>\n",
       "      <td>2.15</td>\n",
       "      <td>3.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>209</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37346 rows × 445 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:25.858252Z",
     "start_time": "2025-02-04T03:21:25.835749Z"
    }
   },
   "cell_type": "code",
   "source": "# For Test\n",
   "id": "866fbac3337d0235",
   "outputs": [],
   "execution_count": 77
  },
  {
   "cell_type": "markdown",
   "id": "b14da74e",
   "metadata": {},
   "source": [
    "### 2.3 Feature Engineering <br>\n",
    "* $\\phi(x)$ feature transformation $\\Rightarrow$ last match result, win/loss streak to date, wins to season date\n",
    "* $\\phi(x)$ feature engineering $\\Rightarrow$ average the home, away, and draw odds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843342cb",
   "metadata": {},
   "source": [
    "**2.3.1 Last Match Result** <br>\n",
    "Indicate the result from the last match played between both teams"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 定义一个函数来计算两队之间上一场比赛的结果\n",
    "def compute_last_matches(df):\n",
    "    \n",
    "    unique_matchups = list(set((list(zip(df.HomeTeam, df.AwayTeam)))))\n",
    "    df['Last Match Result'] = np.nan\n",
    "    for home, away in unique_matchups:\n",
    "        matchup_df = df[(df.HomeTeam == home) & (df.AwayTeam == away)]\n",
    "        # 使用 shift(1) 方法将 FTR（全场比赛结果）列中的数据向下移动一行，这样每行的 last_match_result 将对应于这两队之前的一场比赛的结果。fill_value='Na' 确保了数据移动后空出的位置填充为 'Na'。\n",
    "        # last_match_result = matchup_df.FTR.shift(1, fill_value='Na')\n",
    "        last_match_result = matchup_df.easy_label.shift(1, fill_value='Na')\n",
    "        df.loc[matchup_df.index, 'Last Match Result'] = last_match_result\n",
    "        \n",
    "    lmr_encoder = LabelEncoder()\n",
    "    df['Last Match Result'] = lmr_encoder.fit_transform(df['Last Match Result'])\n",
    "    df.drop(columns = ['easy_label'], inplace = True)\n",
    "    return df"
   ],
   "id": "4b25dbdd59e2d17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:26.055Z",
     "start_time": "2025-02-04T03:21:26.011969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_last_n_matches(df, n=5):\n",
    "    unique_matchups = list(set(zip(df.HomeTeam, df.AwayTeam)))\n",
    "    df['Last 5 Match Results'] = np.nan  # 新增一列用于存储过去 5 场比赛的结果\n",
    "    \n",
    "    for home, away in unique_matchups:\n",
    "        matchup_df = df[(df.HomeTeam == home) & (df.AwayTeam == away)]\n",
    "        \n",
    "        # 获取过去 n 场比赛的结果\n",
    "        # last_n_results = [matchup_df.FTR.shift(i, fill_value='Na') for i in range(1, n+1)]\n",
    "        last_n_results = [matchup_df.easy_label.shift(i, fill_value='Na') for i in range(1, n+1)]\n",
    "        \n",
    "        # 将计算得到的过去 n 场比赛的结果合并为一个字符串或列表，取决于需求\n",
    "        # 这里使用字符串形式：'result1/result2/...'\n",
    "        matchup_df['Last 5 Match Results'] = pd.DataFrame(last_n_results).T.apply(lambda x: '/'.join(x), axis=1)\n",
    "        \n",
    "        # 将计算得到的结果更新回原始 df 中\n",
    "        df.loc[matchup_df.index, 'Last 5 Match Results'] = matchup_df['Last 5 Match Results']\n",
    "    \n",
    "    # 对 Last 5 Match Results 列进行标签编码\n",
    "    lmr_encoder = LabelEncoder()\n",
    "    df['Last 5 Match Results'] = lmr_encoder.fit_transform(df['Last 5 Match Results'])\n",
    "    \n",
    "    # 删除原始的 FTR 列\n",
    "    df.drop(columns=['easy_label'], inplace=True)\n",
    "    \n",
    "    return df\n"
   ],
   "id": "bee90a274731b07e",
   "outputs": [],
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "id": "751bdc97",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:49.302461Z",
     "start_time": "2025-02-04T03:21:26.359188Z"
    }
   },
   "source": [
    "# 定义一个函数来计算两队之间上一场比赛的结果\n",
    "def compute_last_matches(df):\n",
    "    \n",
    "    unique_matchups = list(set((list(zip(df.HomeTeam, df.AwayTeam)))))\n",
    "    df['Last Match Result'] = np.nan\n",
    "    for home, away in unique_matchups:\n",
    "        matchup_df = df[(df.HomeTeam == home) & (df.AwayTeam == away)]\n",
    "        # 使用 shift(1) 方法将 FTR（全场比赛结果）列中的数据向下移动一行，这样每行的 last_match_result 将对应于这两队之前的一场比赛的结果。fill_value='Na' 确保了数据移动后空出的位置填充为 'Na'。\n",
    "        # last_match_result = matchup_df.FTR.shift(1, fill_value='Na')\n",
    "        # 因为easy_label 不适合作为上次比较结果\n",
    "        last_match_result = matchup_df.Result.shift(1, fill_value=3)\n",
    "        # last_match_result = matchup_df.Result_FTR.shift(1, fill_value=3)\n",
    "        df.loc[matchup_df.index, 'Last Match Result'] = last_match_result\n",
    "        \n",
    "    lmr_encoder = LabelEncoder()\n",
    "    df['Last Match Result'] = lmr_encoder.fit_transform(df['Last Match Result'])\n",
    "    df.drop(columns = ['easy_label'], inplace = True)\n",
    "    df.drop(columns = ['FTR'], inplace = True)\n",
    "    return df\n",
    "learning_df = compute_last_matches(learning_df)\n",
    "# learning_df.drop(columns = ['FTR'], inplace = True)"
   ],
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:49.341394Z",
     "start_time": "2025-02-04T03:21:49.303466Z"
    }
   },
   "cell_type": "code",
   "source": "learning_df",
   "id": "39989f3e6bc102f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        HomeTeam       AwayTeam  FTHG  FTAG  B365H  B365D  B365A   WHH  WHD  \\\n",
       "0        Auxerre           Nice   1.0   2.0  1.727  3.100  4.500  1.66  3.1   \n",
       "1       Guingamp      Marseille   0.0   1.0  2.500  2.875  2.625  2.60  3.1   \n",
       "2        Hamburg       Hannover   0.0   3.0  1.571  3.500  5.000  1.57  3.5   \n",
       "3         Hertha  Werder Bremen   0.0   3.0  1.833  3.200  3.750  1.72  3.4   \n",
       "4           Lens        Le Mans   0.0   0.0  1.571  3.250  5.500  1.66  3.3   \n",
       "...          ...            ...   ...   ...    ...    ...    ...   ...  ...   \n",
       "37341    Udinese           Roma   1.0   2.0  3.300  3.300  2.250  3.30  3.2   \n",
       "37342  Vallecano         Girona   2.0   1.0  2.750  2.900  2.880  2.75  3.0   \n",
       "37343     Alaves          Celta   1.0   1.0  2.630  3.000  3.000  2.62  3.2   \n",
       "37344      Genoa          Monza   2.0   0.0  2.250  3.000  3.600  2.25  3.0   \n",
       "37345    Venezia         Verona   1.0   1.0  2.150  3.500  3.250  2.15  3.3   \n",
       "\n",
       "        WHA  ...  AwayTeam 206  Result  Year  Month  Sin_Month  Cos_Month  \\\n",
       "0      5.00  ...             0       0  2003      2   0.866025   0.500000   \n",
       "1      2.40  ...             0       0  2003      2   0.866025   0.500000   \n",
       "2      5.00  ...             0       0  2003      2   0.866025   0.500000   \n",
       "3      4.00  ...             0       0  2003      2   0.866025   0.500000   \n",
       "4      4.50  ...             0       1  2003      2   0.866025   0.500000   \n",
       "...     ...  ...           ...     ...   ...    ...        ...        ...   \n",
       "37341  2.20  ...             0       0  2024      7  -0.500000  -0.866025   \n",
       "37342  2.80  ...             0       2  2024      7  -0.500000  -0.866025   \n",
       "37343  2.75  ...             0       1  2024      7  -0.500000  -0.866025   \n",
       "37344  3.40  ...             0       2  2024      7  -0.500000  -0.866025   \n",
       "37345  3.25  ...             0       1  2024      7  -0.500000  -0.866025   \n",
       "\n",
       "       DayofYear   Sin_Day   Cos_Day  Last Match Result  \n",
       "0             33  0.538005  0.842942                  3  \n",
       "1             33  0.538005  0.842942                  3  \n",
       "2             33  0.538005  0.842942                  3  \n",
       "3             33  0.538005  0.842942                  3  \n",
       "4             33  0.538005  0.842942                  3  \n",
       "...          ...       ...       ...                ...  \n",
       "37341        208 -0.425000 -0.905193                  0  \n",
       "37342        208 -0.425000 -0.905193                  0  \n",
       "37343        209 -0.440519 -0.897743                  2  \n",
       "37344        209 -0.440519 -0.897743                  0  \n",
       "37345        209 -0.440519 -0.897743                  0  \n",
       "\n",
       "[37346 rows x 444 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>WHH</th>\n",
       "      <th>WHD</th>\n",
       "      <th>WHA</th>\n",
       "      <th>...</th>\n",
       "      <th>AwayTeam 206</th>\n",
       "      <th>Result</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Sin_Month</th>\n",
       "      <th>Cos_Month</th>\n",
       "      <th>DayofYear</th>\n",
       "      <th>Sin_Day</th>\n",
       "      <th>Cos_Day</th>\n",
       "      <th>Last Match Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Auxerre</td>\n",
       "      <td>Nice</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.727</td>\n",
       "      <td>3.100</td>\n",
       "      <td>4.500</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>33</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Guingamp</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.500</td>\n",
       "      <td>2.875</td>\n",
       "      <td>2.625</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>33</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hamburg</td>\n",
       "      <td>Hannover</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.571</td>\n",
       "      <td>3.500</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>33</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hertha</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.833</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.750</td>\n",
       "      <td>1.72</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>33</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lens</td>\n",
       "      <td>Le Mans</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.571</td>\n",
       "      <td>3.250</td>\n",
       "      <td>5.500</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>33</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37341</th>\n",
       "      <td>Udinese</td>\n",
       "      <td>Roma</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.300</td>\n",
       "      <td>3.300</td>\n",
       "      <td>2.250</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>208</td>\n",
       "      <td>-0.425000</td>\n",
       "      <td>-0.905193</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37342</th>\n",
       "      <td>Vallecano</td>\n",
       "      <td>Girona</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.750</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.880</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>208</td>\n",
       "      <td>-0.425000</td>\n",
       "      <td>-0.905193</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37343</th>\n",
       "      <td>Alaves</td>\n",
       "      <td>Celta</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.630</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>209</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37344</th>\n",
       "      <td>Genoa</td>\n",
       "      <td>Monza</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.250</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.600</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>209</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37345</th>\n",
       "      <td>Venezia</td>\n",
       "      <td>Verona</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.150</td>\n",
       "      <td>3.500</td>\n",
       "      <td>3.250</td>\n",
       "      <td>2.15</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>209</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37346 rows × 444 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "id": "8ff554cf",
   "metadata": {},
   "source": [
    "**2.3.2 Home and Away Win/Loss Streak** <br>\n",
    "Important note about this feature: the win/loss streak is the teams *home* and *away* win streak, *not* its ***consecutive*** win/loss streak."
   ]
  },
  {
   "cell_type": "code",
   "id": "41f1fcc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:49.354749Z",
     "start_time": "2025-02-04T03:21:49.342399Z"
    }
   },
   "source": [
    "# https://stackoverflow.com/questions/52976336/compute-winning-streak-with-pandas\n",
    "# https://joshdevlin.com/blog/calculate-streaks-in-pandas/"
   ],
   "outputs": [],
   "execution_count": 81
  },
  {
   "cell_type": "code",
   "id": "4658caa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:49.372215Z",
     "start_time": "2025-02-04T03:21:49.355758Z"
    }
   },
   "source": [
    "def compute_winstreak(df):\n",
    "    \n",
    "    years = df.Year.unique()\n",
    "    df_lst = []    \n",
    "    for year in years:\n",
    "        \n",
    "        year_df = df[df.Year == year]\n",
    "        year_df['HomeWin'] = year_df.Result.replace([0, 1, 2], [0, 0, 1])\n",
    "        year_df['AwayWin'] = year_df.Result.replace([0, 1, 2], [1, 0, 0])  \n",
    "        # year_df['HomeWin'] = year_df.Result_FTR.replace([0, 1, 2], [0, 0, 1])\n",
    "        # year_df['AwayWin'] = year_df.Result_FTR.replace([0, 1, 2], [1, 0, 0])\n",
    "        year_df['HomeWinStreak'] = None\n",
    "        year_df['AwayWinStreak'] = None\n",
    "        \n",
    "        hometeams = year_df.HomeTeam.unique()\n",
    "        awayteams = year_df.AwayTeam.unique()\n",
    "        if year > 2024:\n",
    "            # 将 AwayWin = 3 当作 0 来处理，保持计算连胜记录\n",
    "            year_df['HomeWin'] = year_df['HomeWin'].replace(3, 0)\n",
    "            year_df['AwayWin'] = year_df['AwayWin'].replace(3, 0)\n",
    "        \n",
    "        for team in hometeams:\n",
    "            team_df = year_df[(year_df.HomeTeam == team)]\n",
    "            team_df = team_df.sort_values(['Year', 'DayofYear'], ascending = (True, True))\n",
    "            \n",
    "            home_win_streak = 0  # 初始化连胜场数\n",
    "            streaks = []  # 用来存储每场比赛的连续胜利次数\n",
    "            for idx, row in team_df.iterrows():\n",
    "                streaks.append(home_win_streak)  # 当前场次视为未进行，记录上一场的连胜次数\n",
    "                # 计算当前场次的连胜，忽略当前比赛的胜负\n",
    "                if row['HomeWin'] == 1:  # 如果上一场比赛主队赢\n",
    "                    home_win_streak += 1  # 连胜场数递增\n",
    "                else:  # 如果上一场比赛主队输了\n",
    "                    home_win_streak = 0  # 连胜场数重置为 0\n",
    "            # 将计算出的连胜场数赋值到 DataFrame 中\n",
    "            team_df['HomeWinStreak'] = streaks\n",
    "            # 将更新后的数据回写到原 DataFrame\n",
    "            year_df.loc[team_df.index, 'HomeWinStreak'] = team_df.HomeWinStreak\n",
    "            \n",
    "            # team_grouper = (team_df.HomeWin != team_df.HomeWin.shift()).cumsum()\n",
    "            # team_df['HomeWinStreak'] = team_df[['HomeWin']].groupby(team_grouper).cumsum()\n",
    "            # team_df.loc[team_df.HomeWinStreak >0, 'HomeWinStreak'] -= 1\n",
    "            # year_df.loc[team_df.index, 'HomeWinStreak'] = team_df.HomeWinStreak\n",
    "            \n",
    "        for team in awayteams:\n",
    "            team_df = year_df[(year_df.AwayTeam == team)]\n",
    "            team_df = team_df.sort_values(['Year', 'DayofYear'], ascending = (True, True))\n",
    "\n",
    "            away_win_streak = 0  # 初始化连胜场数\n",
    "            streaks = []  # 用来存储每场比赛的连续胜利次数\n",
    "            for idx, row in team_df.iterrows():\n",
    "                streaks.append(away_win_streak)  # 当前场次视为未进行，记录上一场的连胜次数\n",
    "                # 计算当前场次的连胜，忽略当前比赛的胜负\n",
    "                if row['AwayWin'] == 1:  # 如果上一场比赛主队赢\n",
    "                    away_win_streak += 1  # 连胜场数递增\n",
    "                else:  # 如果上一场比赛主队输了\n",
    "                    away_win_streak = 0  # 连胜场数重置为 0\n",
    "            # 将计算出的连胜场数赋值到 DataFrame 中\n",
    "            team_df['AwayWinStreak'] = streaks\n",
    "            # 将更新后的数据回写到原 DataFrame\n",
    "            year_df.loc[team_df.index, 'AwayWinStreak'] = team_df.AwayWinStreak\n",
    "            \n",
    "            # team_grouper = (team_df.AwayWin != team_df.AwayWin.shift()).cumsum()\n",
    "            # team_df['AwayWinStreak'] = team_df[['AwayWin']].groupby(team_grouper).cumsum()\n",
    "            # team_df.loc[team_df.AwayWinStreak >0, 'AwayWinStreak'] -= 1\n",
    "            # year_df.loc[team_df.index, 'AwayWinStreak'] = team_df.AwayWinStreak\n",
    "            \n",
    "        df_lst.append(year_df)\n",
    "        \n",
    "    return pd.concat(df_lst, axis = 0).drop(columns = ['HomeWin', 'AwayWin'])#,'DayofYear'])"
   ],
   "outputs": [],
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "id": "aca53647",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:57.567479Z",
     "start_time": "2025-02-04T03:21:49.373220Z"
    }
   },
   "source": [
    "learning_df = compute_winstreak(learning_df)\n",
    "learning_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        HomeTeam       AwayTeam  FTHG  FTAG  B365H  B365D  B365A   WHH  WHD  \\\n",
       "0        Auxerre           Nice   1.0   2.0  1.727  3.100  4.500  1.66  3.1   \n",
       "1       Guingamp      Marseille   0.0   1.0  2.500  2.875  2.625  2.60  3.1   \n",
       "2        Hamburg       Hannover   0.0   3.0  1.571  3.500  5.000  1.57  3.5   \n",
       "3         Hertha  Werder Bremen   0.0   3.0  1.833  3.200  3.750  1.72  3.4   \n",
       "4           Lens        Le Mans   0.0   0.0  1.571  3.250  5.500  1.66  3.3   \n",
       "...          ...            ...   ...   ...    ...    ...    ...   ...  ...   \n",
       "37341    Udinese           Roma   1.0   2.0  3.300  3.300  2.250  3.30  3.2   \n",
       "37342  Vallecano         Girona   2.0   1.0  2.750  2.900  2.880  2.75  3.0   \n",
       "37343     Alaves          Celta   1.0   1.0  2.630  3.000  3.000  2.62  3.2   \n",
       "37344      Genoa          Monza   2.0   0.0  2.250  3.000  3.600  2.25  3.0   \n",
       "37345    Venezia         Verona   1.0   1.0  2.150  3.500  3.250  2.15  3.3   \n",
       "\n",
       "        WHA  ...  Year  Month  Sin_Month  Cos_Month  DayofYear   Sin_Day  \\\n",
       "0      5.00  ...  2003      2   0.866025   0.500000         33  0.538005   \n",
       "1      2.40  ...  2003      2   0.866025   0.500000         33  0.538005   \n",
       "2      5.00  ...  2003      2   0.866025   0.500000         33  0.538005   \n",
       "3      4.00  ...  2003      2   0.866025   0.500000         33  0.538005   \n",
       "4      4.50  ...  2003      2   0.866025   0.500000         33  0.538005   \n",
       "...     ...  ...   ...    ...        ...        ...        ...       ...   \n",
       "37341  2.20  ...  2024      7  -0.500000  -0.866025        208 -0.425000   \n",
       "37342  2.80  ...  2024      7  -0.500000  -0.866025        208 -0.425000   \n",
       "37343  2.75  ...  2024      7  -0.500000  -0.866025        209 -0.440519   \n",
       "37344  3.40  ...  2024      7  -0.500000  -0.866025        209 -0.440519   \n",
       "37345  3.25  ...  2024      7  -0.500000  -0.866025        209 -0.440519   \n",
       "\n",
       "        Cos_Day  Last Match Result  HomeWinStreak  AwayWinStreak  \n",
       "0      0.842942                  3              0              0  \n",
       "1      0.842942                  3              0              0  \n",
       "2      0.842942                  3              0              0  \n",
       "3      0.842942                  3              0              0  \n",
       "4      0.842942                  3              0              0  \n",
       "...         ...                ...            ...            ...  \n",
       "37341 -0.905193                  0              0              0  \n",
       "37342 -0.905193                  0              1              1  \n",
       "37343 -0.897743                  2              0              0  \n",
       "37344 -0.897743                  0              1              0  \n",
       "37345 -0.897743                  0              0              0  \n",
       "\n",
       "[37346 rows x 446 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>WHH</th>\n",
       "      <th>WHD</th>\n",
       "      <th>WHA</th>\n",
       "      <th>...</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Sin_Month</th>\n",
       "      <th>Cos_Month</th>\n",
       "      <th>DayofYear</th>\n",
       "      <th>Sin_Day</th>\n",
       "      <th>Cos_Day</th>\n",
       "      <th>Last Match Result</th>\n",
       "      <th>HomeWinStreak</th>\n",
       "      <th>AwayWinStreak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Auxerre</td>\n",
       "      <td>Nice</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.727</td>\n",
       "      <td>3.100</td>\n",
       "      <td>4.500</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>33</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Guingamp</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.500</td>\n",
       "      <td>2.875</td>\n",
       "      <td>2.625</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.40</td>\n",
       "      <td>...</td>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>33</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hamburg</td>\n",
       "      <td>Hannover</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.571</td>\n",
       "      <td>3.500</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>33</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hertha</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.833</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.750</td>\n",
       "      <td>1.72</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>33</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lens</td>\n",
       "      <td>Le Mans</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.571</td>\n",
       "      <td>3.250</td>\n",
       "      <td>5.500</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.50</td>\n",
       "      <td>...</td>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>33</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37341</th>\n",
       "      <td>Udinese</td>\n",
       "      <td>Roma</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.300</td>\n",
       "      <td>3.300</td>\n",
       "      <td>2.250</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.20</td>\n",
       "      <td>...</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>208</td>\n",
       "      <td>-0.425000</td>\n",
       "      <td>-0.905193</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37342</th>\n",
       "      <td>Vallecano</td>\n",
       "      <td>Girona</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.750</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.880</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>...</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>208</td>\n",
       "      <td>-0.425000</td>\n",
       "      <td>-0.905193</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37343</th>\n",
       "      <td>Alaves</td>\n",
       "      <td>Celta</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.630</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.75</td>\n",
       "      <td>...</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>209</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37344</th>\n",
       "      <td>Genoa</td>\n",
       "      <td>Monza</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.250</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.600</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>...</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>209</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37345</th>\n",
       "      <td>Venezia</td>\n",
       "      <td>Verona</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.150</td>\n",
       "      <td>3.500</td>\n",
       "      <td>3.250</td>\n",
       "      <td>2.15</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.25</td>\n",
       "      <td>...</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>209</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37346 rows × 446 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 83
  },
  {
   "cell_type": "markdown",
   "id": "94e1c235",
   "metadata": {},
   "source": [
    "**2.3.4 Season Home/Away Wins to Date** <br>\n",
    "Indicate the number of wins for a team as home and away to date within current season"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "toy = learning_df[(learning_df.Year == 2010) & (learning_df.HomeTeam == 'Barcelona')][['HomeTeam', 'AwayTeam', 'Result']]\n",
    "toy['HomeWin'] = toy.Result.replace([0, 1, 2], [0, 0, 1])\n",
    "toy['HomeWinsToDate'] = toy.HomeWin.cumsum()"
   ],
   "id": "2e5f10c06aee07e5"
  },
  {
   "cell_type": "code",
   "id": "885dc94a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:21:57.584042Z",
     "start_time": "2025-02-04T03:21:57.567479Z"
    }
   },
   "source": [
    "def compute_winstodate(df):\n",
    "    \n",
    "    years = df.Year.unique()\n",
    "    df_lst = []    \n",
    "    for year in years:\n",
    "        \n",
    "        year_df = df[df.Year == year]\n",
    "        year_df['HomeWin'] = year_df.Result.replace([0, 1, 2], [0, 0, 1])\n",
    "        year_df['AwayWin'] = year_df.Result.replace([0, 1, 2], [1, 0, 0]) \n",
    "        # year_df['HomeWin'] = year_df.Result_FTR.replace([0, 1, 2], [0, 0, 1])\n",
    "        # year_df['AwayWin'] = year_df.Result_FTR.replace([0, 1, 2], [1, 0, 0])\n",
    "        year_df['HomeWinsToDate'] = None\n",
    "        year_df['AwayWinsToDate'] = None\n",
    "        \n",
    "        hometeams = year_df.HomeTeam.unique()\n",
    "        awayteams = year_df.AwayTeam.unique()\n",
    "        if year > 2024:\n",
    "            # 将 AwayWin = 3 当作 0 来处理，保持计算连胜记录\n",
    "            year_df['HomeWin'] = year_df['HomeWin'].replace(3, 0)\n",
    "            year_df['AwayWin'] = year_df['AwayWin'].replace(3, 0)\n",
    "            \n",
    "        for team in hometeams:\n",
    "            team_df = year_df[(year_df.HomeTeam == team)]\n",
    "            team_df = team_df.sort_values(['Year', 'DayofYear'], ascending = (True, True))\n",
    "\n",
    "            # 计算截至当前场次之前的累计胜利次数（不包含当前场次）\n",
    "            team_df['HomeWinsToDate'] = team_df.HomeWin.shift(1).cumsum()\n",
    "            # 填充 NaN 值为 0，因为第一场比赛没有上一场比赛的数据\n",
    "            team_df['HomeWinsToDate'].fillna(0, inplace=True)\n",
    "            # 将更新后的数据回写到原 DataFrame\n",
    "            year_df.loc[team_df.index, 'HomeWinsToDate'] = team_df.HomeWinsToDate\n",
    "    \n",
    "            # team_df['HomeWinsToDate'] = team_df.HomeWin.cumsum()\n",
    "            # year_df.loc[team_df.index, 'HomeWinsToDate'] = team_df.HomeWinsToDate\n",
    "            \n",
    "        for team in awayteams:\n",
    "            team_df = year_df[(year_df.AwayTeam == team)]\n",
    "            team_df = team_df.sort_values(['Year', 'DayofYear'], ascending = (True, True))\n",
    "            \n",
    "            # 计算截至当前场次之前的累计胜利次数（不包含当前场次）\n",
    "            team_df['AwayWinsToDate'] = team_df.AwayWin.shift(1).cumsum()\n",
    "            # 填充 NaN 值为 0，因为第一场比赛没有上一场比赛的数据\n",
    "            team_df['AwayWinsToDate'].fillna(0, inplace=True)\n",
    "            # 将更新后的数据回写到原 DataFrame\n",
    "            year_df.loc[team_df.index, 'AwayWinsToDate'] = team_df.AwayWinsToDate\n",
    "            \n",
    "            # team_df['AwayWinsToDate'] = team_df.AwayWin.cumsum()\n",
    "            # year_df.loc[team_df.index, 'AwayWinsToDate'] = team_df.AwayWinsToDate\n",
    "            \n",
    "        df_lst.append(year_df)\n",
    "        \n",
    "    return pd.concat(df_lst, axis = 0).drop(columns = ['HomeWin', 'AwayWin','DayofYear'])"
   ],
   "outputs": [],
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "id": "b60fbbf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:04.549167Z",
     "start_time": "2025-02-04T03:21:57.584550Z"
    }
   },
   "source": [
    "learning_df = compute_winstodate(learning_df)\n",
    "learning_df.drop(columns = ['HomeTeam', 'AwayTeam'], inplace = True)"
   ],
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:04.587744Z",
     "start_time": "2025-02-04T03:22:04.550177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# learning_df\n",
    "learning_df"
   ],
   "id": "b23f5c1f7a05744b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       FTHG  FTAG  B365H  B365D  B365A   WHH  WHD   WHA   AHh  B365AHH  ...  \\\n",
       "0       1.0   2.0  1.727  3.100  4.500  1.66  3.1  5.00 -0.75    2.050  ...   \n",
       "1       0.0   1.0  2.500  2.875  2.625  2.60  3.1  2.40  0.00    1.925  ...   \n",
       "2       0.0   3.0  1.571  3.500  5.000  1.57  3.5  5.00 -0.75    1.800  ...   \n",
       "3       0.0   3.0  1.833  3.200  3.750  1.72  3.4  4.00 -0.75    2.025  ...   \n",
       "4       0.0   0.0  1.571  3.250  5.500  1.66  3.3  4.50 -0.75    1.900  ...   \n",
       "...     ...   ...    ...    ...    ...   ...  ...   ...   ...      ...  ...   \n",
       "37341   1.0   2.0  3.300  3.300  2.250  3.30  3.2  2.20  0.25    1.990  ...   \n",
       "37342   2.0   1.0  2.750  2.900  2.880  2.75  3.0  2.80  0.00    1.910  ...   \n",
       "37343   1.0   1.0  2.630  3.000  3.000  2.62  3.2  2.75  0.00    1.890  ...   \n",
       "37344   2.0   0.0  2.250  3.000  3.600  2.25  3.0  3.40 -0.25    1.950  ...   \n",
       "37345   1.0   1.0  2.150  3.500  3.250  2.15  3.3  3.25 -0.25    1.920  ...   \n",
       "\n",
       "       Month  Sin_Month  Cos_Month   Sin_Day   Cos_Day  Last Match Result  \\\n",
       "0          2   0.866025   0.500000  0.538005  0.842942                  3   \n",
       "1          2   0.866025   0.500000  0.538005  0.842942                  3   \n",
       "2          2   0.866025   0.500000  0.538005  0.842942                  3   \n",
       "3          2   0.866025   0.500000  0.538005  0.842942                  3   \n",
       "4          2   0.866025   0.500000  0.538005  0.842942                  3   \n",
       "...      ...        ...        ...       ...       ...                ...   \n",
       "37341      7  -0.500000  -0.866025 -0.425000 -0.905193                  0   \n",
       "37342      7  -0.500000  -0.866025 -0.425000 -0.905193                  0   \n",
       "37343      7  -0.500000  -0.866025 -0.440519 -0.897743                  2   \n",
       "37344      7  -0.500000  -0.866025 -0.440519 -0.897743                  0   \n",
       "37345      7  -0.500000  -0.866025 -0.440519 -0.897743                  0   \n",
       "\n",
       "       HomeWinStreak  AwayWinStreak  HomeWinsToDate  AwayWinsToDate  \n",
       "0                  0              0             0.0             0.0  \n",
       "1                  0              0             0.0             0.0  \n",
       "2                  0              0             0.0             0.0  \n",
       "3                  0              0             0.0             0.0  \n",
       "4                  0              0             0.0             0.0  \n",
       "...              ...            ...             ...             ...  \n",
       "37341              0              0             4.0             0.0  \n",
       "37342              1              1             3.0             3.0  \n",
       "37343              0              0             3.0             1.0  \n",
       "37344              1              0             1.0             1.0  \n",
       "37345              0              0             3.0             3.0  \n",
       "\n",
       "[37346 rows x 445 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>WHH</th>\n",
       "      <th>WHD</th>\n",
       "      <th>WHA</th>\n",
       "      <th>AHh</th>\n",
       "      <th>B365AHH</th>\n",
       "      <th>...</th>\n",
       "      <th>Month</th>\n",
       "      <th>Sin_Month</th>\n",
       "      <th>Cos_Month</th>\n",
       "      <th>Sin_Day</th>\n",
       "      <th>Cos_Day</th>\n",
       "      <th>Last Match Result</th>\n",
       "      <th>HomeWinStreak</th>\n",
       "      <th>AwayWinStreak</th>\n",
       "      <th>HomeWinsToDate</th>\n",
       "      <th>AwayWinsToDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.727</td>\n",
       "      <td>3.100</td>\n",
       "      <td>4.500</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>2.050</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.500</td>\n",
       "      <td>2.875</td>\n",
       "      <td>2.625</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.925</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.571</td>\n",
       "      <td>3.500</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.800</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.833</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.750</td>\n",
       "      <td>1.72</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.00</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>2.025</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.571</td>\n",
       "      <td>3.250</td>\n",
       "      <td>5.500</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.50</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.900</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37341</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.300</td>\n",
       "      <td>3.300</td>\n",
       "      <td>2.250</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.990</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.425000</td>\n",
       "      <td>-0.905193</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37342</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.750</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.880</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.910</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.425000</td>\n",
       "      <td>-0.905193</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37343</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.630</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.890</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37344</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.250</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.600</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.950</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37345</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.150</td>\n",
       "      <td>3.500</td>\n",
       "      <td>3.250</td>\n",
       "      <td>2.15</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.920</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37346 rows × 445 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:05.706060Z",
     "start_time": "2025-02-04T03:22:04.588749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 保存为pkl文件\n",
    "learning_df.to_pickle('E:/Data/PKL/learning_df.pkl')"
   ],
   "id": "749b75afd7a2bc77",
   "outputs": [],
   "execution_count": 87
  },
  {
   "cell_type": "markdown",
   "id": "36c2496f",
   "metadata": {},
   "source": [
    "**2.3.5 Website Odds** <br>\n",
    "The `betting odds` recorded by various betting websites offer insight into sentiment surrounding the outcome of a particular game. "
   ]
  },
  {
   "cell_type": "code",
   "id": "1efc8290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:05.762381Z",
     "start_time": "2025-02-04T03:22:05.711585Z"
    }
   },
   "source": [
    "# betting_feats = ['B365H', 'B365D', 'B365A', 'IWH', 'IWD', 'IWA', 'WHH', 'WHD', 'WHA', \"AHh\", \"B365AHH\", \"B365AHA\"]\n",
    "betting_feats = ['B365H', 'B365D', 'B365A']\n",
    "betting_feats"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B365H', 'B365D', 'B365A']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "id": "031548ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:05.808193Z",
     "start_time": "2025-02-04T03:22:05.762892Z"
    }
   },
   "source": [
    "def compute_meanodds(df, betting_feats):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    home_odds = []\n",
    "    away_odds = []\n",
    "    draw_odds = []\n",
    "    for odd in betting_feats:\n",
    "        odd_type = odd[-1]\n",
    "        if odd_type == 'H':\n",
    "            home_odds.append(odd)\n",
    "        elif odd_type == 'A':\n",
    "            away_odds.append(odd)\n",
    "        else:\n",
    "            draw_odds.append(odd)\n",
    "    avg_home_odds = df[home_odds].mean(axis=1)\n",
    "    avg_away_odds = df[away_odds].mean(axis=1)\n",
    "    avg_draw_odds = df[draw_odds].mean(axis=1)\n",
    "    \n",
    "    ordered_cols = ['HomeOdds', 'AwayOdds', 'DrawOdds'] + df.columns.tolist()\n",
    "    \n",
    "    df['HomeOdds'] = avg_home_odds\n",
    "    df['AwayOdds'] = avg_away_odds\n",
    "    df['DrawOdds'] = avg_draw_odds\n",
    "    \n",
    "    return df[ordered_cols]"
   ],
   "outputs": [],
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "id": "08e0a28b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:05.877110Z",
     "start_time": "2025-02-04T03:22:05.809196Z"
    }
   },
   "source": [
    "learning_df = compute_meanodds(learning_df, betting_feats)"
   ],
   "outputs": [],
   "execution_count": 90
  },
  {
   "cell_type": "markdown",
   "id": "0e4768c8",
   "metadata": {},
   "source": [
    "### 2.4 Peek @ Learning DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "id": "b54348f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:05.943633Z",
     "start_time": "2025-02-04T03:22:05.878113Z"
    }
   },
   "source": [
    "learning_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       HomeOdds  AwayOdds  DrawOdds  FTHG  FTAG  B365H  B365D  B365A   WHH  \\\n",
       "0         1.727     4.500     3.100   1.0   2.0  1.727  3.100  4.500  1.66   \n",
       "1         2.500     2.625     2.875   0.0   1.0  2.500  2.875  2.625  2.60   \n",
       "2         1.571     5.000     3.500   0.0   3.0  1.571  3.500  5.000  1.57   \n",
       "3         1.833     3.750     3.200   0.0   3.0  1.833  3.200  3.750  1.72   \n",
       "4         1.571     5.500     3.250   0.0   0.0  1.571  3.250  5.500  1.66   \n",
       "...         ...       ...       ...   ...   ...    ...    ...    ...   ...   \n",
       "37341     3.300     2.250     3.300   1.0   2.0  3.300  3.300  2.250  3.30   \n",
       "37342     2.750     2.880     2.900   2.0   1.0  2.750  2.900  2.880  2.75   \n",
       "37343     2.630     3.000     3.000   1.0   1.0  2.630  3.000  3.000  2.62   \n",
       "37344     2.250     3.600     3.000   2.0   0.0  2.250  3.000  3.600  2.25   \n",
       "37345     2.150     3.250     3.500   1.0   1.0  2.150  3.500  3.250  2.15   \n",
       "\n",
       "       WHD  ...  Month  Sin_Month  Cos_Month   Sin_Day   Cos_Day  \\\n",
       "0      3.1  ...      2   0.866025   0.500000  0.538005  0.842942   \n",
       "1      3.1  ...      2   0.866025   0.500000  0.538005  0.842942   \n",
       "2      3.5  ...      2   0.866025   0.500000  0.538005  0.842942   \n",
       "3      3.4  ...      2   0.866025   0.500000  0.538005  0.842942   \n",
       "4      3.3  ...      2   0.866025   0.500000  0.538005  0.842942   \n",
       "...    ...  ...    ...        ...        ...       ...       ...   \n",
       "37341  3.2  ...      7  -0.500000  -0.866025 -0.425000 -0.905193   \n",
       "37342  3.0  ...      7  -0.500000  -0.866025 -0.425000 -0.905193   \n",
       "37343  3.2  ...      7  -0.500000  -0.866025 -0.440519 -0.897743   \n",
       "37344  3.0  ...      7  -0.500000  -0.866025 -0.440519 -0.897743   \n",
       "37345  3.3  ...      7  -0.500000  -0.866025 -0.440519 -0.897743   \n",
       "\n",
       "       Last Match Result  HomeWinStreak  AwayWinStreak  HomeWinsToDate  \\\n",
       "0                      3              0              0             0.0   \n",
       "1                      3              0              0             0.0   \n",
       "2                      3              0              0             0.0   \n",
       "3                      3              0              0             0.0   \n",
       "4                      3              0              0             0.0   \n",
       "...                  ...            ...            ...             ...   \n",
       "37341                  0              0              0             4.0   \n",
       "37342                  0              1              1             3.0   \n",
       "37343                  2              0              0             3.0   \n",
       "37344                  0              1              0             1.0   \n",
       "37345                  0              0              0             3.0   \n",
       "\n",
       "       AwayWinsToDate  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  \n",
       "...               ...  \n",
       "37341             0.0  \n",
       "37342             3.0  \n",
       "37343             1.0  \n",
       "37344             1.0  \n",
       "37345             3.0  \n",
       "\n",
       "[37346 rows x 448 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomeOdds</th>\n",
       "      <th>AwayOdds</th>\n",
       "      <th>DrawOdds</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>WHH</th>\n",
       "      <th>WHD</th>\n",
       "      <th>...</th>\n",
       "      <th>Month</th>\n",
       "      <th>Sin_Month</th>\n",
       "      <th>Cos_Month</th>\n",
       "      <th>Sin_Day</th>\n",
       "      <th>Cos_Day</th>\n",
       "      <th>Last Match Result</th>\n",
       "      <th>HomeWinStreak</th>\n",
       "      <th>AwayWinStreak</th>\n",
       "      <th>HomeWinsToDate</th>\n",
       "      <th>AwayWinsToDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.727</td>\n",
       "      <td>4.500</td>\n",
       "      <td>3.100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.727</td>\n",
       "      <td>3.100</td>\n",
       "      <td>4.500</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.500</td>\n",
       "      <td>2.625</td>\n",
       "      <td>2.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.500</td>\n",
       "      <td>2.875</td>\n",
       "      <td>2.625</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.571</td>\n",
       "      <td>5.000</td>\n",
       "      <td>3.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.571</td>\n",
       "      <td>3.500</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.833</td>\n",
       "      <td>3.750</td>\n",
       "      <td>3.200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.833</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.750</td>\n",
       "      <td>1.72</td>\n",
       "      <td>3.4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.571</td>\n",
       "      <td>5.500</td>\n",
       "      <td>3.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.571</td>\n",
       "      <td>3.250</td>\n",
       "      <td>5.500</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37341</th>\n",
       "      <td>3.300</td>\n",
       "      <td>2.250</td>\n",
       "      <td>3.300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.300</td>\n",
       "      <td>3.300</td>\n",
       "      <td>2.250</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.2</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.425000</td>\n",
       "      <td>-0.905193</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37342</th>\n",
       "      <td>2.750</td>\n",
       "      <td>2.880</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.750</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.880</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.425000</td>\n",
       "      <td>-0.905193</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37343</th>\n",
       "      <td>2.630</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.630</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37344</th>\n",
       "      <td>2.250</td>\n",
       "      <td>3.600</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.250</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.600</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37345</th>\n",
       "      <td>2.150</td>\n",
       "      <td>3.250</td>\n",
       "      <td>3.500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.150</td>\n",
       "      <td>3.500</td>\n",
       "      <td>3.250</td>\n",
       "      <td>2.15</td>\n",
       "      <td>3.3</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37346 rows × 448 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:05.993080Z",
     "start_time": "2025-02-04T03:22:05.946072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_df.drop(columns = ['WHH', 'WHD', 'WHA', 'HomeOdds', 'AwayOdds', 'DrawOdds', 'FTHG', 'FTAG'], inplace = True)\n",
    "# learning_df.drop(columns = ['WHH', 'WHD', 'WHA', 'HomeOdds', 'AwayOdds', 'DrawOdds', 'FTHG', 'FTAG', 'Result'], inplace = True)"
   ],
   "id": "228a748b486ff190",
   "outputs": [],
   "execution_count": 92
  },
  {
   "cell_type": "markdown",
   "id": "7b46d936",
   "metadata": {},
   "source": [
    "# 3. Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e10788",
   "metadata": {},
   "source": [
    "* Establish a baseline Logistic Regression model fit over the entire learning dataframe without special regard to *division* and *team*. \n",
    "* Train model over 16 seasons, and predict for the remaining 3 seasons (approximate 80-20 split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ddd44e",
   "metadata": {},
   "source": [
    "### 3.1 Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "id": "a82f4a6b",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:06.032196Z",
     "start_time": "2025-02-04T03:22:05.994084Z"
    }
   },
   "source": [
    "split = 0.80\n",
    "no_seasons = 20\n",
    "\n",
    "print('No. seasons to train over: ' + str(round(split*no_seasons)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. seasons to train over: 16\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "id": "f22ecde1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:06.084529Z",
     "start_time": "2025-02-04T03:22:06.033351Z"
    }
   },
   "source": [
    "X, y = learning_df.loc[:, learning_df.columns != 'Result'], learning_df[['Result']]\n",
    "# X, y = learning_df.loc[:, learning_df.columns != 'Result_FTR'], learning_df[['Result_FTR']]"
   ],
   "outputs": [],
   "execution_count": 94
  },
  {
   "cell_type": "code",
   "id": "60373664",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:06.124292Z",
     "start_time": "2025-02-04T03:22:06.085533Z"
    }
   },
   "source": [
    "# full_feat = ['HomeWinStreak','AwayWinStreak','HomeWinsToDate', 'AwayWinsToDate', 'Last Match Result',\n",
    "#              'HomeTeamELO', 'AwayTeamELO', 'HomeOdds', 'AwayOdds', 'DrawOdds'] + betting_feats\n",
    "\n",
    "# exclude_feats = ['HomeWinsToDate', 'AwayWinsToDate', 'Last Match Result'] "
   ],
   "outputs": [],
   "execution_count": 95
  },
  {
   "cell_type": "code",
   "id": "1db1f4fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:06.162332Z",
     "start_time": "2025-02-04T03:22:06.125296Z"
    }
   },
   "source": [
    "# X = X[X.columns[~X.columns.isin(exclude_feats)]]\n",
    "# X"
   ],
   "outputs": [],
   "execution_count": 96
  },
  {
   "cell_type": "code",
   "id": "4c621fff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:06.224382Z",
     "start_time": "2025-02-04T03:22:06.164335Z"
    }
   },
   "source": [
    "X"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       B365H  B365D  B365A   AHh  B365AHH  B365AHA  HomeTeamELO  AwayTeamELO  \\\n",
       "0      1.727  3.100  4.500 -0.75    2.050    1.850  1702.604858  1611.196045   \n",
       "1      2.500  2.875  2.625  0.00    1.925    1.975  1685.016113  1665.625732   \n",
       "2      1.571  3.500  5.000 -0.75    1.800    2.100  1718.566284  1649.805298   \n",
       "3      1.833  3.200  3.750 -0.75    2.025    1.875  1719.916748  1692.120972   \n",
       "4      1.571  3.250  5.500 -0.75    1.900    2.000  1697.354004  1539.958130   \n",
       "...      ...    ...    ...   ...      ...      ...          ...          ...   \n",
       "37341  3.300  3.300  2.250  0.25    1.990    1.940  1667.065796  1772.048584   \n",
       "37342  2.750  2.900  2.880  0.00    1.910    1.990  1635.277222  1704.093872   \n",
       "37343  2.630  3.000  3.000  0.00    1.890    2.010  1621.804321  1637.579956   \n",
       "37344  2.250  3.000  3.600 -0.25    1.950    1.980  1682.297119  1628.680542   \n",
       "37345  2.150  3.500  3.250 -0.25    1.920    2.010  1575.471802  1615.984619   \n",
       "\n",
       "       balance_val  Div 0  ...  Month  Sin_Month  Cos_Month   Sin_Day  \\\n",
       "0               -1      0  ...      2   0.866025   0.500000  0.538005   \n",
       "1                0      0  ...      2   0.866025   0.500000  0.538005   \n",
       "2               -1      1  ...      2   0.866025   0.500000  0.538005   \n",
       "3               -1      1  ...      2   0.866025   0.500000  0.538005   \n",
       "4               -1      0  ...      2   0.866025   0.500000  0.538005   \n",
       "...            ...    ...  ...    ...        ...        ...       ...   \n",
       "37341            1      0  ...      7  -0.500000  -0.866025 -0.425000   \n",
       "37342            0      0  ...      7  -0.500000  -0.866025 -0.425000   \n",
       "37343            0      0  ...      7  -0.500000  -0.866025 -0.440519   \n",
       "37344           -1      0  ...      7  -0.500000  -0.866025 -0.440519   \n",
       "37345           -1      0  ...      7  -0.500000  -0.866025 -0.440519   \n",
       "\n",
       "        Cos_Day  Last Match Result  HomeWinStreak  AwayWinStreak  \\\n",
       "0      0.842942                  3              0              0   \n",
       "1      0.842942                  3              0              0   \n",
       "2      0.842942                  3              0              0   \n",
       "3      0.842942                  3              0              0   \n",
       "4      0.842942                  3              0              0   \n",
       "...         ...                ...            ...            ...   \n",
       "37341 -0.905193                  0              0              0   \n",
       "37342 -0.905193                  0              1              1   \n",
       "37343 -0.897743                  2              0              0   \n",
       "37344 -0.897743                  0              1              0   \n",
       "37345 -0.897743                  0              0              0   \n",
       "\n",
       "       HomeWinsToDate  AwayWinsToDate  \n",
       "0                 0.0             0.0  \n",
       "1                 0.0             0.0  \n",
       "2                 0.0             0.0  \n",
       "3                 0.0             0.0  \n",
       "4                 0.0             0.0  \n",
       "...               ...             ...  \n",
       "37341             4.0             0.0  \n",
       "37342             3.0             3.0  \n",
       "37343             3.0             1.0  \n",
       "37344             1.0             1.0  \n",
       "37345             3.0             3.0  \n",
       "\n",
       "[37346 rows x 439 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>AHh</th>\n",
       "      <th>B365AHH</th>\n",
       "      <th>B365AHA</th>\n",
       "      <th>HomeTeamELO</th>\n",
       "      <th>AwayTeamELO</th>\n",
       "      <th>balance_val</th>\n",
       "      <th>Div 0</th>\n",
       "      <th>...</th>\n",
       "      <th>Month</th>\n",
       "      <th>Sin_Month</th>\n",
       "      <th>Cos_Month</th>\n",
       "      <th>Sin_Day</th>\n",
       "      <th>Cos_Day</th>\n",
       "      <th>Last Match Result</th>\n",
       "      <th>HomeWinStreak</th>\n",
       "      <th>AwayWinStreak</th>\n",
       "      <th>HomeWinsToDate</th>\n",
       "      <th>AwayWinsToDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.727</td>\n",
       "      <td>3.100</td>\n",
       "      <td>4.500</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>2.050</td>\n",
       "      <td>1.850</td>\n",
       "      <td>1702.604858</td>\n",
       "      <td>1611.196045</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.500</td>\n",
       "      <td>2.875</td>\n",
       "      <td>2.625</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.925</td>\n",
       "      <td>1.975</td>\n",
       "      <td>1685.016113</td>\n",
       "      <td>1665.625732</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.571</td>\n",
       "      <td>3.500</td>\n",
       "      <td>5.000</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.800</td>\n",
       "      <td>2.100</td>\n",
       "      <td>1718.566284</td>\n",
       "      <td>1649.805298</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.833</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.750</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>2.025</td>\n",
       "      <td>1.875</td>\n",
       "      <td>1719.916748</td>\n",
       "      <td>1692.120972</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.571</td>\n",
       "      <td>3.250</td>\n",
       "      <td>5.500</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.900</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1697.354004</td>\n",
       "      <td>1539.958130</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.538005</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37341</th>\n",
       "      <td>3.300</td>\n",
       "      <td>3.300</td>\n",
       "      <td>2.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.990</td>\n",
       "      <td>1.940</td>\n",
       "      <td>1667.065796</td>\n",
       "      <td>1772.048584</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.425000</td>\n",
       "      <td>-0.905193</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37342</th>\n",
       "      <td>2.750</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.910</td>\n",
       "      <td>1.990</td>\n",
       "      <td>1635.277222</td>\n",
       "      <td>1704.093872</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.425000</td>\n",
       "      <td>-0.905193</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37343</th>\n",
       "      <td>2.630</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.890</td>\n",
       "      <td>2.010</td>\n",
       "      <td>1621.804321</td>\n",
       "      <td>1637.579956</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37344</th>\n",
       "      <td>2.250</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.600</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.950</td>\n",
       "      <td>1.980</td>\n",
       "      <td>1682.297119</td>\n",
       "      <td>1628.680542</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37345</th>\n",
       "      <td>2.150</td>\n",
       "      <td>3.500</td>\n",
       "      <td>3.250</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.920</td>\n",
       "      <td>2.010</td>\n",
       "      <td>1575.471802</td>\n",
       "      <td>1615.984619</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37346 rows × 439 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:06.278092Z",
     "start_time": "2025-02-04T03:22:06.225388Z"
    }
   },
   "cell_type": "code",
   "source": "y",
   "id": "232473f400896cfa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Result\n",
       "0           0\n",
       "1           0\n",
       "2           0\n",
       "3           0\n",
       "4           1\n",
       "...       ...\n",
       "37341       0\n",
       "37342       2\n",
       "37343       1\n",
       "37344       2\n",
       "37345       1\n",
       "\n",
       "[37346 rows x 1 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37341</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37342</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37343</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37344</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37345</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37346 rows × 1 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:06.323476Z",
     "start_time": "2025-02-04T03:22:06.278092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "split_year = 2024\n",
    "start_year = 2019\n",
    "# start_year = split_year - 15\n",
    "# split_month = 12 - 6"
   ],
   "id": "44dc55d09c257a7d",
   "outputs": [],
   "execution_count": 99
  },
  {
   "cell_type": "code",
   "id": "0e361977",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:06.382783Z",
     "start_time": "2025-02-04T03:22:06.324476Z"
    }
   },
   "source": [
    "# 切分训练集和测试集\n",
    "xTr, xTe = X[(X.Year < split_year) & (X.Year >= start_year)], X[X.Year >= split_year]\n",
    "yTr, yTe = y.loc[xTr.index, :], y.loc[xTe.index, :]\n",
    "# # 切分训练集和测试集\n",
    "# xTr, xTe = X[((X.Year < split_year) & (X.Year >= start_year))\n",
    "#  | ((X.Year == split_year) & (X.Month < split_month)) ], X[(X.Year > split_year) | ((X.Year == split_year) & (X.Month >= split_month)) ]\n",
    "# yTr, yTe = y.loc[xTr.index, :], y.loc[xTe.index, :]"
   ],
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2 Normalization <br>\n",
    "Following our various feature transformations and development, we arrived to a sparse dataframe with the exception of a few features(*Year, DayofYear*). It will be important to *normalize* these features as they are in gross magnitudes compared to the remaining features. During model training, having dominating features (in scale relative to others) can be dangerous as the weight updates may mistakengly favor these larger-scale features because it will have the largest influence on the target output. "
   ],
   "id": "50636cdc8bb898d6"
  },
  {
   "cell_type": "code",
   "id": "a72d269a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:06.429962Z",
     "start_time": "2025-02-04T03:22:06.383788Z"
    }
   },
   "source": [
    "# minmax_scaler.fit_transform()：这个方法首先拟合数据，即计算数据的最小值和最大值，这些值用于后续的缩放。然后，它将这些参数用于转换数据，将原始数据缩放到0和1之间。\n",
    "# minmax_scaler.transform()：这个方法使用在训练数据上计算得到的最小值和最大值来转换测试数据。这确保了训练数据和测试数据使用相同的缩放标准。\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "xTr.loc[:, ['Year']] = minmax_scaler.fit_transform(xTr.loc[:, ['Year']])\n",
    "xTe.loc[:, ['Year']] = minmax_scaler.transform(xTe.loc[:, ['Year']])\n",
    "# 保存到文件\n",
    "import time\n",
    "local_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
    "joblib.dump(minmax_scaler, f'minmax_scaler_{local_time}.pkl')  # 保存为 .pkl 文件"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['minmax_scaler_2025_02_04_11_22_06.pkl']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 101
  },
  {
   "cell_type": "code",
   "id": "11ee9c78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:06.481460Z",
     "start_time": "2025-02-04T03:22:06.430967Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "# to_scale = ['HomeWinStreak','AwayWinStreak','HomeWinsToDate', 'AwayWinsToDate', 'HomeTeamELO', 'AwayTeamELO', 'HomeOdds', 'AwayOdds', 'DrawOdds'] + betting_feats\n",
    "to_scale = ['HomeTeamELO', 'AwayTeamELO'] + betting_feats\n",
    "\n",
    "xTr.loc[:, to_scale] = std_scaler.fit_transform(xTr.loc[:, to_scale])\n",
    "xTe.loc[:, to_scale] = std_scaler.transform(xTe.loc[:, to_scale])\n",
    "# 保存到文件\n",
    "import time\n",
    "local_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
    "joblib.dump(std_scaler, f'std_scaler_{local_time}.pkl')  # 保存为 .pkl 文件"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['std_scaler_2025_02_04_11_22_06.pkl']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 102
  },
  {
   "cell_type": "code",
   "id": "76792bc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:06.564181Z",
     "start_time": "2025-02-04T03:22:06.485463Z"
    }
   },
   "source": [
    "xTr"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          B365H     B365D     B365A   AHh  B365AHH  B365AHA  HomeTeamELO  \\\n",
       "27411 -0.900990  4.955212  4.460696 -2.25     1.96     1.94     2.950768   \n",
       "27412  0.121878 -0.629606 -0.579036  0.00     2.07     1.72    -1.032273   \n",
       "27413 -0.224577 -0.796317 -0.327504 -0.25     2.02     1.88    -1.096578   \n",
       "27414 -0.455547 -0.379540 -0.206284 -0.50     2.01     1.89     0.014493   \n",
       "27415 -0.224577 -0.712962 -0.357809 -0.25     2.05     1.85    -1.494997   \n",
       "...         ...       ...       ...   ...      ...      ...          ...   \n",
       "36342 -0.780005  0.995826  0.975614 -1.50     2.01     1.89     0.833673   \n",
       "36343 -0.747010  0.787437  0.596801 -1.25     1.95     1.95     0.780336   \n",
       "36344  0.671807 -0.046118 -0.751775  0.50     2.06     1.84    -0.120739   \n",
       "36345  1.496700 -0.046118 -0.833598  1.00     1.98     1.92    -0.470864   \n",
       "36346 -0.714014  0.579048  0.521038 -1.25     2.05     1.85     1.682098   \n",
       "\n",
       "       AwayTeamELO  balance_val  Div 0  ...  Month     Sin_Month  Cos_Month  \\\n",
       "27411    -0.480580           -2      0  ...      2  8.660254e-01   0.500000   \n",
       "27412     0.326923            0      0  ...      2  8.660254e-01   0.500000   \n",
       "27413    -1.111298           -1      0  ...      2  8.660254e-01   0.500000   \n",
       "27414    -0.572309           -1      0  ...      2  8.660254e-01   0.500000   \n",
       "27415    -1.621927           -1      0  ...      2  8.660254e-01   0.500000   \n",
       "...            ...          ...    ...  ...    ...           ...        ...   \n",
       "36342    -0.820631           -2      0  ...     11 -5.000000e-01   0.866025   \n",
       "36343    -0.632290           -1      0  ...     11 -5.000000e-01   0.866025   \n",
       "36344     1.501672            1      0  ...     11 -5.000000e-01   0.866025   \n",
       "36345     2.329773            1      0  ...     11 -5.000000e-01   0.866025   \n",
       "36346     0.585685           -1      0  ...     12 -2.449294e-16   1.000000   \n",
       "\n",
       "        Sin_Day   Cos_Day  Last Match Result  HomeWinStreak  AwayWinStreak  \\\n",
       "27411  0.635432  0.772157                  1              0              0   \n",
       "27412  0.635432  0.772157                  2              0              0   \n",
       "27413  0.648630  0.761104                  0              0              0   \n",
       "27414  0.648630  0.761104                  3              0              0   \n",
       "27415  0.648630  0.761104                  0              0              0   \n",
       "...         ...       ...                ...            ...            ...   \n",
       "36342 -0.566702  0.823923                  2              4              0   \n",
       "36343 -0.566702  0.823923                  1              0              0   \n",
       "36344 -0.566702  0.823923                  0              0              1   \n",
       "36345 -0.566702  0.823923                  0              0              1   \n",
       "36346 -0.478734  0.877960                  2              3              1   \n",
       "\n",
       "       HomeWinsToDate  AwayWinsToDate  \n",
       "27411             0.0             0.0  \n",
       "27412             0.0             0.0  \n",
       "27413             0.0             0.0  \n",
       "27414             0.0             0.0  \n",
       "27415             0.0             0.0  \n",
       "...               ...             ...  \n",
       "36342            10.0             2.0  \n",
       "36343             6.0             2.0  \n",
       "36344             6.0            10.0  \n",
       "36345             6.0            15.0  \n",
       "36346            13.0             6.0  \n",
       "\n",
       "[8936 rows x 439 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>AHh</th>\n",
       "      <th>B365AHH</th>\n",
       "      <th>B365AHA</th>\n",
       "      <th>HomeTeamELO</th>\n",
       "      <th>AwayTeamELO</th>\n",
       "      <th>balance_val</th>\n",
       "      <th>Div 0</th>\n",
       "      <th>...</th>\n",
       "      <th>Month</th>\n",
       "      <th>Sin_Month</th>\n",
       "      <th>Cos_Month</th>\n",
       "      <th>Sin_Day</th>\n",
       "      <th>Cos_Day</th>\n",
       "      <th>Last Match Result</th>\n",
       "      <th>HomeWinStreak</th>\n",
       "      <th>AwayWinStreak</th>\n",
       "      <th>HomeWinsToDate</th>\n",
       "      <th>AwayWinsToDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27411</th>\n",
       "      <td>-0.900990</td>\n",
       "      <td>4.955212</td>\n",
       "      <td>4.460696</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>1.96</td>\n",
       "      <td>1.94</td>\n",
       "      <td>2.950768</td>\n",
       "      <td>-0.480580</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.635432</td>\n",
       "      <td>0.772157</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27412</th>\n",
       "      <td>0.121878</td>\n",
       "      <td>-0.629606</td>\n",
       "      <td>-0.579036</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.72</td>\n",
       "      <td>-1.032273</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.635432</td>\n",
       "      <td>0.772157</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27413</th>\n",
       "      <td>-0.224577</td>\n",
       "      <td>-0.796317</td>\n",
       "      <td>-0.327504</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>2.02</td>\n",
       "      <td>1.88</td>\n",
       "      <td>-1.096578</td>\n",
       "      <td>-1.111298</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.648630</td>\n",
       "      <td>0.761104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27414</th>\n",
       "      <td>-0.455547</td>\n",
       "      <td>-0.379540</td>\n",
       "      <td>-0.206284</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>-0.572309</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.648630</td>\n",
       "      <td>0.761104</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27415</th>\n",
       "      <td>-0.224577</td>\n",
       "      <td>-0.712962</td>\n",
       "      <td>-0.357809</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.85</td>\n",
       "      <td>-1.494997</td>\n",
       "      <td>-1.621927</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.648630</td>\n",
       "      <td>0.761104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36342</th>\n",
       "      <td>-0.780005</td>\n",
       "      <td>0.995826</td>\n",
       "      <td>0.975614</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.833673</td>\n",
       "      <td>-0.820631</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.566702</td>\n",
       "      <td>0.823923</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36343</th>\n",
       "      <td>-0.747010</td>\n",
       "      <td>0.787437</td>\n",
       "      <td>0.596801</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.780336</td>\n",
       "      <td>-0.632290</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.566702</td>\n",
       "      <td>0.823923</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36344</th>\n",
       "      <td>0.671807</td>\n",
       "      <td>-0.046118</td>\n",
       "      <td>-0.751775</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2.06</td>\n",
       "      <td>1.84</td>\n",
       "      <td>-0.120739</td>\n",
       "      <td>1.501672</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.566702</td>\n",
       "      <td>0.823923</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36345</th>\n",
       "      <td>1.496700</td>\n",
       "      <td>-0.046118</td>\n",
       "      <td>-0.833598</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.92</td>\n",
       "      <td>-0.470864</td>\n",
       "      <td>2.329773</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.566702</td>\n",
       "      <td>0.823923</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36346</th>\n",
       "      <td>-0.714014</td>\n",
       "      <td>0.579048</td>\n",
       "      <td>0.521038</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.682098</td>\n",
       "      <td>0.585685</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.478734</td>\n",
       "      <td>0.877960</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8936 rows × 439 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 103
  },
  {
   "cell_type": "code",
   "id": "4696b592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:06.630572Z",
     "start_time": "2025-02-04T03:22:06.565184Z"
    }
   },
   "source": [
    "xTe"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          B365H     B365D     B365A   AHh  B365AHH  B365AHA  HomeTeamELO  \\\n",
       "36347 -0.714014 -0.046118  1.127139 -1.00     1.84     2.06     0.585328   \n",
       "36348 -0.235576 -0.546251 -0.388114 -0.25     2.06     1.84     0.131709   \n",
       "36349 -0.428051 -0.712962 -0.054758 -0.50     2.06     1.87    -0.332145   \n",
       "36350  0.011892 -0.671284 -0.500243  0.00     2.03     1.90    -0.865586   \n",
       "36351  2.321593  0.579048 -0.866934  1.25     1.95     1.98    -1.222583   \n",
       "...         ...       ...       ...   ...      ...      ...          ...   \n",
       "37341  0.286857 -0.629606 -0.615402  0.25     1.99     1.94    -0.216969   \n",
       "37342 -0.015604 -0.963028 -0.424480  0.00     1.91     1.99    -0.479657   \n",
       "37343 -0.081596 -0.879673 -0.388114  0.00     1.89     2.01    -0.590991   \n",
       "37344 -0.290569 -0.879673 -0.206284 -0.25     1.95     1.98    -0.091103   \n",
       "37345 -0.345562 -0.462895 -0.312351 -0.25     1.92     2.01    -0.973865   \n",
       "\n",
       "       AwayTeamELO  balance_val  Div 0  ...  Month  Sin_Month  Cos_Month  \\\n",
       "36347    -0.545998           -1      0  ...      2   0.866025   0.500000   \n",
       "36348     0.831459           -1      0  ...      2   0.866025   0.500000   \n",
       "36349    -0.391280           -1      0  ...      2   0.866025   0.500000   \n",
       "36350    -0.138709            0      0  ...      2   0.866025   0.500000   \n",
       "36351     1.549273            1      0  ...      2   0.866025   0.500000   \n",
       "...            ...          ...    ...  ...    ...        ...        ...   \n",
       "37341     0.652390            1      0  ...      7  -0.500000  -0.866025   \n",
       "37342     0.091215            0      0  ...      7  -0.500000  -0.866025   \n",
       "37343    -0.458062            0      0  ...      7  -0.500000  -0.866025   \n",
       "37344    -0.531554           -1      0  ...      7  -0.500000  -0.866025   \n",
       "37345    -0.636398           -1      0  ...      7  -0.500000  -0.866025   \n",
       "\n",
       "        Sin_Day   Cos_Day  Last Match Result  HomeWinStreak  AwayWinStreak  \\\n",
       "36347  0.711657  0.702527                  1              0              0   \n",
       "36348  0.711657  0.702527                  1              0              0   \n",
       "36349  0.723644  0.690173                  1              0              0   \n",
       "36350  0.723644  0.690173                  0              0              0   \n",
       "36351  0.723644  0.690173                  0              0              0   \n",
       "...         ...       ...                ...            ...            ...   \n",
       "37341 -0.425000 -0.905193                  0              0              0   \n",
       "37342 -0.425000 -0.905193                  0              1              1   \n",
       "37343 -0.440519 -0.897743                  2              0              0   \n",
       "37344 -0.440519 -0.897743                  0              1              0   \n",
       "37345 -0.440519 -0.897743                  0              0              0   \n",
       "\n",
       "       HomeWinsToDate  AwayWinsToDate  \n",
       "36347             0.0             0.0  \n",
       "36348             0.0             0.0  \n",
       "36349             0.0             0.0  \n",
       "36350             0.0             0.0  \n",
       "36351             0.0             0.0  \n",
       "...               ...             ...  \n",
       "37341             4.0             0.0  \n",
       "37342             3.0             3.0  \n",
       "37343             3.0             1.0  \n",
       "37344             1.0             1.0  \n",
       "37345             3.0             3.0  \n",
       "\n",
       "[999 rows x 439 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>AHh</th>\n",
       "      <th>B365AHH</th>\n",
       "      <th>B365AHA</th>\n",
       "      <th>HomeTeamELO</th>\n",
       "      <th>AwayTeamELO</th>\n",
       "      <th>balance_val</th>\n",
       "      <th>Div 0</th>\n",
       "      <th>...</th>\n",
       "      <th>Month</th>\n",
       "      <th>Sin_Month</th>\n",
       "      <th>Cos_Month</th>\n",
       "      <th>Sin_Day</th>\n",
       "      <th>Cos_Day</th>\n",
       "      <th>Last Match Result</th>\n",
       "      <th>HomeWinStreak</th>\n",
       "      <th>AwayWinStreak</th>\n",
       "      <th>HomeWinsToDate</th>\n",
       "      <th>AwayWinsToDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36347</th>\n",
       "      <td>-0.714014</td>\n",
       "      <td>-0.046118</td>\n",
       "      <td>1.127139</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.84</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.585328</td>\n",
       "      <td>-0.545998</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.711657</td>\n",
       "      <td>0.702527</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36348</th>\n",
       "      <td>-0.235576</td>\n",
       "      <td>-0.546251</td>\n",
       "      <td>-0.388114</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>2.06</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.131709</td>\n",
       "      <td>0.831459</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.711657</td>\n",
       "      <td>0.702527</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36349</th>\n",
       "      <td>-0.428051</td>\n",
       "      <td>-0.712962</td>\n",
       "      <td>-0.054758</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>2.06</td>\n",
       "      <td>1.87</td>\n",
       "      <td>-0.332145</td>\n",
       "      <td>-0.391280</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.723644</td>\n",
       "      <td>0.690173</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36350</th>\n",
       "      <td>0.011892</td>\n",
       "      <td>-0.671284</td>\n",
       "      <td>-0.500243</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.03</td>\n",
       "      <td>1.90</td>\n",
       "      <td>-0.865586</td>\n",
       "      <td>-0.138709</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.723644</td>\n",
       "      <td>0.690173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36351</th>\n",
       "      <td>2.321593</td>\n",
       "      <td>0.579048</td>\n",
       "      <td>-0.866934</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.98</td>\n",
       "      <td>-1.222583</td>\n",
       "      <td>1.549273</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.723644</td>\n",
       "      <td>0.690173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37341</th>\n",
       "      <td>0.286857</td>\n",
       "      <td>-0.629606</td>\n",
       "      <td>-0.615402</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.94</td>\n",
       "      <td>-0.216969</td>\n",
       "      <td>0.652390</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.425000</td>\n",
       "      <td>-0.905193</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37342</th>\n",
       "      <td>-0.015604</td>\n",
       "      <td>-0.963028</td>\n",
       "      <td>-0.424480</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.479657</td>\n",
       "      <td>0.091215</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.425000</td>\n",
       "      <td>-0.905193</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37343</th>\n",
       "      <td>-0.081596</td>\n",
       "      <td>-0.879673</td>\n",
       "      <td>-0.388114</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.01</td>\n",
       "      <td>-0.590991</td>\n",
       "      <td>-0.458062</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37344</th>\n",
       "      <td>-0.290569</td>\n",
       "      <td>-0.879673</td>\n",
       "      <td>-0.206284</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.98</td>\n",
       "      <td>-0.091103</td>\n",
       "      <td>-0.531554</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37345</th>\n",
       "      <td>-0.345562</td>\n",
       "      <td>-0.462895</td>\n",
       "      <td>-0.312351</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2.01</td>\n",
       "      <td>-0.973865</td>\n",
       "      <td>-0.636398</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 439 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 104
  },
  {
   "cell_type": "markdown",
   "id": "8b0abe45",
   "metadata": {},
   "source": [
    "### 3.3 HomeWins Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "c8002824",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:07.559983Z",
     "start_time": "2025-02-04T03:22:06.631573Z"
    }
   },
   "source": [
    "from sklearn.metrics import accuracy_score"
   ],
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:07.612010Z",
     "start_time": "2025-02-04T03:22:07.561988Z"
    }
   },
   "cell_type": "code",
   "source": "xTr.shape",
   "id": "bdd8d4925bb687b9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8936, 439)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:07.735814Z",
     "start_time": "2025-02-04T03:22:07.613014Z"
    }
   },
   "cell_type": "code",
   "source": "xTe.shape",
   "id": "53f767c6fd6ea837",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 439)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 107
  },
  {
   "cell_type": "code",
   "id": "f13115e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:07.807872Z",
     "start_time": "2025-02-04T03:22:07.736817Z"
    }
   },
   "source": [
    "# training score\n",
    "baseline_Tr = np.full((xTr.shape[0], 1), 2) \n",
    "accuracy_score(yTr.Result.values, baseline_Tr.ravel())"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4309534467323187"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 108
  },
  {
   "cell_type": "code",
   "id": "16d2cf5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:22:07.891327Z",
     "start_time": "2025-02-04T03:22:07.807872Z"
    }
   },
   "source": [
    "# testing score\n",
    "baseline_preds_Te = np.full((xTe.shape[0]  , 1), 2) #predicts home wins all the time\n",
    "accuracy_score(yTe.Result.values, baseline_preds_Te.ravel())"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42742742742742745"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 109
  },
  {
   "cell_type": "markdown",
   "id": "dff87ca8",
   "metadata": {},
   "source": [
    "### 3.4 Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e03fdd6",
   "metadata": {},
   "source": [
    "**3.4.1** $l2$ Regularized"
   ]
  },
  {
   "cell_type": "code",
   "id": "6c5f52f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:23:17.649033Z",
     "start_time": "2025-02-04T03:22:07.892329Z"
    }
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "l2_lr = LogisticRegression(max_iter = 10000, n_jobs=-1).fit(xTr, yTr.values.ravel())"
   ],
   "outputs": [],
   "execution_count": 110
  },
  {
   "cell_type": "code",
   "id": "54140ce0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:23:17.812460Z",
     "start_time": "2025-02-04T03:23:17.650041Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, l2_lr.predict(xTr))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5515890778871978"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "id": "8c59602e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:23:17.846563Z",
     "start_time": "2025-02-04T03:23:17.813466Z"
    }
   },
   "source": [
    "# testing score\n",
    "lr_preds = l2_lr.predict(xTe)\n",
    "accuracy_score(yTe.Result.values, lr_preds)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5315315315315315"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 112
  },
  {
   "cell_type": "markdown",
   "id": "8adcb166",
   "metadata": {},
   "source": [
    "**3.4.1** $l2$ Penalty Tuning"
   ]
  },
  {
   "cell_type": "code",
   "id": "3009cb8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:24:50.302398Z",
     "start_time": "2025-02-04T03:23:17.847569Z"
    }
   },
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "logistic_params = {'C':[0.001,0.01,0.10]}\n",
    "\n",
    "# logistic_randsearch = RandomizedSearchCV(estimator=LogisticRegression(max_iter=10000),\n",
    "#                                          param_distributions=logistic_params,\n",
    "logistic_randsearch = GridSearchCV(estimator=LogisticRegression(max_iter=10000),\n",
    "                                         param_grid=logistic_params,\n",
    "                                         scoring='accuracy',\n",
    "                                         verbose=1,\n",
    "                                         cv=5,\n",
    "                                         n_jobs=-1)\n",
    "\n",
    "logistic_rand_results = logistic_randsearch.fit(xTr, yTr.values.ravel())\n",
    "print(\"Best: %f using %s\" % (logistic_rand_results.best_score_, logistic_rand_results.best_params_))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best: 0.534804 using {'C': 0.001}\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "cell_type": "code",
   "id": "a9231529",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:24:50.315413Z",
     "start_time": "2025-02-04T03:24:50.303405Z"
    }
   },
   "source": [
    "l2_rs = logistic_rand_results.best_estimator_"
   ],
   "outputs": [],
   "execution_count": 114
  },
  {
   "cell_type": "code",
   "id": "9197a0f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:24:50.459404Z",
     "start_time": "2025-02-04T03:24:50.315413Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, l2_rs.predict(xTr))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5363697403760072"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 115
  },
  {
   "cell_type": "code",
   "id": "5c45f431",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:24:50.492247Z",
     "start_time": "2025-02-04T03:24:50.460409Z"
    }
   },
   "source": [
    "# testing score\n",
    "accuracy_score(yTe.Result.values, l2_rs.predict(xTe))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5205205205205206"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 116
  },
  {
   "cell_type": "markdown",
   "id": "febcc06f",
   "metadata": {},
   "source": [
    "**3.4.4** $l1$ Regularized"
   ]
  },
  {
   "cell_type": "code",
   "id": "d9502351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:27:02.056816Z",
     "start_time": "2025-02-04T03:24:50.492247Z"
    }
   },
   "source": [
    "l1_lr = LogisticRegression(penalty='l1', solver='saga', max_iter = 10000, n_jobs=-1).fit(xTr, yTr.values.ravel())"
   ],
   "outputs": [],
   "execution_count": 117
  },
  {
   "cell_type": "code",
   "id": "82d75648",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:27:02.201112Z",
     "start_time": "2025-02-04T03:27:02.058330Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, l1_lr.predict(xTr))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5534914950760967"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 118
  },
  {
   "cell_type": "code",
   "id": "4faea9e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:27:02.244190Z",
     "start_time": "2025-02-04T03:27:02.205622Z"
    }
   },
   "source": [
    "# testing score\n",
    "l1_preds = l1_lr.predict(xTe)\n",
    "accuracy_score(yTe.Result.values, l1_preds)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5335335335335335"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 119
  },
  {
   "cell_type": "markdown",
   "id": "445dcdb4",
   "metadata": {},
   "source": [
    "**3.4.5** Penalty Tuning"
   ]
  },
  {
   "cell_type": "code",
   "id": "3314235e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:31:35.510967Z",
     "start_time": "2025-02-04T03:27:02.244698Z"
    }
   },
   "source": [
    "l1_params = {'C':[0.001,0.01,0.10,0.2,0.3,0.5,0.7,0.8]}\n",
    "\n",
    "# l1_randsearch = RandomizedSearchCV(estimator=LogisticRegression(penalty='l1',solver='saga', max_iter=10000),\n",
    "#                                          param_distributions=l1_params,\n",
    "l1_randsearch = GridSearchCV(estimator=LogisticRegression(penalty='l1',solver='saga', max_iter=10000),\n",
    "                                         param_grid=l1_params,\n",
    "                                         scoring='accuracy',\n",
    "                                         verbose=1,\n",
    "                                         n_jobs=-1,\n",
    "                                         cv=5)\n",
    "\n",
    "l1_rand_results = l1_randsearch.fit(xTr, yTr.values.ravel())\n",
    "print(\"Best: %f using %s\" % (l1_rand_results.best_score_, l1_rand_results.best_params_))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best: 0.534693 using {'C': 0.01}\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "cell_type": "code",
   "id": "f4607e83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:31:35.522705Z",
     "start_time": "2025-02-04T03:31:35.511471Z"
    }
   },
   "source": [
    "l1_rs = l1_randsearch.best_estimator_ #LogisticRegression(C=0.10, solver='saga', max_iter=10000).fit(xTr, yTr.values.ravel())#"
   ],
   "outputs": [],
   "execution_count": 121
  },
  {
   "cell_type": "code",
   "id": "4895d05a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:31:35.674723Z",
     "start_time": "2025-02-04T03:31:35.523277Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, l1_rs.predict(xTr))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5363697403760072"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 122
  },
  {
   "cell_type": "code",
   "id": "7016a84c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:31:35.707275Z",
     "start_time": "2025-02-04T03:31:35.675727Z"
    }
   },
   "source": [
    "# testing score\n",
    "accuracy_score(yTe.Result.values, l1_rs.predict(xTe))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5225225225225225"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 123
  },
  {
   "cell_type": "markdown",
   "id": "8b988c26",
   "metadata": {},
   "source": [
    "### 3.5 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "id": "0f4908d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:31:51.982136Z",
     "start_time": "2025-02-04T03:31:35.708279Z"
    }
   },
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(max_iter=100000).fit(xTr, yTr.values.ravel())"
   ],
   "outputs": [],
   "execution_count": 124
  },
  {
   "cell_type": "code",
   "id": "1a18a45a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:32:08.975600Z",
     "start_time": "2025-02-04T03:31:51.983140Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, svm.predict(xTr))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5475604297224709"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 125
  },
  {
   "cell_type": "code",
   "id": "7311630d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:32:10.922176Z",
     "start_time": "2025-02-04T03:32:08.976604Z"
    }
   },
   "source": [
    "# testing score\n",
    "accuracy_score(yTe.Result.values, svm.predict(xTe))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5315315315315315"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:32:12.853149Z",
     "start_time": "2025-02-04T03:32:10.922821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predict_svm = svm.predict(xTe)\n",
    "series_svm = pd.Series(predict_svm, name='Predicted')\n",
    "compare_result = pd.concat([series_svm, yTe.reset_index()], axis=1)\n",
    "compare_result"
   ],
   "id": "63ff47239dab82c6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Predicted  index  Result\n",
       "0            2  36347       1\n",
       "1            2  36348       1\n",
       "2            2  36349       2\n",
       "3            0  36350       1\n",
       "4            0  36351       0\n",
       "..         ...    ...     ...\n",
       "994          0  37341       0\n",
       "995          2  37342       2\n",
       "996          2  37343       1\n",
       "997          2  37344       2\n",
       "998          2  37345       1\n",
       "\n",
       "[999 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>index</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>36347</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>36348</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>36349</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>36350</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>36351</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>0</td>\n",
       "      <td>37341</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2</td>\n",
       "      <td>37342</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2</td>\n",
       "      <td>37343</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2</td>\n",
       "      <td>37344</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2</td>\n",
       "      <td>37345</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:32:14.018816Z",
     "start_time": "2025-02-04T03:32:12.853832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "result_subset = compare_result.tail(200)\n",
    "plt.figure(figsize=(10,10))\n",
    "result_subset.plot(x='index', y=['Predicted', 'Result'], kind='line')\n",
    "plt.title(\"Prediction vs Real\")\n",
    "plt.show()"
   ],
   "id": "b3202c7e1788a64e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:32:14.104491Z",
     "start_time": "2025-02-04T03:32:14.024338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "local_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
    "dump(svm, f'./sklearn_svm_{local_time}.joblib')"
   ],
   "id": "619560b8158e3d4e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./sklearn_svm_2025_02_04_11_32_14.joblib']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 129
  },
  {
   "cell_type": "markdown",
   "id": "b2f00bed",
   "metadata": {},
   "source": [
    "**3.5.2** Penalty Tuning"
   ]
  },
  {
   "cell_type": "code",
   "id": "5b998f7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:36:21.038528Z",
     "start_time": "2025-02-04T03:32:14.105497Z"
    }
   },
   "source": [
    "svm_params = {'C':[0.001,0.01,0.10,0.2,0.3,0.5,0.7,0.8]}\n",
    "\n",
    "# svm_randsearch = RandomizedSearchCV(estimator=SVC(max_iter=100000),\n",
    "#                                          param_distributions=svm_params,\n",
    "svm_randsearch = GridSearchCV(estimator=SVC(max_iter=100000),\n",
    "                                         param_grid=svm_params,\n",
    "                                         scoring='accuracy',\n",
    "                                         verbose=2,\n",
    "                                         cv=5,\n",
    "                                         n_jobs=-1)\n",
    "\n",
    "svm_rand_results = svm_randsearch.fit(xTr, yTr.values.ravel())\n",
    "print(\"Best: %f using %s\" % (svm_rand_results.best_score_, svm_rand_results.best_params_))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best: 0.535364 using {'C': 0.5}\n"
     ]
    }
   ],
   "execution_count": 130
  },
  {
   "cell_type": "code",
   "id": "7f9627ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:36:21.047319Z",
     "start_time": "2025-02-04T03:36:21.039572Z"
    }
   },
   "source": [
    "svm_rs = svm_rand_results.best_estimator_"
   ],
   "outputs": [],
   "execution_count": 131
  },
  {
   "cell_type": "code",
   "id": "e32e6caf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:36:37.644335Z",
     "start_time": "2025-02-04T03:36:21.047825Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, svm_rs.predict(xTr))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5450984780662489"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 132
  },
  {
   "cell_type": "code",
   "id": "e88837b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:36:39.498654Z",
     "start_time": "2025-02-04T03:36:37.644840Z"
    }
   },
   "source": [
    "# testing score\n",
    "accuracy_score(yTe.Result.values, svm_rs.predict(xTe))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5335335335335335"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:36:39.544917Z",
     "start_time": "2025-02-04T03:36:39.499660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "local_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
    "dump(svm_rs, f'./sklearn_svm_randsearch_{local_time}.joblib')"
   ],
   "id": "2be3ad74a6193c29",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./sklearn_svm_randsearch_2025_02_04_11_36_39.joblib']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### improve model",
   "id": "46f2d2ec166d29de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:53:07.883496Z",
     "start_time": "2025-02-04T03:36:39.546423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.neural_network._stochastic_optimizers import AdamOptimizer\n",
    "from sklearn.base import clone\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "class ImprovedMLPClassifier(MLPClassifier):\n",
    "    def __init__(self, **kwargs):\n",
    "        # Remove dropout if it's in kwargs since MLPClassifier doesn't support it\n",
    "        if 'dropout' in kwargs:\n",
    "            del kwargs['dropout']\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_scores_ = []\n",
    "        self.val_scores_ = []  # Changed from validation_scores_ to val_scores_\n",
    "        self.loss_curve_ = []\n",
    "        self.best_val_score_ = -np.inf\n",
    "        self.best_model_params_ = None  # Store best model parameters instead of the whole model\n",
    "        self.iter_ = 0\n",
    "        \n",
    "    def _partial_fit(self, X, y, *args, **kwargs):\n",
    "        if self.early_stopping:\n",
    "            # Split validation set\n",
    "            n_samples = X.shape[0]\n",
    "            n_val = int(n_samples * self.validation_fraction)\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            val_idx = indices[:n_val]\n",
    "            train_idx = indices[n_val:]\n",
    "            \n",
    "            X_val = X[val_idx]\n",
    "            y_val = y[val_idx]\n",
    "            X_train = X[train_idx]\n",
    "            y_train = y[train_idx]\n",
    "            \n",
    "            # Train\n",
    "            super()._partial_fit(X_train, y_train, *args, **kwargs)\n",
    "            \n",
    "            # Record scores\n",
    "            train_score = accuracy_score(y_train, self.predict(X_train))\n",
    "            val_score = accuracy_score(y_val, self.predict(X_val))\n",
    "            \n",
    "            self.train_scores_.append(train_score)\n",
    "            self.val_scores_.append(val_score)\n",
    "            \n",
    "            # Save best model parameters\n",
    "            if val_score > self.best_val_score_:\n",
    "                self.best_val_score_ = val_score\n",
    "                self.best_model_params_ = {\n",
    "                    'coefs_': [coef.copy() for coef in self.coefs_],\n",
    "                    'intercepts_': [intercept.copy() for intercept in self.intercepts_]\n",
    "                }\n",
    "            \n",
    "            self.iter_ += 1\n",
    "            if self.verbose and self.iter_ % 10 == 0:\n",
    "                print(f\"Epoch {len(self.train_scores_)}\")\n",
    "                print(f\"Training Score: {train_score:.4f}\")\n",
    "                print(f\"Validation Score: {val_score:.4f}\")\n",
    "                print(f\"Best Validation Score: {self.best_val_score_:.4f}\")\n",
    "                if hasattr(self, 'loss_curve_') and len(self.loss_curve_) > 0:\n",
    "                    print(f\"Loss: {self.loss_curve_[-1]:.4f}\")\n",
    "                print(\"-\" * 50)\n",
    "        else:\n",
    "            super()._partial_fit(X, y, *args, **kwargs)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Initialize loss curve if it doesn't exist\n",
    "        if not hasattr(self, 'loss_curve_'):\n",
    "            self.loss_curve_ = []\n",
    "            \n",
    "        super().fit(X, y)\n",
    "        \n",
    "        # Restore best model parameters if available\n",
    "        if self.early_stopping and self.best_model_params_ is not None:\n",
    "            self.coefs_ = self.best_model_params_['coefs_']\n",
    "            self.intercepts_ = self.best_model_params_['intercepts_']\n",
    "        return self\n",
    "\n",
    "def train_and_evaluate_model(X_train, y_train, X_test, y_test, random_state=13):\n",
    "    # Create and train model with better hyperparameters\n",
    "    model = ImprovedMLPClassifier(\n",
    "        hidden_layer_sizes=(64, 32),  # Smaller network\n",
    "        activation='relu',\n",
    "        batch_size=32,  # Smaller batch size\n",
    "        max_iter=10000,\n",
    "        learning_rate_init=0.0001,\n",
    "        early_stopping=False,\n",
    "        validation_fraction=0.2,\n",
    "        alpha=0.01,  # Stronger regularization\n",
    "        verbose=True,\n",
    "        tol=1e-4,\n",
    "        random_state=random_state,\n",
    "        learning_rate='adaptive',\n",
    "        n_iter_no_change=200\n",
    "    )\n",
    "    \n",
    "    # Split training data for validation\n",
    "    X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_final, y_train_final)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_score = accuracy_score(y_train_final, model.predict(X_train_final))\n",
    "    val_score = accuracy_score(y_val, model.predict(X_val))\n",
    "    test_score = accuracy_score(y_test, model.predict(X_test))\n",
    "    \n",
    "    print(f\"\\nFinal Scores:\")\n",
    "    print(f\"Training Accuracy: {train_score:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_score:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_score:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def plot_learning_curves(model):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Learning curves\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(model.train_scores_, label='Training Score')\n",
    "    plt.plot(model.validation_scores_, label='Validation Score')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Learning Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Loss curve\n",
    "    if hasattr(model, 'loss_curve_') and len(model.loss_curve_) > 0:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(model.loss_curve_)\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss Curve')\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 使用示例\n",
    "def run_experiment(xTr, yTr, test_size=0.2, random_state=42):\n",
    "    # 准备数据\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        xTr, yTr.values.ravel(), \n",
    "        test_size=test_size, \n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # 训练和评估模型\n",
    "    model = train_and_evaluate_model(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # 绘制学习曲线\n",
    "    plot_learning_curves(model)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 运行完整实验\n",
    "model = run_experiment(xTr, yTr)\n",
    "\n",
    "# 或者分步执行\n",
    "X_train, X_test, y_train, y_test = train_test_split(xTr, yTr.values.ravel(), test_size=0.2, random_state=13)\n",
    "model = train_and_evaluate_model(X_train, y_train, X_test, y_test)\n",
    "# plot_learning_curves(model)"
   ],
   "id": "3559d0ac4a3141f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.12196769\n",
      "Iteration 2, loss = 1.02890206\n",
      "Iteration 3, loss = 1.00159758\n",
      "Iteration 4, loss = 0.98949599\n",
      "Iteration 5, loss = 0.98298443\n",
      "Iteration 6, loss = 0.97907830\n",
      "Iteration 7, loss = 0.97587690\n",
      "Iteration 8, loss = 0.97283001\n",
      "Iteration 9, loss = 0.96999932\n",
      "Iteration 10, loss = 0.96777966\n",
      "Iteration 11, loss = 0.96527181\n",
      "Iteration 12, loss = 0.96386376\n",
      "Iteration 13, loss = 0.96171799\n",
      "Iteration 14, loss = 0.95881193\n",
      "Iteration 15, loss = 0.95693142\n",
      "Iteration 16, loss = 0.95464633\n",
      "Iteration 17, loss = 0.95234281\n",
      "Iteration 18, loss = 0.95054325\n",
      "Iteration 19, loss = 0.94870526\n",
      "Iteration 20, loss = 0.94608855\n",
      "Iteration 21, loss = 0.94454063\n",
      "Iteration 22, loss = 0.94226926\n",
      "Iteration 23, loss = 0.94032652\n",
      "Iteration 24, loss = 0.93822176\n",
      "Iteration 25, loss = 0.93634972\n",
      "Iteration 26, loss = 0.93384548\n",
      "Iteration 27, loss = 0.93163680\n",
      "Iteration 28, loss = 0.92924116\n",
      "Iteration 29, loss = 0.92756186\n",
      "Iteration 30, loss = 0.92537935\n",
      "Iteration 31, loss = 0.92337965\n",
      "Iteration 32, loss = 0.92109301\n",
      "Iteration 33, loss = 0.91867278\n",
      "Iteration 34, loss = 0.91686984\n",
      "Iteration 35, loss = 0.91442382\n",
      "Iteration 36, loss = 0.91238578\n",
      "Iteration 37, loss = 0.90953499\n",
      "Iteration 38, loss = 0.90735905\n",
      "Iteration 39, loss = 0.90496226\n",
      "Iteration 40, loss = 0.90257756\n",
      "Iteration 41, loss = 0.90009966\n",
      "Iteration 42, loss = 0.89823593\n",
      "Iteration 43, loss = 0.89571419\n",
      "Iteration 44, loss = 0.89316078\n",
      "Iteration 45, loss = 0.89121666\n",
      "Iteration 46, loss = 0.88746522\n",
      "Iteration 47, loss = 0.88555455\n",
      "Iteration 48, loss = 0.88313899\n",
      "Iteration 49, loss = 0.88047513\n",
      "Iteration 50, loss = 0.87852977\n",
      "Iteration 51, loss = 0.87578622\n",
      "Iteration 52, loss = 0.87333399\n",
      "Iteration 53, loss = 0.87013796\n",
      "Iteration 54, loss = 0.86725443\n",
      "Iteration 55, loss = 0.86558103\n",
      "Iteration 56, loss = 0.86255698\n",
      "Iteration 57, loss = 0.85996369\n",
      "Iteration 58, loss = 0.85705278\n",
      "Iteration 59, loss = 0.85460293\n",
      "Iteration 60, loss = 0.85163777\n",
      "Iteration 61, loss = 0.84837282\n",
      "Iteration 62, loss = 0.84593488\n",
      "Iteration 63, loss = 0.84349509\n",
      "Iteration 64, loss = 0.84051977\n",
      "Iteration 65, loss = 0.83777590\n",
      "Iteration 66, loss = 0.83464583\n",
      "Iteration 67, loss = 0.83163785\n",
      "Iteration 68, loss = 0.82896248\n",
      "Iteration 69, loss = 0.82689858\n",
      "Iteration 70, loss = 0.82291843\n",
      "Iteration 71, loss = 0.82023031\n",
      "Iteration 72, loss = 0.81834039\n",
      "Iteration 73, loss = 0.81504312\n",
      "Iteration 74, loss = 0.81102453\n",
      "Iteration 75, loss = 0.80878442\n",
      "Iteration 76, loss = 0.80513079\n",
      "Iteration 77, loss = 0.80350737\n",
      "Iteration 78, loss = 0.79962885\n",
      "Iteration 79, loss = 0.79691600\n",
      "Iteration 80, loss = 0.79392664\n",
      "Iteration 81, loss = 0.79051365\n",
      "Iteration 82, loss = 0.78876097\n",
      "Iteration 83, loss = 0.78451745\n",
      "Iteration 84, loss = 0.78165492\n",
      "Iteration 85, loss = 0.77833804\n",
      "Iteration 86, loss = 0.77563575\n",
      "Iteration 87, loss = 0.77259910\n",
      "Iteration 88, loss = 0.76971308\n",
      "Iteration 89, loss = 0.76591501\n",
      "Iteration 90, loss = 0.76388899\n",
      "Iteration 91, loss = 0.75994404\n",
      "Iteration 92, loss = 0.75781090\n",
      "Iteration 93, loss = 0.75414609\n",
      "Iteration 94, loss = 0.75074876\n",
      "Iteration 95, loss = 0.74862208\n",
      "Iteration 96, loss = 0.74450728\n",
      "Iteration 97, loss = 0.74156526\n",
      "Iteration 98, loss = 0.73866251\n",
      "Iteration 99, loss = 0.73475021\n",
      "Iteration 100, loss = 0.73162997\n",
      "Iteration 101, loss = 0.72813066\n",
      "Iteration 102, loss = 0.72527216\n",
      "Iteration 103, loss = 0.72072825\n",
      "Iteration 104, loss = 0.71838829\n",
      "Iteration 105, loss = 0.71561835\n",
      "Iteration 106, loss = 0.71099686\n",
      "Iteration 107, loss = 0.70789533\n",
      "Iteration 108, loss = 0.70500983\n",
      "Iteration 109, loss = 0.70274468\n",
      "Iteration 110, loss = 0.69839314\n",
      "Iteration 111, loss = 0.69435908\n",
      "Iteration 112, loss = 0.69086376\n",
      "Iteration 113, loss = 0.68781031\n",
      "Iteration 114, loss = 0.68416989\n",
      "Iteration 115, loss = 0.68144365\n",
      "Iteration 116, loss = 0.67768274\n",
      "Iteration 117, loss = 0.67456692\n",
      "Iteration 118, loss = 0.67090119\n",
      "Iteration 119, loss = 0.66812994\n",
      "Iteration 120, loss = 0.66627729\n",
      "Iteration 121, loss = 0.66145616\n",
      "Iteration 122, loss = 0.65821410\n",
      "Iteration 123, loss = 0.65547747\n",
      "Iteration 124, loss = 0.65261349\n",
      "Iteration 125, loss = 0.64899810\n",
      "Iteration 126, loss = 0.64464503\n",
      "Iteration 127, loss = 0.64198635\n",
      "Iteration 128, loss = 0.63975931\n",
      "Iteration 129, loss = 0.63640014\n",
      "Iteration 130, loss = 0.63266112\n",
      "Iteration 131, loss = 0.62884689\n",
      "Iteration 132, loss = 0.62718846\n",
      "Iteration 133, loss = 0.62357175\n",
      "Iteration 134, loss = 0.62002369\n",
      "Iteration 135, loss = 0.61767493\n",
      "Iteration 136, loss = 0.61402998\n",
      "Iteration 137, loss = 0.61093926\n",
      "Iteration 138, loss = 0.60799297\n",
      "Iteration 139, loss = 0.60470425\n",
      "Iteration 140, loss = 0.60082064\n",
      "Iteration 141, loss = 0.59790494\n",
      "Iteration 142, loss = 0.59551566\n",
      "Iteration 143, loss = 0.59103791\n",
      "Iteration 144, loss = 0.58910840\n",
      "Iteration 145, loss = 0.58444162\n",
      "Iteration 146, loss = 0.58252138\n",
      "Iteration 147, loss = 0.57914309\n",
      "Iteration 148, loss = 0.57628015\n",
      "Iteration 149, loss = 0.57376740\n",
      "Iteration 150, loss = 0.56958519\n",
      "Iteration 151, loss = 0.56777989\n",
      "Iteration 152, loss = 0.56440074\n",
      "Iteration 153, loss = 0.56105190\n",
      "Iteration 154, loss = 0.55794737\n",
      "Iteration 155, loss = 0.55391947\n",
      "Iteration 156, loss = 0.55163317\n",
      "Iteration 157, loss = 0.54769919\n",
      "Iteration 158, loss = 0.54579049\n",
      "Iteration 159, loss = 0.54273996\n",
      "Iteration 160, loss = 0.54070956\n",
      "Iteration 161, loss = 0.53670849\n",
      "Iteration 162, loss = 0.53313858\n",
      "Iteration 163, loss = 0.53097278\n",
      "Iteration 164, loss = 0.52897202\n",
      "Iteration 165, loss = 0.52514105\n",
      "Iteration 166, loss = 0.52144847\n",
      "Iteration 167, loss = 0.51829851\n",
      "Iteration 168, loss = 0.51770972\n",
      "Iteration 169, loss = 0.51239805\n",
      "Iteration 170, loss = 0.51051969\n",
      "Iteration 171, loss = 0.50758175\n",
      "Iteration 172, loss = 0.50524948\n",
      "Iteration 173, loss = 0.50106006\n",
      "Iteration 174, loss = 0.49834744\n",
      "Iteration 175, loss = 0.49539900\n",
      "Iteration 176, loss = 0.49287578\n",
      "Iteration 177, loss = 0.49049753\n",
      "Iteration 178, loss = 0.48717145\n",
      "Iteration 179, loss = 0.48268557\n",
      "Iteration 180, loss = 0.48076989\n",
      "Iteration 181, loss = 0.47833788\n",
      "Iteration 182, loss = 0.47510528\n",
      "Iteration 183, loss = 0.47269949\n",
      "Iteration 184, loss = 0.46980217\n",
      "Iteration 185, loss = 0.46689455\n",
      "Iteration 186, loss = 0.46570247\n",
      "Iteration 187, loss = 0.46092286\n",
      "Iteration 188, loss = 0.45890142\n",
      "Iteration 189, loss = 0.45660216\n",
      "Iteration 190, loss = 0.45354218\n",
      "Iteration 191, loss = 0.45206286\n",
      "Iteration 192, loss = 0.44761715\n",
      "Iteration 193, loss = 0.44483621\n",
      "Iteration 194, loss = 0.44461479\n",
      "Iteration 195, loss = 0.44052403\n",
      "Iteration 196, loss = 0.43931939\n",
      "Iteration 197, loss = 0.43612003\n",
      "Iteration 198, loss = 0.43174248\n",
      "Iteration 199, loss = 0.42977864\n",
      "Iteration 200, loss = 0.42770423\n",
      "Iteration 201, loss = 0.42471840\n",
      "Iteration 202, loss = 0.42231978\n",
      "Iteration 203, loss = 0.42009058\n",
      "Iteration 204, loss = 0.41740455\n",
      "Iteration 205, loss = 0.41523869\n",
      "Iteration 206, loss = 0.41284185\n",
      "Iteration 207, loss = 0.41049576\n",
      "Iteration 208, loss = 0.40757545\n",
      "Iteration 209, loss = 0.40568635\n",
      "Iteration 210, loss = 0.40221148\n",
      "Iteration 211, loss = 0.40125629\n",
      "Iteration 212, loss = 0.39766943\n",
      "Iteration 213, loss = 0.39510788\n",
      "Iteration 214, loss = 0.39310042\n",
      "Iteration 215, loss = 0.39056166\n",
      "Iteration 216, loss = 0.38767839\n",
      "Iteration 217, loss = 0.38640516\n",
      "Iteration 218, loss = 0.38266753\n",
      "Iteration 219, loss = 0.38025427\n",
      "Iteration 220, loss = 0.37933682\n",
      "Iteration 221, loss = 0.37580918\n",
      "Iteration 222, loss = 0.37399241\n",
      "Iteration 223, loss = 0.37068951\n",
      "Iteration 224, loss = 0.37003881\n",
      "Iteration 225, loss = 0.36652148\n",
      "Iteration 226, loss = 0.36391386\n",
      "Iteration 227, loss = 0.36293736\n",
      "Iteration 228, loss = 0.36174307\n",
      "Iteration 229, loss = 0.35904289\n",
      "Iteration 230, loss = 0.35665922\n",
      "Iteration 231, loss = 0.35476498\n",
      "Iteration 232, loss = 0.35048259\n",
      "Iteration 233, loss = 0.34919818\n",
      "Iteration 234, loss = 0.34661002\n",
      "Iteration 235, loss = 0.34513759\n",
      "Iteration 236, loss = 0.34259249\n",
      "Iteration 237, loss = 0.34137179\n",
      "Iteration 238, loss = 0.33881848\n",
      "Iteration 239, loss = 0.33675913\n",
      "Iteration 240, loss = 0.33450218\n",
      "Iteration 241, loss = 0.33274840\n",
      "Iteration 242, loss = 0.33035836\n",
      "Iteration 243, loss = 0.32777218\n",
      "Iteration 244, loss = 0.32665923\n",
      "Iteration 245, loss = 0.32389623\n",
      "Iteration 246, loss = 0.32269475\n",
      "Iteration 247, loss = 0.31942691\n",
      "Iteration 248, loss = 0.31906365\n",
      "Iteration 249, loss = 0.31526892\n",
      "Iteration 250, loss = 0.31476878\n",
      "Iteration 251, loss = 0.31217240\n",
      "Iteration 252, loss = 0.31042350\n",
      "Iteration 253, loss = 0.30843913\n",
      "Iteration 254, loss = 0.30572409\n",
      "Iteration 255, loss = 0.30577701\n",
      "Iteration 256, loss = 0.30165146\n",
      "Iteration 257, loss = 0.30109973\n",
      "Iteration 258, loss = 0.29899606\n",
      "Iteration 259, loss = 0.29598646\n",
      "Iteration 260, loss = 0.29441158\n",
      "Iteration 261, loss = 0.29424404\n",
      "Iteration 262, loss = 0.29059104\n",
      "Iteration 263, loss = 0.28969938\n",
      "Iteration 264, loss = 0.28706933\n",
      "Iteration 265, loss = 0.28613520\n",
      "Iteration 266, loss = 0.28444118\n",
      "Iteration 267, loss = 0.28185394\n",
      "Iteration 268, loss = 0.28001898\n",
      "Iteration 269, loss = 0.27918069\n",
      "Iteration 270, loss = 0.27728387\n",
      "Iteration 271, loss = 0.27596840\n",
      "Iteration 272, loss = 0.27461749\n",
      "Iteration 273, loss = 0.27207598\n",
      "Iteration 274, loss = 0.27002320\n",
      "Iteration 275, loss = 0.26715615\n",
      "Iteration 276, loss = 0.26733781\n",
      "Iteration 277, loss = 0.26519778\n",
      "Iteration 278, loss = 0.26405400\n",
      "Iteration 279, loss = 0.26241063\n",
      "Iteration 280, loss = 0.26140690\n",
      "Iteration 281, loss = 0.25773082\n",
      "Iteration 282, loss = 0.25643476\n",
      "Iteration 283, loss = 0.25556010\n",
      "Iteration 284, loss = 0.25377307\n",
      "Iteration 285, loss = 0.25127089\n",
      "Iteration 286, loss = 0.24996171\n",
      "Iteration 287, loss = 0.24994105\n",
      "Iteration 288, loss = 0.24793134\n",
      "Iteration 289, loss = 0.24656797\n",
      "Iteration 290, loss = 0.24415790\n",
      "Iteration 291, loss = 0.24364256\n",
      "Iteration 292, loss = 0.24207556\n",
      "Iteration 293, loss = 0.24193905\n",
      "Iteration 294, loss = 0.23921822\n",
      "Iteration 295, loss = 0.23803020\n",
      "Iteration 296, loss = 0.23440495\n",
      "Iteration 297, loss = 0.23419384\n",
      "Iteration 298, loss = 0.23341448\n",
      "Iteration 299, loss = 0.23061109\n",
      "Iteration 300, loss = 0.23048148\n",
      "Iteration 301, loss = 0.22733994\n",
      "Iteration 302, loss = 0.22762560\n",
      "Iteration 303, loss = 0.22543644\n",
      "Iteration 304, loss = 0.22479124\n",
      "Iteration 305, loss = 0.22488247\n",
      "Iteration 306, loss = 0.22182096\n",
      "Iteration 307, loss = 0.22089174\n",
      "Iteration 308, loss = 0.21900966\n",
      "Iteration 309, loss = 0.21746402\n",
      "Iteration 310, loss = 0.21666433\n",
      "Iteration 311, loss = 0.21622083\n",
      "Iteration 312, loss = 0.21351646\n",
      "Iteration 313, loss = 0.21260766\n",
      "Iteration 314, loss = 0.21142654\n",
      "Iteration 315, loss = 0.21007040\n",
      "Iteration 316, loss = 0.20875055\n",
      "Iteration 317, loss = 0.20758713\n",
      "Iteration 318, loss = 0.20588160\n",
      "Iteration 319, loss = 0.20520055\n",
      "Iteration 320, loss = 0.20370934\n",
      "Iteration 321, loss = 0.20389238\n",
      "Iteration 322, loss = 0.20269476\n",
      "Iteration 323, loss = 0.20089000\n",
      "Iteration 324, loss = 0.19993664\n",
      "Iteration 325, loss = 0.19888344\n",
      "Iteration 326, loss = 0.19729278\n",
      "Iteration 327, loss = 0.19718588\n",
      "Iteration 328, loss = 0.19575972\n",
      "Iteration 329, loss = 0.19535747\n",
      "Iteration 330, loss = 0.19255748\n",
      "Iteration 331, loss = 0.19227023\n",
      "Iteration 332, loss = 0.19153206\n",
      "Iteration 333, loss = 0.18967399\n",
      "Iteration 334, loss = 0.18848261\n",
      "Iteration 335, loss = 0.18777757\n",
      "Iteration 336, loss = 0.18741559\n",
      "Iteration 337, loss = 0.18580148\n",
      "Iteration 338, loss = 0.18583600\n",
      "Iteration 339, loss = 0.18376668\n",
      "Iteration 340, loss = 0.18232377\n",
      "Iteration 341, loss = 0.18238647\n",
      "Iteration 342, loss = 0.18187441\n",
      "Iteration 343, loss = 0.17995878\n",
      "Iteration 344, loss = 0.17979485\n",
      "Iteration 345, loss = 0.17802294\n",
      "Iteration 346, loss = 0.17739979\n",
      "Iteration 347, loss = 0.17612095\n",
      "Iteration 348, loss = 0.17525846\n",
      "Iteration 349, loss = 0.17550993\n",
      "Iteration 350, loss = 0.17507843\n",
      "Iteration 351, loss = 0.17286990\n",
      "Iteration 352, loss = 0.17275105\n",
      "Iteration 353, loss = 0.17148595\n",
      "Iteration 354, loss = 0.16988952\n",
      "Iteration 355, loss = 0.16927790\n",
      "Iteration 356, loss = 0.16903710\n",
      "Iteration 357, loss = 0.16766748\n",
      "Iteration 358, loss = 0.16776721\n",
      "Iteration 359, loss = 0.16642003\n",
      "Iteration 360, loss = 0.16544965\n",
      "Iteration 361, loss = 0.16405669\n",
      "Iteration 362, loss = 0.16418607\n",
      "Iteration 363, loss = 0.16281879\n",
      "Iteration 364, loss = 0.16277642\n",
      "Iteration 365, loss = 0.16286069\n",
      "Iteration 366, loss = 0.16182879\n",
      "Iteration 367, loss = 0.16097844\n",
      "Iteration 368, loss = 0.15921130\n",
      "Iteration 369, loss = 0.15912938\n",
      "Iteration 370, loss = 0.15792392\n",
      "Iteration 371, loss = 0.15743980\n",
      "Iteration 372, loss = 0.15736862\n",
      "Iteration 373, loss = 0.15718127\n",
      "Iteration 374, loss = 0.15491420\n",
      "Iteration 375, loss = 0.15510434\n",
      "Iteration 376, loss = 0.15383299\n",
      "Iteration 377, loss = 0.15393274\n",
      "Iteration 378, loss = 0.15381786\n",
      "Iteration 379, loss = 0.15211985\n",
      "Iteration 380, loss = 0.15196274\n",
      "Iteration 381, loss = 0.15078708\n",
      "Iteration 382, loss = 0.15064868\n",
      "Iteration 383, loss = 0.14974815\n",
      "Iteration 384, loss = 0.14920192\n",
      "Iteration 385, loss = 0.14870071\n",
      "Iteration 386, loss = 0.14874190\n",
      "Iteration 387, loss = 0.14863436\n",
      "Iteration 388, loss = 0.14694680\n",
      "Iteration 389, loss = 0.14654923\n",
      "Iteration 390, loss = 0.14615239\n",
      "Iteration 391, loss = 0.14541122\n",
      "Iteration 392, loss = 0.14457345\n",
      "Iteration 393, loss = 0.14508058\n",
      "Iteration 394, loss = 0.14344249\n",
      "Iteration 395, loss = 0.14351782\n",
      "Iteration 396, loss = 0.14335709\n",
      "Iteration 397, loss = 0.14192106\n",
      "Iteration 398, loss = 0.14121189\n",
      "Iteration 399, loss = 0.14158328\n",
      "Iteration 400, loss = 0.14103337\n",
      "Iteration 401, loss = 0.13988493\n",
      "Iteration 402, loss = 0.14033453\n",
      "Iteration 403, loss = 0.13940602\n",
      "Iteration 404, loss = 0.13900485\n",
      "Iteration 405, loss = 0.13899559\n",
      "Iteration 406, loss = 0.13791647\n",
      "Iteration 407, loss = 0.13749707\n",
      "Iteration 408, loss = 0.13755497\n",
      "Iteration 409, loss = 0.13643685\n",
      "Iteration 410, loss = 0.13616754\n",
      "Iteration 411, loss = 0.13644582\n",
      "Iteration 412, loss = 0.13607297\n",
      "Iteration 413, loss = 0.13549582\n",
      "Iteration 414, loss = 0.13522662\n",
      "Iteration 415, loss = 0.13448094\n",
      "Iteration 416, loss = 0.13378075\n",
      "Iteration 417, loss = 0.13360619\n",
      "Iteration 418, loss = 0.13322317\n",
      "Iteration 419, loss = 0.13386878\n",
      "Iteration 420, loss = 0.13178908\n",
      "Iteration 421, loss = 0.13275691\n",
      "Iteration 422, loss = 0.13296232\n",
      "Iteration 423, loss = 0.13136508\n",
      "Iteration 424, loss = 0.13104980\n",
      "Iteration 425, loss = 0.13094467\n",
      "Iteration 426, loss = 0.13003008\n",
      "Iteration 427, loss = 0.13082464\n",
      "Iteration 428, loss = 0.12962348\n",
      "Iteration 429, loss = 0.12982660\n",
      "Iteration 430, loss = 0.12861859\n",
      "Iteration 431, loss = 0.12867351\n",
      "Iteration 432, loss = 0.12879183\n",
      "Iteration 433, loss = 0.12793454\n",
      "Iteration 434, loss = 0.12793043\n",
      "Iteration 435, loss = 0.12779301\n",
      "Iteration 436, loss = 0.12764749\n",
      "Iteration 437, loss = 0.12655254\n",
      "Iteration 438, loss = 0.12679920\n",
      "Iteration 439, loss = 0.12602279\n",
      "Iteration 440, loss = 0.12590534\n",
      "Iteration 441, loss = 0.12551360\n",
      "Iteration 442, loss = 0.12538428\n",
      "Iteration 443, loss = 0.12595422\n",
      "Iteration 444, loss = 0.12566171\n",
      "Iteration 445, loss = 0.12433617\n",
      "Iteration 446, loss = 0.12408817\n",
      "Iteration 447, loss = 0.12414225\n",
      "Iteration 448, loss = 0.12401069\n",
      "Iteration 449, loss = 0.12372735\n",
      "Iteration 450, loss = 0.12370775\n",
      "Iteration 451, loss = 0.12313861\n",
      "Iteration 452, loss = 0.12305814\n",
      "Iteration 453, loss = 0.12343017\n",
      "Iteration 454, loss = 0.12263217\n",
      "Iteration 455, loss = 0.12294040\n",
      "Iteration 456, loss = 0.12220761\n",
      "Iteration 457, loss = 0.12214403\n",
      "Iteration 458, loss = 0.12140461\n",
      "Iteration 459, loss = 0.12166580\n",
      "Iteration 460, loss = 0.12110787\n",
      "Iteration 461, loss = 0.12076953\n",
      "Iteration 462, loss = 0.12174432\n",
      "Iteration 463, loss = 0.12046977\n",
      "Iteration 464, loss = 0.12041566\n",
      "Iteration 465, loss = 0.11991702\n",
      "Iteration 466, loss = 0.12003459\n",
      "Iteration 467, loss = 0.11999735\n",
      "Iteration 468, loss = 0.11925947\n",
      "Iteration 469, loss = 0.11942333\n",
      "Iteration 470, loss = 0.11884169\n",
      "Iteration 471, loss = 0.11876141\n",
      "Iteration 472, loss = 0.11893853\n",
      "Iteration 473, loss = 0.11849969\n",
      "Iteration 474, loss = 0.11784403\n",
      "Iteration 475, loss = 0.11754524\n",
      "Iteration 476, loss = 0.11787979\n",
      "Iteration 477, loss = 0.11856070\n",
      "Iteration 478, loss = 0.11765912\n",
      "Iteration 479, loss = 0.11827900\n",
      "Iteration 480, loss = 0.11700468\n",
      "Iteration 481, loss = 0.11702316\n",
      "Iteration 482, loss = 0.11677519\n",
      "Iteration 483, loss = 0.11699733\n",
      "Iteration 484, loss = 0.11617434\n",
      "Iteration 485, loss = 0.11594478\n",
      "Iteration 486, loss = 0.11649178\n",
      "Iteration 487, loss = 0.11633850\n",
      "Iteration 488, loss = 0.11664640\n",
      "Iteration 489, loss = 0.11526415\n",
      "Iteration 490, loss = 0.11621714\n",
      "Iteration 491, loss = 0.11593875\n",
      "Iteration 492, loss = 0.11522830\n",
      "Iteration 493, loss = 0.11532223\n",
      "Iteration 494, loss = 0.11538622\n",
      "Iteration 495, loss = 0.11497627\n",
      "Iteration 496, loss = 0.11477639\n",
      "Iteration 497, loss = 0.11424288\n",
      "Iteration 498, loss = 0.11482596\n",
      "Iteration 499, loss = 0.11422429\n",
      "Iteration 500, loss = 0.11388446\n",
      "Iteration 501, loss = 0.11438167\n",
      "Iteration 502, loss = 0.11321799\n",
      "Iteration 503, loss = 0.11361653\n",
      "Iteration 504, loss = 0.11337827\n",
      "Iteration 505, loss = 0.11334779\n",
      "Iteration 506, loss = 0.11291497\n",
      "Iteration 507, loss = 0.11285478\n",
      "Iteration 508, loss = 0.11392081\n",
      "Iteration 509, loss = 0.11310773\n",
      "Iteration 510, loss = 0.11287062\n",
      "Iteration 511, loss = 0.11314415\n",
      "Iteration 512, loss = 0.11221044\n",
      "Iteration 513, loss = 0.11170456\n",
      "Iteration 514, loss = 0.11227410\n",
      "Iteration 515, loss = 0.11181368\n",
      "Iteration 516, loss = 0.11158754\n",
      "Iteration 517, loss = 0.11188218\n",
      "Iteration 518, loss = 0.11137080\n",
      "Iteration 519, loss = 0.11157785\n",
      "Iteration 520, loss = 0.11139983\n",
      "Iteration 521, loss = 0.11149910\n",
      "Iteration 522, loss = 0.11108042\n",
      "Iteration 523, loss = 0.11078758\n",
      "Iteration 524, loss = 0.11139948\n",
      "Iteration 525, loss = 0.11050457\n",
      "Iteration 526, loss = 0.11097780\n",
      "Iteration 527, loss = 0.11049114\n",
      "Iteration 528, loss = 0.11101609\n",
      "Iteration 529, loss = 0.11093652\n",
      "Iteration 530, loss = 0.11039926\n",
      "Iteration 531, loss = 0.11041535\n",
      "Iteration 532, loss = 0.11050576\n",
      "Iteration 533, loss = 0.10947905\n",
      "Iteration 534, loss = 0.11007568\n",
      "Iteration 535, loss = 0.10921495\n",
      "Iteration 536, loss = 0.10941578\n",
      "Iteration 537, loss = 0.10940577\n",
      "Iteration 538, loss = 0.10947895\n",
      "Iteration 539, loss = 0.10941581\n",
      "Iteration 540, loss = 0.10917124\n",
      "Iteration 541, loss = 0.10864078\n",
      "Iteration 542, loss = 0.10875445\n",
      "Iteration 543, loss = 0.10928455\n",
      "Iteration 544, loss = 0.10932301\n",
      "Iteration 545, loss = 0.10853285\n",
      "Iteration 546, loss = 0.10837816\n",
      "Iteration 547, loss = 0.10851260\n",
      "Iteration 548, loss = 0.10834104\n",
      "Iteration 549, loss = 0.10824835\n",
      "Iteration 550, loss = 0.10775039\n",
      "Iteration 551, loss = 0.10808291\n",
      "Iteration 552, loss = 0.10822635\n",
      "Iteration 553, loss = 0.10820055\n",
      "Iteration 554, loss = 0.10760646\n",
      "Iteration 555, loss = 0.10777275\n",
      "Iteration 556, loss = 0.10778278\n",
      "Iteration 557, loss = 0.10713703\n",
      "Iteration 558, loss = 0.10706382\n",
      "Iteration 559, loss = 0.10745932\n",
      "Iteration 560, loss = 0.10713388\n",
      "Iteration 561, loss = 0.10711733\n",
      "Iteration 562, loss = 0.10735525\n",
      "Iteration 563, loss = 0.10674483\n",
      "Iteration 564, loss = 0.10664941\n",
      "Iteration 565, loss = 0.10658368\n",
      "Iteration 566, loss = 0.10674251\n",
      "Iteration 567, loss = 0.10750602\n",
      "Iteration 568, loss = 0.10668321\n",
      "Iteration 569, loss = 0.10610421\n",
      "Iteration 570, loss = 0.10611857\n",
      "Iteration 571, loss = 0.10654497\n",
      "Iteration 572, loss = 0.10617675\n",
      "Iteration 573, loss = 0.10644363\n",
      "Iteration 574, loss = 0.10567250\n",
      "Iteration 575, loss = 0.10613930\n",
      "Iteration 576, loss = 0.10554526\n",
      "Iteration 577, loss = 0.10533937\n",
      "Iteration 578, loss = 0.10555367\n",
      "Iteration 579, loss = 0.10565689\n",
      "Iteration 580, loss = 0.10570147\n",
      "Iteration 581, loss = 0.10499254\n",
      "Iteration 582, loss = 0.10520306\n",
      "Iteration 583, loss = 0.10490785\n",
      "Iteration 584, loss = 0.10551507\n",
      "Iteration 585, loss = 0.10505488\n",
      "Iteration 586, loss = 0.10529136\n",
      "Iteration 587, loss = 0.10470679\n",
      "Iteration 588, loss = 0.10480520\n",
      "Iteration 589, loss = 0.10492111\n",
      "Iteration 590, loss = 0.10494207\n",
      "Iteration 591, loss = 0.10504607\n",
      "Iteration 592, loss = 0.10402210\n",
      "Iteration 593, loss = 0.10425314\n",
      "Iteration 594, loss = 0.10451384\n",
      "Iteration 595, loss = 0.10441427\n",
      "Iteration 596, loss = 0.10419860\n",
      "Iteration 597, loss = 0.10376960\n",
      "Iteration 598, loss = 0.10394649\n",
      "Iteration 599, loss = 0.10412083\n",
      "Iteration 600, loss = 0.10405660\n",
      "Iteration 601, loss = 0.10391340\n",
      "Iteration 602, loss = 0.10398535\n",
      "Iteration 603, loss = 0.10398024\n",
      "Iteration 604, loss = 0.10413282\n",
      "Iteration 605, loss = 0.10341051\n",
      "Iteration 606, loss = 0.10386175\n",
      "Iteration 607, loss = 0.10367927\n",
      "Iteration 608, loss = 0.10308308\n",
      "Iteration 609, loss = 0.10313871\n",
      "Iteration 610, loss = 0.10346411\n",
      "Iteration 611, loss = 0.10316433\n",
      "Iteration 612, loss = 0.10326426\n",
      "Iteration 613, loss = 0.10279128\n",
      "Iteration 614, loss = 0.10297532\n",
      "Iteration 615, loss = 0.10273176\n",
      "Iteration 616, loss = 0.10354991\n",
      "Iteration 617, loss = 0.10327202\n",
      "Iteration 618, loss = 0.10242975\n",
      "Iteration 619, loss = 0.10277782\n",
      "Iteration 620, loss = 0.10247449\n",
      "Iteration 621, loss = 0.10252296\n",
      "Iteration 622, loss = 0.10228212\n",
      "Iteration 623, loss = 0.10227412\n",
      "Iteration 624, loss = 0.10240122\n",
      "Iteration 625, loss = 0.10241861\n",
      "Iteration 626, loss = 0.10246035\n",
      "Iteration 627, loss = 0.10233877\n",
      "Iteration 628, loss = 0.10211435\n",
      "Iteration 629, loss = 0.10205800\n",
      "Iteration 630, loss = 0.10198151\n",
      "Iteration 631, loss = 0.10160168\n",
      "Iteration 632, loss = 0.10197771\n",
      "Iteration 633, loss = 0.10178992\n",
      "Iteration 634, loss = 0.10145380\n",
      "Iteration 635, loss = 0.10148351\n",
      "Iteration 636, loss = 0.10177280\n",
      "Iteration 637, loss = 0.10155569\n",
      "Iteration 638, loss = 0.10140004\n",
      "Iteration 639, loss = 0.10149677\n",
      "Iteration 640, loss = 0.10114690\n",
      "Iteration 641, loss = 0.10090306\n",
      "Iteration 642, loss = 0.10152158\n",
      "Iteration 643, loss = 0.10099985\n",
      "Iteration 644, loss = 0.10105032\n",
      "Iteration 645, loss = 0.10066967\n",
      "Iteration 646, loss = 0.10083783\n",
      "Iteration 647, loss = 0.10067321\n",
      "Iteration 648, loss = 0.10074112\n",
      "Iteration 649, loss = 0.10132720\n",
      "Iteration 650, loss = 0.10092790\n",
      "Iteration 651, loss = 0.10104753\n",
      "Iteration 652, loss = 0.10035751\n",
      "Iteration 653, loss = 0.10106720\n",
      "Iteration 654, loss = 0.10046599\n",
      "Iteration 655, loss = 0.10034534\n",
      "Iteration 656, loss = 0.10008143\n",
      "Iteration 657, loss = 0.10027922\n",
      "Iteration 658, loss = 0.10030393\n",
      "Iteration 659, loss = 0.10003371\n",
      "Iteration 660, loss = 0.10018209\n",
      "Iteration 661, loss = 0.10100896\n",
      "Iteration 662, loss = 0.09988980\n",
      "Iteration 663, loss = 0.10009585\n",
      "Iteration 664, loss = 0.09968307\n",
      "Iteration 665, loss = 0.10012432\n",
      "Iteration 666, loss = 0.10052495\n",
      "Iteration 667, loss = 0.10014153\n",
      "Iteration 668, loss = 0.09954946\n",
      "Iteration 669, loss = 0.09947744\n",
      "Iteration 670, loss = 0.09965737\n",
      "Iteration 671, loss = 0.09958904\n",
      "Iteration 672, loss = 0.09921126\n",
      "Iteration 673, loss = 0.09996734\n",
      "Iteration 674, loss = 0.09932253\n",
      "Iteration 675, loss = 0.09930856\n",
      "Iteration 676, loss = 0.09947946\n",
      "Iteration 677, loss = 0.09926247\n",
      "Iteration 678, loss = 0.09906512\n",
      "Iteration 679, loss = 0.09916411\n",
      "Iteration 680, loss = 0.09922698\n",
      "Iteration 681, loss = 0.09884700\n",
      "Iteration 682, loss = 0.09941518\n",
      "Iteration 683, loss = 0.09888557\n",
      "Iteration 684, loss = 0.09928348\n",
      "Iteration 685, loss = 0.09881601\n",
      "Iteration 686, loss = 0.09898395\n",
      "Iteration 687, loss = 0.09913079\n",
      "Iteration 688, loss = 0.09894409\n",
      "Iteration 689, loss = 0.09881765\n",
      "Iteration 690, loss = 0.09881681\n",
      "Iteration 691, loss = 0.09877374\n",
      "Iteration 692, loss = 0.09849450\n",
      "Iteration 693, loss = 0.09850947\n",
      "Iteration 694, loss = 0.09818839\n",
      "Iteration 695, loss = 0.09836544\n",
      "Iteration 696, loss = 0.09840619\n",
      "Iteration 697, loss = 0.09809942\n",
      "Iteration 698, loss = 0.09843604\n",
      "Iteration 699, loss = 0.09853160\n",
      "Iteration 700, loss = 0.09843620\n",
      "Iteration 701, loss = 0.09828092\n",
      "Iteration 702, loss = 0.09810484\n",
      "Iteration 703, loss = 0.09798122\n",
      "Iteration 704, loss = 0.09796342\n",
      "Iteration 705, loss = 0.09796737\n",
      "Iteration 706, loss = 0.09847571\n",
      "Iteration 707, loss = 0.09848750\n",
      "Iteration 708, loss = 0.09828574\n",
      "Iteration 709, loss = 0.09793518\n",
      "Iteration 710, loss = 0.09781678\n",
      "Iteration 711, loss = 0.09803348\n",
      "Iteration 712, loss = 0.09768645\n",
      "Iteration 713, loss = 0.09755291\n",
      "Iteration 714, loss = 0.09770518\n",
      "Iteration 715, loss = 0.09703011\n",
      "Iteration 716, loss = 0.09743044\n",
      "Iteration 717, loss = 0.09758102\n",
      "Iteration 718, loss = 0.09748867\n",
      "Iteration 719, loss = 0.09792473\n",
      "Iteration 720, loss = 0.09732948\n",
      "Iteration 721, loss = 0.09749343\n",
      "Iteration 722, loss = 0.09707022\n",
      "Iteration 723, loss = 0.09713436\n",
      "Iteration 724, loss = 0.09726277\n",
      "Iteration 725, loss = 0.09727328\n",
      "Iteration 726, loss = 0.09717974\n",
      "Iteration 727, loss = 0.09732586\n",
      "Iteration 728, loss = 0.09716301\n",
      "Iteration 729, loss = 0.09671329\n",
      "Iteration 730, loss = 0.09657213\n",
      "Iteration 731, loss = 0.09654646\n",
      "Iteration 732, loss = 0.09673109\n",
      "Iteration 733, loss = 0.09660075\n",
      "Iteration 734, loss = 0.09693229\n",
      "Iteration 735, loss = 0.09665061\n",
      "Iteration 736, loss = 0.09698388\n",
      "Iteration 737, loss = 0.09665144\n",
      "Iteration 738, loss = 0.09675154\n",
      "Iteration 739, loss = 0.09684310\n",
      "Iteration 740, loss = 0.09648770\n",
      "Iteration 741, loss = 0.09656627\n",
      "Iteration 742, loss = 0.09672744\n",
      "Iteration 743, loss = 0.09657981\n",
      "Iteration 744, loss = 0.09687740\n",
      "Iteration 745, loss = 0.09652370\n",
      "Iteration 746, loss = 0.09612260\n",
      "Iteration 747, loss = 0.09651626\n",
      "Iteration 748, loss = 0.09631050\n",
      "Iteration 749, loss = 0.09653446\n",
      "Iteration 750, loss = 0.09593610\n",
      "Iteration 751, loss = 0.09592961\n",
      "Iteration 752, loss = 0.09609266\n",
      "Iteration 753, loss = 0.09601273\n",
      "Iteration 754, loss = 0.09607520\n",
      "Iteration 755, loss = 0.09616871\n",
      "Iteration 756, loss = 0.09571425\n",
      "Iteration 757, loss = 0.09577100\n",
      "Iteration 758, loss = 0.09622229\n",
      "Iteration 759, loss = 0.09561278\n",
      "Iteration 760, loss = 0.09598820\n",
      "Iteration 761, loss = 0.09571965\n",
      "Iteration 762, loss = 0.09595792\n",
      "Iteration 763, loss = 0.09572398\n",
      "Iteration 764, loss = 0.09558869\n",
      "Iteration 765, loss = 0.09594820\n",
      "Iteration 766, loss = 0.09552954\n",
      "Iteration 767, loss = 0.09556244\n",
      "Iteration 768, loss = 0.09541953\n",
      "Iteration 769, loss = 0.09523575\n",
      "Iteration 770, loss = 0.09556156\n",
      "Iteration 771, loss = 0.09528067\n",
      "Iteration 772, loss = 0.09548573\n",
      "Iteration 773, loss = 0.09523573\n",
      "Iteration 774, loss = 0.09538585\n",
      "Iteration 775, loss = 0.09564975\n",
      "Iteration 776, loss = 0.09537607\n",
      "Iteration 777, loss = 0.09524415\n",
      "Iteration 778, loss = 0.09497006\n",
      "Iteration 779, loss = 0.09492647\n",
      "Iteration 780, loss = 0.09505942\n",
      "Iteration 781, loss = 0.09515897\n",
      "Iteration 782, loss = 0.09514815\n",
      "Iteration 783, loss = 0.09477100\n",
      "Iteration 784, loss = 0.09505517\n",
      "Iteration 785, loss = 0.09478195\n",
      "Iteration 786, loss = 0.09499325\n",
      "Iteration 787, loss = 0.09503481\n",
      "Iteration 788, loss = 0.09486906\n",
      "Iteration 789, loss = 0.09478209\n",
      "Iteration 790, loss = 0.09482780\n",
      "Iteration 791, loss = 0.09470304\n",
      "Iteration 792, loss = 0.09457235\n",
      "Iteration 793, loss = 0.09454698\n",
      "Iteration 794, loss = 0.09455995\n",
      "Iteration 795, loss = 0.09464877\n",
      "Iteration 796, loss = 0.09453224\n",
      "Iteration 797, loss = 0.09457153\n",
      "Iteration 798, loss = 0.09484132\n",
      "Iteration 799, loss = 0.09506409\n",
      "Iteration 800, loss = 0.09419852\n",
      "Iteration 801, loss = 0.09431779\n",
      "Iteration 802, loss = 0.09436753\n",
      "Iteration 803, loss = 0.09414337\n",
      "Iteration 804, loss = 0.09420347\n",
      "Iteration 805, loss = 0.09430807\n",
      "Iteration 806, loss = 0.09442139\n",
      "Iteration 807, loss = 0.09456345\n",
      "Iteration 808, loss = 0.09434460\n",
      "Iteration 809, loss = 0.09413800\n",
      "Iteration 810, loss = 0.09411206\n",
      "Iteration 811, loss = 0.09480928\n",
      "Iteration 812, loss = 0.09399009\n",
      "Iteration 813, loss = 0.09416438\n",
      "Iteration 814, loss = 0.09399562\n",
      "Iteration 815, loss = 0.09393344\n",
      "Iteration 816, loss = 0.09369950\n",
      "Iteration 817, loss = 0.09385326\n",
      "Iteration 818, loss = 0.09380491\n",
      "Iteration 819, loss = 0.09397826\n",
      "Iteration 820, loss = 0.09383924\n",
      "Iteration 821, loss = 0.09371984\n",
      "Iteration 822, loss = 0.09374609\n",
      "Iteration 823, loss = 0.09375494\n",
      "Iteration 824, loss = 0.09379251\n",
      "Iteration 825, loss = 0.09340557\n",
      "Iteration 826, loss = 0.09372627\n",
      "Iteration 827, loss = 0.09424756\n",
      "Iteration 828, loss = 0.09364019\n",
      "Iteration 829, loss = 0.09358683\n",
      "Iteration 830, loss = 0.09357043\n",
      "Iteration 831, loss = 0.09340111\n",
      "Iteration 832, loss = 0.09390235\n",
      "Iteration 833, loss = 0.09356550\n",
      "Iteration 834, loss = 0.09351895\n",
      "Iteration 835, loss = 0.09316652\n",
      "Iteration 836, loss = 0.09311704\n",
      "Iteration 837, loss = 0.09322747\n",
      "Iteration 838, loss = 0.09356298\n",
      "Iteration 839, loss = 0.09348155\n",
      "Iteration 840, loss = 0.09344555\n",
      "Iteration 841, loss = 0.09301122\n",
      "Iteration 842, loss = 0.09318620\n",
      "Iteration 843, loss = 0.09313671\n",
      "Iteration 844, loss = 0.09311491\n",
      "Iteration 845, loss = 0.09294134\n",
      "Iteration 846, loss = 0.09312248\n",
      "Iteration 847, loss = 0.09313907\n",
      "Iteration 848, loss = 0.09324742\n",
      "Iteration 849, loss = 0.09285265\n",
      "Iteration 850, loss = 0.09277175\n",
      "Iteration 851, loss = 0.09283637\n",
      "Iteration 852, loss = 0.09294113\n",
      "Iteration 853, loss = 0.09294448\n",
      "Iteration 854, loss = 0.09275915\n",
      "Iteration 855, loss = 0.09250984\n",
      "Iteration 856, loss = 0.09289892\n",
      "Iteration 857, loss = 0.09327197\n",
      "Iteration 858, loss = 0.09256649\n",
      "Iteration 859, loss = 0.09268414\n",
      "Iteration 860, loss = 0.09250763\n",
      "Iteration 861, loss = 0.09257928\n",
      "Iteration 862, loss = 0.09253364\n",
      "Iteration 863, loss = 0.09245862\n",
      "Iteration 864, loss = 0.09275057\n",
      "Iteration 865, loss = 0.09252699\n",
      "Iteration 866, loss = 0.09263552\n",
      "Iteration 867, loss = 0.09222580\n",
      "Iteration 868, loss = 0.09269133\n",
      "Iteration 869, loss = 0.09234543\n",
      "Iteration 870, loss = 0.09262618\n",
      "Iteration 871, loss = 0.09251756\n",
      "Iteration 872, loss = 0.09230930\n",
      "Iteration 873, loss = 0.09234505\n",
      "Iteration 874, loss = 0.09231212\n",
      "Iteration 875, loss = 0.09244526\n",
      "Iteration 876, loss = 0.09206871\n",
      "Iteration 877, loss = 0.09201618\n",
      "Iteration 878, loss = 0.09253880\n",
      "Iteration 879, loss = 0.09194940\n",
      "Iteration 880, loss = 0.09193585\n",
      "Iteration 881, loss = 0.09222849\n",
      "Iteration 882, loss = 0.09207478\n",
      "Iteration 883, loss = 0.09182125\n",
      "Iteration 884, loss = 0.09189240\n",
      "Iteration 885, loss = 0.09187789\n",
      "Iteration 886, loss = 0.09275268\n",
      "Iteration 887, loss = 0.09229794\n",
      "Iteration 888, loss = 0.09178921\n",
      "Iteration 889, loss = 0.09181003\n",
      "Iteration 890, loss = 0.09189134\n",
      "Iteration 891, loss = 0.09166190\n",
      "Iteration 892, loss = 0.09166373\n",
      "Iteration 893, loss = 0.09178450\n",
      "Iteration 894, loss = 0.09172482\n",
      "Iteration 895, loss = 0.09179325\n",
      "Iteration 896, loss = 0.09186187\n",
      "Iteration 897, loss = 0.09169956\n",
      "Iteration 898, loss = 0.09164180\n",
      "Iteration 899, loss = 0.09151639\n",
      "Iteration 900, loss = 0.09189636\n",
      "Iteration 901, loss = 0.09184566\n",
      "Iteration 902, loss = 0.09155825\n",
      "Iteration 903, loss = 0.09128338\n",
      "Iteration 904, loss = 0.09230773\n",
      "Iteration 905, loss = 0.09115522\n",
      "Iteration 906, loss = 0.09124488\n",
      "Iteration 907, loss = 0.09140079\n",
      "Iteration 908, loss = 0.09153522\n",
      "Iteration 909, loss = 0.09118239\n",
      "Iteration 910, loss = 0.09132955\n",
      "Iteration 911, loss = 0.09159679\n",
      "Iteration 912, loss = 0.09157642\n",
      "Iteration 913, loss = 0.09167319\n",
      "Iteration 914, loss = 0.09110340\n",
      "Iteration 915, loss = 0.09092867\n",
      "Iteration 916, loss = 0.09108437\n",
      "Iteration 917, loss = 0.09106822\n",
      "Iteration 918, loss = 0.09105215\n",
      "Iteration 919, loss = 0.09122396\n",
      "Iteration 920, loss = 0.09112899\n",
      "Iteration 921, loss = 0.09150542\n",
      "Iteration 922, loss = 0.09084934\n",
      "Iteration 923, loss = 0.09094031\n",
      "Iteration 924, loss = 0.09088193\n",
      "Iteration 925, loss = 0.09121413\n",
      "Iteration 926, loss = 0.09109398\n",
      "Iteration 927, loss = 0.09119878\n",
      "Iteration 928, loss = 0.09093684\n",
      "Iteration 929, loss = 0.09103206\n",
      "Iteration 930, loss = 0.09105265\n",
      "Iteration 931, loss = 0.09060769\n",
      "Iteration 932, loss = 0.09114936\n",
      "Iteration 933, loss = 0.09119977\n",
      "Iteration 934, loss = 0.09073377\n",
      "Iteration 935, loss = 0.09083196\n",
      "Iteration 936, loss = 0.09050972\n",
      "Iteration 937, loss = 0.09067817\n",
      "Iteration 938, loss = 0.09065139\n",
      "Iteration 939, loss = 0.09046942\n",
      "Iteration 940, loss = 0.09075230\n",
      "Iteration 941, loss = 0.09049814\n",
      "Iteration 942, loss = 0.09073497\n",
      "Iteration 943, loss = 0.09072530\n",
      "Iteration 944, loss = 0.09084572\n",
      "Iteration 945, loss = 0.09023210\n",
      "Iteration 946, loss = 0.09063876\n",
      "Iteration 947, loss = 0.09046771\n",
      "Iteration 948, loss = 0.09038054\n",
      "Iteration 949, loss = 0.09042374\n",
      "Iteration 950, loss = 0.09046922\n",
      "Iteration 951, loss = 0.09038865\n",
      "Iteration 952, loss = 0.09021912\n",
      "Iteration 953, loss = 0.09019204\n",
      "Iteration 954, loss = 0.09043404\n",
      "Iteration 955, loss = 0.09050591\n",
      "Iteration 956, loss = 0.09029787\n",
      "Iteration 957, loss = 0.09009529\n",
      "Iteration 958, loss = 0.09007855\n",
      "Iteration 959, loss = 0.09066928\n",
      "Iteration 960, loss = 0.09011189\n",
      "Iteration 961, loss = 0.09013063\n",
      "Iteration 962, loss = 0.08995367\n",
      "Iteration 963, loss = 0.09016027\n",
      "Iteration 964, loss = 0.09003741\n",
      "Iteration 965, loss = 0.09031812\n",
      "Iteration 966, loss = 0.09013398\n",
      "Iteration 967, loss = 0.09001415\n",
      "Iteration 968, loss = 0.08998205\n",
      "Iteration 969, loss = 0.09073870\n",
      "Iteration 970, loss = 0.08993145\n",
      "Iteration 971, loss = 0.08979523\n",
      "Iteration 972, loss = 0.09007117\n",
      "Iteration 973, loss = 0.09053396\n",
      "Iteration 974, loss = 0.08992942\n",
      "Iteration 975, loss = 0.09013879\n",
      "Iteration 976, loss = 0.08955850\n",
      "Iteration 977, loss = 0.08987456\n",
      "Iteration 978, loss = 0.08976646\n",
      "Iteration 979, loss = 0.08979115\n",
      "Iteration 980, loss = 0.08982494\n",
      "Iteration 981, loss = 0.09014240\n",
      "Iteration 982, loss = 0.08989495\n",
      "Iteration 983, loss = 0.08998854\n",
      "Iteration 984, loss = 0.08952617\n",
      "Iteration 985, loss = 0.08971832\n",
      "Iteration 986, loss = 0.08974531\n",
      "Iteration 987, loss = 0.08958571\n",
      "Iteration 988, loss = 0.08952478\n",
      "Iteration 989, loss = 0.09026389\n",
      "Iteration 990, loss = 0.08960996\n",
      "Iteration 991, loss = 0.08928835\n",
      "Iteration 992, loss = 0.08938482\n",
      "Iteration 993, loss = 0.08927767\n",
      "Iteration 994, loss = 0.08932540\n",
      "Iteration 995, loss = 0.08944652\n",
      "Iteration 996, loss = 0.08953479\n",
      "Iteration 997, loss = 0.08958005\n",
      "Iteration 998, loss = 0.08924946\n",
      "Iteration 999, loss = 0.08939122\n",
      "Iteration 1000, loss = 0.08943038\n",
      "Iteration 1001, loss = 0.08975020\n",
      "Iteration 1002, loss = 0.08991241\n",
      "Iteration 1003, loss = 0.08905233\n",
      "Iteration 1004, loss = 0.08952816\n",
      "Iteration 1005, loss = 0.08927638\n",
      "Iteration 1006, loss = 0.08952317\n",
      "Iteration 1007, loss = 0.08961796\n",
      "Iteration 1008, loss = 0.08907415\n",
      "Iteration 1009, loss = 0.08915392\n",
      "Iteration 1010, loss = 0.08931002\n",
      "Iteration 1011, loss = 0.08906140\n",
      "Iteration 1012, loss = 0.08897994\n",
      "Iteration 1013, loss = 0.08931314\n",
      "Iteration 1014, loss = 0.08921059\n",
      "Iteration 1015, loss = 0.08949670\n",
      "Iteration 1016, loss = 0.08922858\n",
      "Iteration 1017, loss = 0.08887174\n",
      "Iteration 1018, loss = 0.08898204\n",
      "Iteration 1019, loss = 0.08947758\n",
      "Iteration 1020, loss = 0.08894616\n",
      "Iteration 1021, loss = 0.08895408\n",
      "Iteration 1022, loss = 0.08916196\n",
      "Iteration 1023, loss = 0.08868573\n",
      "Iteration 1024, loss = 0.08910549\n",
      "Iteration 1025, loss = 0.08901732\n",
      "Iteration 1026, loss = 0.08867461\n",
      "Iteration 1027, loss = 0.08891015\n",
      "Iteration 1028, loss = 0.08911548\n",
      "Iteration 1029, loss = 0.08919191\n",
      "Iteration 1030, loss = 0.08896227\n",
      "Iteration 1031, loss = 0.08872014\n",
      "Iteration 1032, loss = 0.08883800\n",
      "Iteration 1033, loss = 0.08864451\n",
      "Iteration 1034, loss = 0.08899221\n",
      "Iteration 1035, loss = 0.08864037\n",
      "Iteration 1036, loss = 0.08867874\n",
      "Iteration 1037, loss = 0.08831064\n",
      "Iteration 1038, loss = 0.08832147\n",
      "Iteration 1039, loss = 0.08884274\n",
      "Iteration 1040, loss = 0.08858140\n",
      "Iteration 1041, loss = 0.08878774\n",
      "Iteration 1042, loss = 0.08859291\n",
      "Iteration 1043, loss = 0.08842615\n",
      "Iteration 1044, loss = 0.08877135\n",
      "Iteration 1045, loss = 0.08871466\n",
      "Iteration 1046, loss = 0.08828272\n",
      "Iteration 1047, loss = 0.08829861\n",
      "Iteration 1048, loss = 0.08839954\n",
      "Iteration 1049, loss = 0.08819457\n",
      "Iteration 1050, loss = 0.08868350\n",
      "Iteration 1051, loss = 0.08851250\n",
      "Iteration 1052, loss = 0.08834800\n",
      "Iteration 1053, loss = 0.08869744\n",
      "Iteration 1054, loss = 0.08840926\n",
      "Iteration 1055, loss = 0.08827755\n",
      "Iteration 1056, loss = 0.08827246\n",
      "Iteration 1057, loss = 0.08792798\n",
      "Iteration 1058, loss = 0.08804716\n",
      "Iteration 1059, loss = 0.08827982\n",
      "Iteration 1060, loss = 0.08856280\n",
      "Iteration 1061, loss = 0.08831388\n",
      "Iteration 1062, loss = 0.08818870\n",
      "Iteration 1063, loss = 0.08788332\n",
      "Iteration 1064, loss = 0.08816239\n",
      "Iteration 1065, loss = 0.08833394\n",
      "Iteration 1066, loss = 0.08846849\n",
      "Iteration 1067, loss = 0.08825245\n",
      "Iteration 1068, loss = 0.08830497\n",
      "Iteration 1069, loss = 0.08808869\n",
      "Iteration 1070, loss = 0.08794238\n",
      "Iteration 1071, loss = 0.08822727\n",
      "Iteration 1072, loss = 0.08823231\n",
      "Iteration 1073, loss = 0.08782261\n",
      "Iteration 1074, loss = 0.08806320\n",
      "Iteration 1075, loss = 0.08826489\n",
      "Iteration 1076, loss = 0.08812650\n",
      "Iteration 1077, loss = 0.08808548\n",
      "Iteration 1078, loss = 0.08787485\n",
      "Iteration 1079, loss = 0.08819038\n",
      "Iteration 1080, loss = 0.08781919\n",
      "Iteration 1081, loss = 0.08786289\n",
      "Iteration 1082, loss = 0.08768504\n",
      "Iteration 1083, loss = 0.08780926\n",
      "Iteration 1084, loss = 0.08758014\n",
      "Iteration 1085, loss = 0.08792522\n",
      "Iteration 1086, loss = 0.08777755\n",
      "Iteration 1087, loss = 0.08794653\n",
      "Iteration 1088, loss = 0.08793950\n",
      "Iteration 1089, loss = 0.08786824\n",
      "Iteration 1090, loss = 0.08755128\n",
      "Iteration 1091, loss = 0.08774556\n",
      "Iteration 1092, loss = 0.08755430\n",
      "Iteration 1093, loss = 0.08770006\n",
      "Iteration 1094, loss = 0.08785748\n",
      "Iteration 1095, loss = 0.08773815\n",
      "Iteration 1096, loss = 0.08760898\n",
      "Iteration 1097, loss = 0.08812781\n",
      "Iteration 1098, loss = 0.08774578\n",
      "Iteration 1099, loss = 0.08750719\n",
      "Iteration 1100, loss = 0.08738966\n",
      "Iteration 1101, loss = 0.08756560\n",
      "Iteration 1102, loss = 0.08761459\n",
      "Iteration 1103, loss = 0.08736343\n",
      "Iteration 1104, loss = 0.08739585\n",
      "Iteration 1105, loss = 0.08830000\n",
      "Iteration 1106, loss = 0.08741339\n",
      "Iteration 1107, loss = 0.08726547\n",
      "Iteration 1108, loss = 0.08737133\n",
      "Iteration 1109, loss = 0.08732148\n",
      "Iteration 1110, loss = 0.08736576\n",
      "Iteration 1111, loss = 0.08738513\n",
      "Iteration 1112, loss = 0.08738998\n",
      "Iteration 1113, loss = 0.08752445\n",
      "Iteration 1114, loss = 0.08736356\n",
      "Iteration 1115, loss = 0.08772340\n",
      "Iteration 1116, loss = 0.08748072\n",
      "Iteration 1117, loss = 0.08710063\n",
      "Iteration 1118, loss = 0.08770673\n",
      "Iteration 1119, loss = 0.08745175\n",
      "Iteration 1120, loss = 0.08729620\n",
      "Iteration 1121, loss = 0.08802069\n",
      "Iteration 1122, loss = 0.08718288\n",
      "Iteration 1123, loss = 0.08745179\n",
      "Iteration 1124, loss = 0.08703860\n",
      "Iteration 1125, loss = 0.08729665\n",
      "Iteration 1126, loss = 0.08692809\n",
      "Iteration 1127, loss = 0.08684498\n",
      "Iteration 1128, loss = 0.08693970\n",
      "Iteration 1129, loss = 0.08718223\n",
      "Iteration 1130, loss = 0.08708342\n",
      "Iteration 1131, loss = 0.08724488\n",
      "Iteration 1132, loss = 0.08699444\n",
      "Iteration 1133, loss = 0.08697069\n",
      "Iteration 1134, loss = 0.08678981\n",
      "Iteration 1135, loss = 0.08711247\n",
      "Iteration 1136, loss = 0.08717608\n",
      "Iteration 1137, loss = 0.08703755\n",
      "Iteration 1138, loss = 0.08717635\n",
      "Iteration 1139, loss = 0.08678209\n",
      "Iteration 1140, loss = 0.08673281\n",
      "Iteration 1141, loss = 0.08743863\n",
      "Iteration 1142, loss = 0.08663298\n",
      "Iteration 1143, loss = 0.08671703\n",
      "Iteration 1144, loss = 0.08665098\n",
      "Iteration 1145, loss = 0.08690538\n",
      "Iteration 1146, loss = 0.08706348\n",
      "Iteration 1147, loss = 0.08732586\n",
      "Iteration 1148, loss = 0.08700569\n",
      "Iteration 1149, loss = 0.08636758\n",
      "Iteration 1150, loss = 0.08666987\n",
      "Iteration 1151, loss = 0.08666215\n",
      "Iteration 1152, loss = 0.08697185\n",
      "Iteration 1153, loss = 0.08695376\n",
      "Iteration 1154, loss = 0.08662531\n",
      "Iteration 1155, loss = 0.08653240\n",
      "Iteration 1156, loss = 0.08683764\n",
      "Iteration 1157, loss = 0.08663166\n",
      "Iteration 1158, loss = 0.08650900\n",
      "Iteration 1159, loss = 0.08689726\n",
      "Iteration 1160, loss = 0.08678514\n",
      "Iteration 1161, loss = 0.08669921\n",
      "Iteration 1162, loss = 0.08686028\n",
      "Iteration 1163, loss = 0.08641089\n",
      "Iteration 1164, loss = 0.08693073\n",
      "Iteration 1165, loss = 0.08617552\n",
      "Iteration 1166, loss = 0.08657994\n",
      "Iteration 1167, loss = 0.08653878\n",
      "Iteration 1168, loss = 0.08688756\n",
      "Iteration 1169, loss = 0.08637437\n",
      "Iteration 1170, loss = 0.08655241\n",
      "Iteration 1171, loss = 0.08634552\n",
      "Iteration 1172, loss = 0.08656787\n",
      "Iteration 1173, loss = 0.08627064\n",
      "Iteration 1174, loss = 0.08658328\n",
      "Iteration 1175, loss = 0.08631473\n",
      "Iteration 1176, loss = 0.08681974\n",
      "Iteration 1177, loss = 0.08634126\n",
      "Iteration 1178, loss = 0.08637922\n",
      "Iteration 1179, loss = 0.08664543\n",
      "Iteration 1180, loss = 0.08628727\n",
      "Iteration 1181, loss = 0.08630133\n",
      "Iteration 1182, loss = 0.08643610\n",
      "Iteration 1183, loss = 0.08644019\n",
      "Iteration 1184, loss = 0.08616299\n",
      "Iteration 1185, loss = 0.08609986\n",
      "Iteration 1186, loss = 0.08609163\n",
      "Iteration 1187, loss = 0.08674305\n",
      "Iteration 1188, loss = 0.08606952\n",
      "Iteration 1189, loss = 0.08619557\n",
      "Iteration 1190, loss = 0.08631829\n",
      "Iteration 1191, loss = 0.08628987\n",
      "Iteration 1192, loss = 0.08639216\n",
      "Iteration 1193, loss = 0.08612211\n",
      "Iteration 1194, loss = 0.08657098\n",
      "Iteration 1195, loss = 0.08605442\n",
      "Iteration 1196, loss = 0.08646120\n",
      "Iteration 1197, loss = 0.08623771\n",
      "Iteration 1198, loss = 0.08641522\n",
      "Iteration 1199, loss = 0.08620811\n",
      "Iteration 1200, loss = 0.08584520\n",
      "Iteration 1201, loss = 0.08601368\n",
      "Iteration 1202, loss = 0.08620005\n",
      "Iteration 1203, loss = 0.08609628\n",
      "Iteration 1204, loss = 0.08622108\n",
      "Iteration 1205, loss = 0.08600121\n",
      "Iteration 1206, loss = 0.08627546\n",
      "Iteration 1207, loss = 0.08582715\n",
      "Iteration 1208, loss = 0.08572892\n",
      "Iteration 1209, loss = 0.08568705\n",
      "Iteration 1210, loss = 0.08591685\n",
      "Iteration 1211, loss = 0.08614935\n",
      "Iteration 1212, loss = 0.08587934\n",
      "Iteration 1213, loss = 0.08613205\n",
      "Iteration 1214, loss = 0.08620738\n",
      "Iteration 1215, loss = 0.08597037\n",
      "Iteration 1216, loss = 0.08617777\n",
      "Iteration 1217, loss = 0.08638619\n",
      "Iteration 1218, loss = 0.08587538\n",
      "Iteration 1219, loss = 0.08569321\n",
      "Iteration 1220, loss = 0.08584288\n",
      "Iteration 1221, loss = 0.08683620\n",
      "Iteration 1222, loss = 0.08580509\n",
      "Iteration 1223, loss = 0.08578344\n",
      "Iteration 1224, loss = 0.08542544\n",
      "Iteration 1225, loss = 0.08563678\n",
      "Iteration 1226, loss = 0.08571471\n",
      "Iteration 1227, loss = 0.08576812\n",
      "Iteration 1228, loss = 0.08615867\n",
      "Iteration 1229, loss = 0.08556972\n",
      "Iteration 1230, loss = 0.08598994\n",
      "Iteration 1231, loss = 0.08608899\n",
      "Iteration 1232, loss = 0.08540954\n",
      "Iteration 1233, loss = 0.08571487\n",
      "Iteration 1234, loss = 0.08529359\n",
      "Iteration 1235, loss = 0.08561172\n",
      "Iteration 1236, loss = 0.08582122\n",
      "Iteration 1237, loss = 0.08561715\n",
      "Iteration 1238, loss = 0.08595739\n",
      "Iteration 1239, loss = 0.08598305\n",
      "Iteration 1240, loss = 0.08550617\n",
      "Iteration 1241, loss = 0.08552562\n",
      "Iteration 1242, loss = 0.08557856\n",
      "Iteration 1243, loss = 0.08540647\n",
      "Iteration 1244, loss = 0.08542759\n",
      "Iteration 1245, loss = 0.08551657\n",
      "Iteration 1246, loss = 0.08548009\n",
      "Iteration 1247, loss = 0.08564925\n",
      "Iteration 1248, loss = 0.08536596\n",
      "Iteration 1249, loss = 0.08524134\n",
      "Iteration 1250, loss = 0.08571526\n",
      "Iteration 1251, loss = 0.08569221\n",
      "Iteration 1252, loss = 0.08541373\n",
      "Iteration 1253, loss = 0.08527657\n",
      "Iteration 1254, loss = 0.08551795\n",
      "Iteration 1255, loss = 0.08572660\n",
      "Iteration 1256, loss = 0.08530204\n",
      "Iteration 1257, loss = 0.08515132\n",
      "Iteration 1258, loss = 0.08526357\n",
      "Iteration 1259, loss = 0.08522917\n",
      "Iteration 1260, loss = 0.08565023\n",
      "Iteration 1261, loss = 0.08540883\n",
      "Iteration 1262, loss = 0.08532562\n",
      "Iteration 1263, loss = 0.08512628\n",
      "Iteration 1264, loss = 0.08551790\n",
      "Iteration 1265, loss = 0.08510908\n",
      "Iteration 1266, loss = 0.08561197\n",
      "Iteration 1267, loss = 0.08539315\n",
      "Iteration 1268, loss = 0.08542168\n",
      "Iteration 1269, loss = 0.08541330\n",
      "Iteration 1270, loss = 0.08532053\n",
      "Iteration 1271, loss = 0.08562190\n",
      "Iteration 1272, loss = 0.08550456\n",
      "Iteration 1273, loss = 0.08513243\n",
      "Iteration 1274, loss = 0.08510788\n",
      "Iteration 1275, loss = 0.08531535\n",
      "Iteration 1276, loss = 0.08517447\n",
      "Iteration 1277, loss = 0.08490603\n",
      "Iteration 1278, loss = 0.08498325\n",
      "Iteration 1279, loss = 0.08507404\n",
      "Iteration 1280, loss = 0.08524299\n",
      "Iteration 1281, loss = 0.08531446\n",
      "Iteration 1282, loss = 0.08495700\n",
      "Iteration 1283, loss = 0.08513890\n",
      "Iteration 1284, loss = 0.08488342\n",
      "Iteration 1285, loss = 0.08517541\n",
      "Iteration 1286, loss = 0.08501696\n",
      "Iteration 1287, loss = 0.08467495\n",
      "Iteration 1288, loss = 0.08502415\n",
      "Iteration 1289, loss = 0.08528400\n",
      "Iteration 1290, loss = 0.08526033\n",
      "Iteration 1291, loss = 0.08488974\n",
      "Iteration 1292, loss = 0.08487476\n",
      "Iteration 1293, loss = 0.08495343\n",
      "Iteration 1294, loss = 0.08499022\n",
      "Iteration 1295, loss = 0.08471918\n",
      "Iteration 1296, loss = 0.08489960\n",
      "Iteration 1297, loss = 0.08533837\n",
      "Iteration 1298, loss = 0.08550967\n",
      "Iteration 1299, loss = 0.08551822\n",
      "Iteration 1300, loss = 0.08474739\n",
      "Iteration 1301, loss = 0.08448602\n",
      "Iteration 1302, loss = 0.08483416\n",
      "Iteration 1303, loss = 0.08477477\n",
      "Iteration 1304, loss = 0.08514184\n",
      "Iteration 1305, loss = 0.08487716\n",
      "Iteration 1306, loss = 0.08462645\n",
      "Iteration 1307, loss = 0.08470268\n",
      "Iteration 1308, loss = 0.08481845\n",
      "Iteration 1309, loss = 0.08458855\n",
      "Iteration 1310, loss = 0.08481590\n",
      "Iteration 1311, loss = 0.08505813\n",
      "Iteration 1312, loss = 0.08490051\n",
      "Iteration 1313, loss = 0.08452935\n",
      "Iteration 1314, loss = 0.08541899\n",
      "Iteration 1315, loss = 0.08523059\n",
      "Iteration 1316, loss = 0.08462033\n",
      "Iteration 1317, loss = 0.08457602\n",
      "Iteration 1318, loss = 0.08463958\n",
      "Iteration 1319, loss = 0.08432786\n",
      "Iteration 1320, loss = 0.08465468\n",
      "Iteration 1321, loss = 0.08474943\n",
      "Iteration 1322, loss = 0.08447046\n",
      "Iteration 1323, loss = 0.08473502\n",
      "Iteration 1324, loss = 0.08452895\n",
      "Iteration 1325, loss = 0.08464733\n",
      "Iteration 1326, loss = 0.08483658\n",
      "Iteration 1327, loss = 0.08463891\n",
      "Iteration 1328, loss = 0.08452314\n",
      "Iteration 1329, loss = 0.08461873\n",
      "Iteration 1330, loss = 0.08430108\n",
      "Iteration 1331, loss = 0.08462527\n",
      "Iteration 1332, loss = 0.08433887\n",
      "Iteration 1333, loss = 0.08455272\n",
      "Iteration 1334, loss = 0.08472315\n",
      "Iteration 1335, loss = 0.08472515\n",
      "Iteration 1336, loss = 0.08431639\n",
      "Iteration 1337, loss = 0.08430704\n",
      "Iteration 1338, loss = 0.08458190\n",
      "Iteration 1339, loss = 0.08446872\n",
      "Iteration 1340, loss = 0.08460611\n",
      "Iteration 1341, loss = 0.08503590\n",
      "Iteration 1342, loss = 0.08449943\n",
      "Iteration 1343, loss = 0.08420160\n",
      "Iteration 1344, loss = 0.08401808\n",
      "Iteration 1345, loss = 0.08456733\n",
      "Iteration 1346, loss = 0.08455128\n",
      "Iteration 1347, loss = 0.08467896\n",
      "Iteration 1348, loss = 0.08467810\n",
      "Iteration 1349, loss = 0.08399810\n",
      "Iteration 1350, loss = 0.08418612\n",
      "Iteration 1351, loss = 0.08466605\n",
      "Iteration 1352, loss = 0.08432290\n",
      "Iteration 1353, loss = 0.08440732\n",
      "Iteration 1354, loss = 0.08438463\n",
      "Iteration 1355, loss = 0.08397493\n",
      "Iteration 1356, loss = 0.08431071\n",
      "Iteration 1357, loss = 0.08440514\n",
      "Iteration 1358, loss = 0.08457822\n",
      "Iteration 1359, loss = 0.08497077\n",
      "Iteration 1360, loss = 0.08395345\n",
      "Iteration 1361, loss = 0.08389954\n",
      "Iteration 1362, loss = 0.08400490\n",
      "Iteration 1363, loss = 0.08429763\n",
      "Iteration 1364, loss = 0.08465396\n",
      "Iteration 1365, loss = 0.08409202\n",
      "Iteration 1366, loss = 0.08397323\n",
      "Iteration 1367, loss = 0.08419811\n",
      "Iteration 1368, loss = 0.08433604\n",
      "Iteration 1369, loss = 0.08489409\n",
      "Iteration 1370, loss = 0.08397235\n",
      "Iteration 1371, loss = 0.08392612\n",
      "Iteration 1372, loss = 0.08438708\n",
      "Iteration 1373, loss = 0.08397148\n",
      "Iteration 1374, loss = 0.08608258\n",
      "Iteration 1375, loss = 0.08401155\n",
      "Iteration 1376, loss = 0.08440123\n",
      "Iteration 1377, loss = 0.08407150\n",
      "Iteration 1378, loss = 0.08387771\n",
      "Iteration 1379, loss = 0.08362781\n",
      "Iteration 1380, loss = 0.08424422\n",
      "Iteration 1381, loss = 0.08405909\n",
      "Iteration 1382, loss = 0.08412010\n",
      "Iteration 1383, loss = 0.08395670\n",
      "Iteration 1384, loss = 0.08428787\n",
      "Iteration 1385, loss = 0.08402338\n",
      "Iteration 1386, loss = 0.08405957\n",
      "Iteration 1387, loss = 0.08377723\n",
      "Iteration 1388, loss = 0.08388696\n",
      "Iteration 1389, loss = 0.08379459\n",
      "Iteration 1390, loss = 0.08406737\n",
      "Iteration 1391, loss = 0.08387630\n",
      "Iteration 1392, loss = 0.08485038\n",
      "Iteration 1393, loss = 0.08389037\n",
      "Iteration 1394, loss = 0.08403809\n",
      "Iteration 1395, loss = 0.08388127\n",
      "Iteration 1396, loss = 0.08385525\n",
      "Iteration 1397, loss = 0.08375821\n",
      "Iteration 1398, loss = 0.08385841\n",
      "Iteration 1399, loss = 0.08377857\n",
      "Iteration 1400, loss = 0.08366116\n",
      "Iteration 1401, loss = 0.08361038\n",
      "Iteration 1402, loss = 0.08354913\n",
      "Iteration 1403, loss = 0.08401925\n",
      "Iteration 1404, loss = 0.08364383\n",
      "Iteration 1405, loss = 0.08463466\n",
      "Iteration 1406, loss = 0.08385170\n",
      "Iteration 1407, loss = 0.08410261\n",
      "Iteration 1408, loss = 0.08344083\n",
      "Iteration 1409, loss = 0.08412950\n",
      "Iteration 1410, loss = 0.08408910\n",
      "Iteration 1411, loss = 0.08385671\n",
      "Iteration 1412, loss = 0.08387418\n",
      "Iteration 1413, loss = 0.08359140\n",
      "Iteration 1414, loss = 0.08348847\n",
      "Iteration 1415, loss = 0.08373638\n",
      "Iteration 1416, loss = 0.08383229\n",
      "Iteration 1417, loss = 0.08374540\n",
      "Iteration 1418, loss = 0.08352399\n",
      "Iteration 1419, loss = 0.08368983\n",
      "Iteration 1420, loss = 0.08364093\n",
      "Iteration 1421, loss = 0.08375139\n",
      "Iteration 1422, loss = 0.08399583\n",
      "Iteration 1423, loss = 0.08426591\n",
      "Iteration 1424, loss = 0.08344554\n",
      "Iteration 1425, loss = 0.08352618\n",
      "Iteration 1426, loss = 0.08345642\n",
      "Iteration 1427, loss = 0.08384291\n",
      "Iteration 1428, loss = 0.08348009\n",
      "Iteration 1429, loss = 0.08368970\n",
      "Iteration 1430, loss = 0.08382901\n",
      "Iteration 1431, loss = 0.08334956\n",
      "Iteration 1432, loss = 0.08356204\n",
      "Iteration 1433, loss = 0.08342756\n",
      "Iteration 1434, loss = 0.08370244\n",
      "Iteration 1435, loss = 0.08329012\n",
      "Iteration 1436, loss = 0.08338937\n",
      "Iteration 1437, loss = 0.08391051\n",
      "Iteration 1438, loss = 0.08367944\n",
      "Iteration 1439, loss = 0.08349155\n",
      "Iteration 1440, loss = 0.08355522\n",
      "Iteration 1441, loss = 0.08361085\n",
      "Iteration 1442, loss = 0.08359902\n",
      "Iteration 1443, loss = 0.08376673\n",
      "Iteration 1444, loss = 0.08395456\n",
      "Iteration 1445, loss = 0.08355308\n",
      "Iteration 1446, loss = 0.08338929\n",
      "Iteration 1447, loss = 0.08341492\n",
      "Iteration 1448, loss = 0.08331749\n",
      "Iteration 1449, loss = 0.08313642\n",
      "Iteration 1450, loss = 0.08332530\n",
      "Iteration 1451, loss = 0.08326350\n",
      "Iteration 1452, loss = 0.08311965\n",
      "Iteration 1453, loss = 0.08328909\n",
      "Iteration 1454, loss = 0.08353207\n",
      "Iteration 1455, loss = 0.08346225\n",
      "Iteration 1456, loss = 0.08310470\n",
      "Iteration 1457, loss = 0.08358227\n",
      "Iteration 1458, loss = 0.08326479\n",
      "Iteration 1459, loss = 0.08323232\n",
      "Iteration 1460, loss = 0.08357492\n",
      "Iteration 1461, loss = 0.08320974\n",
      "Iteration 1462, loss = 0.08314181\n",
      "Iteration 1463, loss = 0.08377082\n",
      "Iteration 1464, loss = 0.08344709\n",
      "Iteration 1465, loss = 0.08342983\n",
      "Iteration 1466, loss = 0.08325116\n",
      "Iteration 1467, loss = 0.08335941\n",
      "Iteration 1468, loss = 0.08310915\n",
      "Iteration 1469, loss = 0.08337396\n",
      "Iteration 1470, loss = 0.08324949\n",
      "Iteration 1471, loss = 0.08325989\n",
      "Iteration 1472, loss = 0.08324054\n",
      "Iteration 1473, loss = 0.08342407\n",
      "Iteration 1474, loss = 0.08365625\n",
      "Iteration 1475, loss = 0.08322629\n",
      "Iteration 1476, loss = 0.08356694\n",
      "Iteration 1477, loss = 0.08337735\n",
      "Iteration 1478, loss = 0.08336722\n",
      "Iteration 1479, loss = 0.08292515\n",
      "Iteration 1480, loss = 0.08319591\n",
      "Iteration 1481, loss = 0.08312538\n",
      "Iteration 1482, loss = 0.08319986\n",
      "Iteration 1483, loss = 0.08322897\n",
      "Iteration 1484, loss = 0.08295925\n",
      "Iteration 1485, loss = 0.08366391\n",
      "Iteration 1486, loss = 0.08329650\n",
      "Iteration 1487, loss = 0.08286969\n",
      "Iteration 1488, loss = 0.08303456\n",
      "Iteration 1489, loss = 0.08305823\n",
      "Iteration 1490, loss = 0.08312651\n",
      "Iteration 1491, loss = 0.08292667\n",
      "Iteration 1492, loss = 0.08332838\n",
      "Iteration 1493, loss = 0.08317500\n",
      "Iteration 1494, loss = 0.08313153\n",
      "Iteration 1495, loss = 0.08312953\n",
      "Iteration 1496, loss = 0.08301938\n",
      "Iteration 1497, loss = 0.08292751\n",
      "Iteration 1498, loss = 0.08319523\n",
      "Iteration 1499, loss = 0.08288438\n",
      "Iteration 1500, loss = 0.08298902\n",
      "Iteration 1501, loss = 0.08316581\n",
      "Iteration 1502, loss = 0.08292755\n",
      "Iteration 1503, loss = 0.08304655\n",
      "Iteration 1504, loss = 0.08294492\n",
      "Iteration 1505, loss = 0.08273322\n",
      "Iteration 1506, loss = 0.08340795\n",
      "Iteration 1507, loss = 0.08298617\n",
      "Iteration 1508, loss = 0.08342236\n",
      "Iteration 1509, loss = 0.08363407\n",
      "Iteration 1510, loss = 0.08294886\n",
      "Iteration 1511, loss = 0.08280858\n",
      "Iteration 1512, loss = 0.08266060\n",
      "Iteration 1513, loss = 0.08292398\n",
      "Iteration 1514, loss = 0.08290628\n",
      "Iteration 1515, loss = 0.08295265\n",
      "Iteration 1516, loss = 0.08282835\n",
      "Iteration 1517, loss = 0.08291253\n",
      "Iteration 1518, loss = 0.08280305\n",
      "Iteration 1519, loss = 0.08293089\n",
      "Iteration 1520, loss = 0.08308144\n",
      "Iteration 1521, loss = 0.08289282\n",
      "Iteration 1522, loss = 0.08256252\n",
      "Iteration 1523, loss = 0.08307994\n",
      "Iteration 1524, loss = 0.08275115\n",
      "Iteration 1525, loss = 0.08250371\n",
      "Iteration 1526, loss = 0.08296582\n",
      "Iteration 1527, loss = 0.08245850\n",
      "Iteration 1528, loss = 0.08291891\n",
      "Iteration 1529, loss = 0.08290765\n",
      "Iteration 1530, loss = 0.08266033\n",
      "Iteration 1531, loss = 0.08300869\n",
      "Iteration 1532, loss = 0.08242264\n",
      "Iteration 1533, loss = 0.08249651\n",
      "Iteration 1534, loss = 0.08289481\n",
      "Iteration 1535, loss = 0.08279860\n",
      "Iteration 1536, loss = 0.08272263\n",
      "Iteration 1537, loss = 0.08247396\n",
      "Iteration 1538, loss = 0.08285878\n",
      "Iteration 1539, loss = 0.08304201\n",
      "Iteration 1540, loss = 0.08291501\n",
      "Iteration 1541, loss = 0.08258340\n",
      "Iteration 1542, loss = 0.08294735\n",
      "Iteration 1543, loss = 0.08268639\n",
      "Iteration 1544, loss = 0.08275177\n",
      "Iteration 1545, loss = 0.08253826\n",
      "Iteration 1546, loss = 0.08272393\n",
      "Iteration 1547, loss = 0.08284257\n",
      "Iteration 1548, loss = 0.08228398\n",
      "Iteration 1549, loss = 0.08273054\n",
      "Iteration 1550, loss = 0.08258524\n",
      "Iteration 1551, loss = 0.08255147\n",
      "Iteration 1552, loss = 0.08286155\n",
      "Iteration 1553, loss = 0.08276413\n",
      "Iteration 1554, loss = 0.08271851\n",
      "Iteration 1555, loss = 0.08250531\n",
      "Iteration 1556, loss = 0.08248080\n",
      "Iteration 1557, loss = 0.08252431\n",
      "Iteration 1558, loss = 0.08251600\n",
      "Iteration 1559, loss = 0.08307565\n",
      "Iteration 1560, loss = 0.08279120\n",
      "Iteration 1561, loss = 0.08279591\n",
      "Iteration 1562, loss = 0.08297360\n",
      "Iteration 1563, loss = 0.08221848\n",
      "Iteration 1564, loss = 0.08216221\n",
      "Iteration 1565, loss = 0.08253293\n",
      "Iteration 1566, loss = 0.08226470\n",
      "Iteration 1567, loss = 0.08231409\n",
      "Iteration 1568, loss = 0.08238050\n",
      "Iteration 1569, loss = 0.08318456\n",
      "Iteration 1570, loss = 0.08304704\n",
      "Iteration 1571, loss = 0.08288744\n",
      "Iteration 1572, loss = 0.08239999\n",
      "Iteration 1573, loss = 0.08264735\n",
      "Iteration 1574, loss = 0.08226337\n",
      "Iteration 1575, loss = 0.08254352\n",
      "Iteration 1576, loss = 0.08219444\n",
      "Iteration 1577, loss = 0.08220717\n",
      "Iteration 1578, loss = 0.08248416\n",
      "Iteration 1579, loss = 0.08254110\n",
      "Iteration 1580, loss = 0.08216236\n",
      "Iteration 1581, loss = 0.08230671\n",
      "Iteration 1582, loss = 0.08241501\n",
      "Iteration 1583, loss = 0.08243528\n",
      "Iteration 1584, loss = 0.08239460\n",
      "Iteration 1585, loss = 0.08237992\n",
      "Iteration 1586, loss = 0.08233782\n",
      "Iteration 1587, loss = 0.08216132\n",
      "Iteration 1588, loss = 0.08233188\n",
      "Iteration 1589, loss = 0.08226225\n",
      "Iteration 1590, loss = 0.08281597\n",
      "Iteration 1591, loss = 0.08255829\n",
      "Iteration 1592, loss = 0.08219685\n",
      "Iteration 1593, loss = 0.08237056\n",
      "Iteration 1594, loss = 0.08245763\n",
      "Iteration 1595, loss = 0.08219314\n",
      "Iteration 1596, loss = 0.08218642\n",
      "Iteration 1597, loss = 0.08222006\n",
      "Iteration 1598, loss = 0.08246714\n",
      "Iteration 1599, loss = 0.08210852\n",
      "Iteration 1600, loss = 0.08262555\n",
      "Iteration 1601, loss = 0.08230083\n",
      "Iteration 1602, loss = 0.08234660\n",
      "Iteration 1603, loss = 0.08290053\n",
      "Iteration 1604, loss = 0.08229330\n",
      "Iteration 1605, loss = 0.08229725\n",
      "Iteration 1606, loss = 0.08209212\n",
      "Iteration 1607, loss = 0.08227528\n",
      "Iteration 1608, loss = 0.08284946\n",
      "Iteration 1609, loss = 0.08222215\n",
      "Iteration 1610, loss = 0.08205857\n",
      "Iteration 1611, loss = 0.08192452\n",
      "Iteration 1612, loss = 0.08192131\n",
      "Iteration 1613, loss = 0.08222604\n",
      "Iteration 1614, loss = 0.08201252\n",
      "Iteration 1615, loss = 0.08205837\n",
      "Iteration 1616, loss = 0.08226199\n",
      "Iteration 1617, loss = 0.08209884\n",
      "Iteration 1618, loss = 0.08255122\n",
      "Iteration 1619, loss = 0.08244060\n",
      "Iteration 1620, loss = 0.08237680\n",
      "Iteration 1621, loss = 0.08206238\n",
      "Iteration 1622, loss = 0.08200373\n",
      "Iteration 1623, loss = 0.08225394\n",
      "Iteration 1624, loss = 0.08189983\n",
      "Iteration 1625, loss = 0.08200562\n",
      "Iteration 1626, loss = 0.08182219\n",
      "Iteration 1627, loss = 0.08235100\n",
      "Iteration 1628, loss = 0.08195547\n",
      "Iteration 1629, loss = 0.08211169\n",
      "Iteration 1630, loss = 0.08199635\n",
      "Iteration 1631, loss = 0.08212518\n",
      "Iteration 1632, loss = 0.08243131\n",
      "Iteration 1633, loss = 0.08172138\n",
      "Iteration 1634, loss = 0.08214550\n",
      "Iteration 1635, loss = 0.08227945\n",
      "Iteration 1636, loss = 0.08197251\n",
      "Iteration 1637, loss = 0.08276154\n",
      "Iteration 1638, loss = 0.08186455\n",
      "Iteration 1639, loss = 0.08187405\n",
      "Iteration 1640, loss = 0.08213876\n",
      "Iteration 1641, loss = 0.08221394\n",
      "Iteration 1642, loss = 0.08186198\n",
      "Iteration 1643, loss = 0.08187654\n",
      "Iteration 1644, loss = 0.08192892\n",
      "Iteration 1645, loss = 0.08190357\n",
      "Iteration 1646, loss = 0.08208434\n",
      "Iteration 1647, loss = 0.08188779\n",
      "Iteration 1648, loss = 0.08175302\n",
      "Iteration 1649, loss = 0.08200853\n",
      "Iteration 1650, loss = 0.08221488\n",
      "Iteration 1651, loss = 0.08225561\n",
      "Iteration 1652, loss = 0.08224120\n",
      "Iteration 1653, loss = 0.08169304\n",
      "Iteration 1654, loss = 0.08202475\n",
      "Iteration 1655, loss = 0.08202462\n",
      "Iteration 1656, loss = 0.08173935\n",
      "Iteration 1657, loss = 0.08166309\n",
      "Iteration 1658, loss = 0.08200941\n",
      "Iteration 1659, loss = 0.08187563\n",
      "Iteration 1660, loss = 0.08217336\n",
      "Iteration 1661, loss = 0.08206506\n",
      "Iteration 1662, loss = 0.08163161\n",
      "Iteration 1663, loss = 0.08166872\n",
      "Iteration 1664, loss = 0.08172571\n",
      "Iteration 1665, loss = 0.08149302\n",
      "Iteration 1666, loss = 0.08196110\n",
      "Iteration 1667, loss = 0.08175645\n",
      "Iteration 1668, loss = 0.08195628\n",
      "Iteration 1669, loss = 0.08208011\n",
      "Iteration 1670, loss = 0.08155764\n",
      "Iteration 1671, loss = 0.08198978\n",
      "Iteration 1672, loss = 0.08161092\n",
      "Iteration 1673, loss = 0.08160282\n",
      "Iteration 1674, loss = 0.08153495\n",
      "Iteration 1675, loss = 0.08178667\n",
      "Iteration 1676, loss = 0.08161110\n",
      "Iteration 1677, loss = 0.08243695\n",
      "Iteration 1678, loss = 0.08228150\n",
      "Iteration 1679, loss = 0.08146129\n",
      "Iteration 1680, loss = 0.08145263\n",
      "Iteration 1681, loss = 0.08158137\n",
      "Iteration 1682, loss = 0.08162546\n",
      "Iteration 1683, loss = 0.08162072\n",
      "Iteration 1684, loss = 0.08236272\n",
      "Iteration 1685, loss = 0.08183117\n",
      "Iteration 1686, loss = 0.08173661\n",
      "Iteration 1687, loss = 0.08182692\n",
      "Iteration 1688, loss = 0.08190265\n",
      "Iteration 1689, loss = 0.08134407\n",
      "Iteration 1690, loss = 0.08139027\n",
      "Iteration 1691, loss = 0.08150672\n",
      "Iteration 1692, loss = 0.08166706\n",
      "Iteration 1693, loss = 0.08174383\n",
      "Iteration 1694, loss = 0.08167364\n",
      "Iteration 1695, loss = 0.08202169\n",
      "Iteration 1696, loss = 0.08150534\n",
      "Iteration 1697, loss = 0.08142397\n",
      "Iteration 1698, loss = 0.08138610\n",
      "Iteration 1699, loss = 0.08139861\n",
      "Iteration 1700, loss = 0.08162387\n",
      "Iteration 1701, loss = 0.08145719\n",
      "Iteration 1702, loss = 0.08169205\n",
      "Iteration 1703, loss = 0.08168044\n",
      "Iteration 1704, loss = 0.08158328\n",
      "Iteration 1705, loss = 0.08175840\n",
      "Iteration 1706, loss = 0.08151556\n",
      "Iteration 1707, loss = 0.08264669\n",
      "Iteration 1708, loss = 0.08137336\n",
      "Iteration 1709, loss = 0.08123894\n",
      "Iteration 1710, loss = 0.08126226\n",
      "Iteration 1711, loss = 0.08135066\n",
      "Iteration 1712, loss = 0.08152160\n",
      "Iteration 1713, loss = 0.08166963\n",
      "Iteration 1714, loss = 0.08165799\n",
      "Iteration 1715, loss = 0.08167677\n",
      "Iteration 1716, loss = 0.08160025\n",
      "Iteration 1717, loss = 0.08155997\n",
      "Iteration 1718, loss = 0.08154879\n",
      "Iteration 1719, loss = 0.08125706\n",
      "Iteration 1720, loss = 0.08131880\n",
      "Iteration 1721, loss = 0.08163809\n",
      "Iteration 1722, loss = 0.08192329\n",
      "Iteration 1723, loss = 0.08166074\n",
      "Iteration 1724, loss = 0.08154024\n",
      "Iteration 1725, loss = 0.08125691\n",
      "Iteration 1726, loss = 0.08160466\n",
      "Iteration 1727, loss = 0.08154703\n",
      "Iteration 1728, loss = 0.08117702\n",
      "Iteration 1729, loss = 0.08156510\n",
      "Iteration 1730, loss = 0.08149990\n",
      "Iteration 1731, loss = 0.08144793\n",
      "Iteration 1732, loss = 0.08163643\n",
      "Iteration 1733, loss = 0.08149609\n",
      "Iteration 1734, loss = 0.08137887\n",
      "Iteration 1735, loss = 0.08163421\n",
      "Iteration 1736, loss = 0.08154825\n",
      "Iteration 1737, loss = 0.08153123\n",
      "Iteration 1738, loss = 0.08164841\n",
      "Iteration 1739, loss = 0.08112214\n",
      "Iteration 1740, loss = 0.08124599\n",
      "Iteration 1741, loss = 0.08128774\n",
      "Iteration 1742, loss = 0.08138185\n",
      "Iteration 1743, loss = 0.08168659\n",
      "Iteration 1744, loss = 0.08141717\n",
      "Iteration 1745, loss = 0.08148302\n",
      "Iteration 1746, loss = 0.08120523\n",
      "Iteration 1747, loss = 0.08114850\n",
      "Iteration 1748, loss = 0.08140825\n",
      "Iteration 1749, loss = 0.08148078\n",
      "Iteration 1750, loss = 0.08156090\n",
      "Iteration 1751, loss = 0.08125956\n",
      "Iteration 1752, loss = 0.08132950\n",
      "Iteration 1753, loss = 0.08140077\n",
      "Iteration 1754, loss = 0.08118859\n",
      "Iteration 1755, loss = 0.08109088\n",
      "Iteration 1756, loss = 0.08142187\n",
      "Iteration 1757, loss = 0.08116892\n",
      "Iteration 1758, loss = 0.08128396\n",
      "Iteration 1759, loss = 0.08218668\n",
      "Iteration 1760, loss = 0.08137262\n",
      "Iteration 1761, loss = 0.08112691\n",
      "Iteration 1762, loss = 0.08138476\n",
      "Iteration 1763, loss = 0.08110394\n",
      "Iteration 1764, loss = 0.08103414\n",
      "Iteration 1765, loss = 0.08106078\n",
      "Iteration 1766, loss = 0.08141830\n",
      "Iteration 1767, loss = 0.08111139\n",
      "Iteration 1768, loss = 0.08143019\n",
      "Iteration 1769, loss = 0.08152134\n",
      "Iteration 1770, loss = 0.08098710\n",
      "Iteration 1771, loss = 0.08117727\n",
      "Iteration 1772, loss = 0.08108926\n",
      "Iteration 1773, loss = 0.08152526\n",
      "Iteration 1774, loss = 0.08086216\n",
      "Iteration 1775, loss = 0.08086123\n",
      "Iteration 1776, loss = 0.08120791\n",
      "Iteration 1777, loss = 0.08128988\n",
      "Iteration 1778, loss = 0.08152492\n",
      "Iteration 1779, loss = 0.08102661\n",
      "Iteration 1780, loss = 0.08104349\n",
      "Iteration 1781, loss = 0.08094312\n",
      "Iteration 1782, loss = 0.08188087\n",
      "Iteration 1783, loss = 0.08117125\n",
      "Iteration 1784, loss = 0.08122416\n",
      "Iteration 1785, loss = 0.08119497\n",
      "Iteration 1786, loss = 0.08087128\n",
      "Iteration 1787, loss = 0.08113169\n",
      "Iteration 1788, loss = 0.08099665\n",
      "Iteration 1789, loss = 0.08094599\n",
      "Iteration 1790, loss = 0.08101280\n",
      "Iteration 1791, loss = 0.08105530\n",
      "Iteration 1792, loss = 0.08157439\n",
      "Iteration 1793, loss = 0.08133710\n",
      "Iteration 1794, loss = 0.08108782\n",
      "Iteration 1795, loss = 0.08103278\n",
      "Iteration 1796, loss = 0.08131073\n",
      "Iteration 1797, loss = 0.08127740\n",
      "Iteration 1798, loss = 0.08108090\n",
      "Iteration 1799, loss = 0.08116548\n",
      "Iteration 1800, loss = 0.08079772\n",
      "Iteration 1801, loss = 0.08084356\n",
      "Iteration 1802, loss = 0.08103545\n",
      "Iteration 1803, loss = 0.08099329\n",
      "Iteration 1804, loss = 0.08165051\n",
      "Iteration 1805, loss = 0.08133623\n",
      "Iteration 1806, loss = 0.08071712\n",
      "Iteration 1807, loss = 0.08090771\n",
      "Iteration 1808, loss = 0.08086606\n",
      "Iteration 1809, loss = 0.08088196\n",
      "Iteration 1810, loss = 0.08120280\n",
      "Iteration 1811, loss = 0.08131179\n",
      "Iteration 1812, loss = 0.08091198\n",
      "Iteration 1813, loss = 0.08098753\n",
      "Iteration 1814, loss = 0.08073916\n",
      "Iteration 1815, loss = 0.08122579\n",
      "Iteration 1816, loss = 0.08101874\n",
      "Iteration 1817, loss = 0.08134235\n",
      "Iteration 1818, loss = 0.08095777\n",
      "Iteration 1819, loss = 0.08089163\n",
      "Iteration 1820, loss = 0.08065001\n",
      "Iteration 1821, loss = 0.08079420\n",
      "Iteration 1822, loss = 0.08093304\n",
      "Iteration 1823, loss = 0.08124431\n",
      "Iteration 1824, loss = 0.08081900\n",
      "Iteration 1825, loss = 0.08096316\n",
      "Iteration 1826, loss = 0.08081282\n",
      "Iteration 1827, loss = 0.08070304\n",
      "Iteration 1828, loss = 0.08099019\n",
      "Iteration 1829, loss = 0.08112764\n",
      "Iteration 1830, loss = 0.08074720\n",
      "Iteration 1831, loss = 0.08129486\n",
      "Iteration 1832, loss = 0.08193831\n",
      "Iteration 1833, loss = 0.08081482\n",
      "Iteration 1834, loss = 0.08090544\n",
      "Iteration 1835, loss = 0.08100278\n",
      "Iteration 1836, loss = 0.08065348\n",
      "Iteration 1837, loss = 0.08114544\n",
      "Iteration 1838, loss = 0.08078679\n",
      "Iteration 1839, loss = 0.08049389\n",
      "Iteration 1840, loss = 0.08061990\n",
      "Iteration 1841, loss = 0.08097395\n",
      "Iteration 1842, loss = 0.08059939\n",
      "Iteration 1843, loss = 0.08068816\n",
      "Iteration 1844, loss = 0.08062943\n",
      "Iteration 1845, loss = 0.08092119\n",
      "Iteration 1846, loss = 0.08081499\n",
      "Iteration 1847, loss = 0.08043894\n",
      "Iteration 1848, loss = 0.08086274\n",
      "Iteration 1849, loss = 0.08069514\n",
      "Iteration 1850, loss = 0.08067084\n",
      "Iteration 1851, loss = 0.08086789\n",
      "Iteration 1852, loss = 0.08100627\n",
      "Iteration 1853, loss = 0.08104518\n",
      "Iteration 1854, loss = 0.08074672\n",
      "Iteration 1855, loss = 0.08088330\n",
      "Iteration 1856, loss = 0.08079542\n",
      "Iteration 1857, loss = 0.08051825\n",
      "Iteration 1858, loss = 0.08091210\n",
      "Iteration 1859, loss = 0.08054743\n",
      "Iteration 1860, loss = 0.08122518\n",
      "Iteration 1861, loss = 0.08058845\n",
      "Iteration 1862, loss = 0.08201311\n",
      "Iteration 1863, loss = 0.08083273\n",
      "Iteration 1864, loss = 0.08046810\n",
      "Iteration 1865, loss = 0.08027435\n",
      "Iteration 1866, loss = 0.08048292\n",
      "Iteration 1867, loss = 0.08073785\n",
      "Iteration 1868, loss = 0.08057191\n",
      "Iteration 1869, loss = 0.08072205\n",
      "Iteration 1870, loss = 0.08077838\n",
      "Iteration 1871, loss = 0.08055614\n",
      "Iteration 1872, loss = 0.08068323\n",
      "Iteration 1873, loss = 0.08064697\n",
      "Iteration 1874, loss = 0.08100664\n",
      "Iteration 1875, loss = 0.08073705\n",
      "Iteration 1876, loss = 0.08087123\n",
      "Iteration 1877, loss = 0.08061685\n",
      "Iteration 1878, loss = 0.08076171\n",
      "Iteration 1879, loss = 0.08071120\n",
      "Iteration 1880, loss = 0.08066094\n",
      "Iteration 1881, loss = 0.08060146\n",
      "Iteration 1882, loss = 0.08079936\n",
      "Iteration 1883, loss = 0.08032355\n",
      "Iteration 1884, loss = 0.08049189\n",
      "Iteration 1885, loss = 0.08045645\n",
      "Iteration 1886, loss = 0.08068291\n",
      "Iteration 1887, loss = 0.08043967\n",
      "Iteration 1888, loss = 0.08095667\n",
      "Iteration 1889, loss = 0.08072590\n",
      "Iteration 1890, loss = 0.08071785\n",
      "Iteration 1891, loss = 0.08054414\n",
      "Iteration 1892, loss = 0.08051803\n",
      "Iteration 1893, loss = 0.08034234\n",
      "Iteration 1894, loss = 0.08062454\n",
      "Iteration 1895, loss = 0.08021470\n",
      "Iteration 1896, loss = 0.08065382\n",
      "Iteration 1897, loss = 0.08114518\n",
      "Iteration 1898, loss = 0.08037073\n",
      "Iteration 1899, loss = 0.08052342\n",
      "Iteration 1900, loss = 0.08029907\n",
      "Iteration 1901, loss = 0.08017217\n",
      "Iteration 1902, loss = 0.08020137\n",
      "Iteration 1903, loss = 0.08058775\n",
      "Iteration 1904, loss = 0.08043529\n",
      "Iteration 1905, loss = 0.08042640\n",
      "Iteration 1906, loss = 0.08031361\n",
      "Iteration 1907, loss = 0.08031827\n",
      "Iteration 1908, loss = 0.08023427\n",
      "Iteration 1909, loss = 0.08021704\n",
      "Iteration 1910, loss = 0.08050155\n",
      "Iteration 1911, loss = 0.08027362\n",
      "Iteration 1912, loss = 0.08034523\n",
      "Iteration 1913, loss = 0.08034129\n",
      "Iteration 1914, loss = 0.08067463\n",
      "Iteration 1915, loss = 0.08050893\n",
      "Iteration 1916, loss = 0.08052063\n",
      "Iteration 1917, loss = 0.08028489\n",
      "Iteration 1918, loss = 0.08021973\n",
      "Iteration 1919, loss = 0.08067931\n",
      "Iteration 1920, loss = 0.08061601\n",
      "Iteration 1921, loss = 0.08063638\n",
      "Iteration 1922, loss = 0.08026259\n",
      "Iteration 1923, loss = 0.08031176\n",
      "Iteration 1924, loss = 0.08013206\n",
      "Iteration 1925, loss = 0.08027492\n",
      "Iteration 1926, loss = 0.08049620\n",
      "Iteration 1927, loss = 0.08028534\n",
      "Iteration 1928, loss = 0.08027795\n",
      "Iteration 1929, loss = 0.08025828\n",
      "Iteration 1930, loss = 0.08043396\n",
      "Iteration 1931, loss = 0.08023920\n",
      "Iteration 1932, loss = 0.08053023\n",
      "Iteration 1933, loss = 0.08073676\n",
      "Iteration 1934, loss = 0.08059122\n",
      "Iteration 1935, loss = 0.07998882\n",
      "Iteration 1936, loss = 0.08021194\n",
      "Iteration 1937, loss = 0.08020454\n",
      "Iteration 1938, loss = 0.07997703\n",
      "Iteration 1939, loss = 0.08029062\n",
      "Iteration 1940, loss = 0.08019780\n",
      "Iteration 1941, loss = 0.08022490\n",
      "Iteration 1942, loss = 0.08001464\n",
      "Iteration 1943, loss = 0.07984058\n",
      "Iteration 1944, loss = 0.08025991\n",
      "Iteration 1945, loss = 0.08016361\n",
      "Iteration 1946, loss = 0.08046076\n",
      "Iteration 1947, loss = 0.08053176\n",
      "Iteration 1948, loss = 0.08019984\n",
      "Iteration 1949, loss = 0.08016946\n",
      "Iteration 1950, loss = 0.08056777\n",
      "Iteration 1951, loss = 0.08025681\n",
      "Iteration 1952, loss = 0.07996995\n",
      "Iteration 1953, loss = 0.08014135\n",
      "Iteration 1954, loss = 0.07996319\n",
      "Iteration 1955, loss = 0.08013249\n",
      "Iteration 1956, loss = 0.08003364\n",
      "Iteration 1957, loss = 0.08067854\n",
      "Iteration 1958, loss = 0.08083551\n",
      "Iteration 1959, loss = 0.08012542\n",
      "Iteration 1960, loss = 0.07984675\n",
      "Iteration 1961, loss = 0.08010411\n",
      "Iteration 1962, loss = 0.08038861\n",
      "Iteration 1963, loss = 0.07999113\n",
      "Iteration 1964, loss = 0.08005956\n",
      "Iteration 1965, loss = 0.08027548\n",
      "Iteration 1966, loss = 0.08008835\n",
      "Iteration 1967, loss = 0.07978557\n",
      "Iteration 1968, loss = 0.08003849\n",
      "Iteration 1969, loss = 0.08005035\n",
      "Iteration 1970, loss = 0.07998070\n",
      "Iteration 1971, loss = 0.08011598\n",
      "Iteration 1972, loss = 0.08020320\n",
      "Iteration 1973, loss = 0.08008531\n",
      "Iteration 1974, loss = 0.07989134\n",
      "Iteration 1975, loss = 0.07983805\n",
      "Iteration 1976, loss = 0.08070856\n",
      "Iteration 1977, loss = 0.08040656\n",
      "Iteration 1978, loss = 0.07980436\n",
      "Iteration 1979, loss = 0.08018300\n",
      "Iteration 1980, loss = 0.07999379\n",
      "Iteration 1981, loss = 0.08010401\n",
      "Iteration 1982, loss = 0.07993448\n",
      "Iteration 1983, loss = 0.08026952\n",
      "Iteration 1984, loss = 0.08023670\n",
      "Iteration 1985, loss = 0.07989281\n",
      "Iteration 1986, loss = 0.08023728\n",
      "Iteration 1987, loss = 0.08003922\n",
      "Iteration 1988, loss = 0.07986831\n",
      "Iteration 1989, loss = 0.07996510\n",
      "Iteration 1990, loss = 0.07975346\n",
      "Iteration 1991, loss = 0.07989323\n",
      "Iteration 1992, loss = 0.07995243\n",
      "Iteration 1993, loss = 0.07985388\n",
      "Iteration 1994, loss = 0.07994007\n",
      "Iteration 1995, loss = 0.07990424\n",
      "Iteration 1996, loss = 0.07973977\n",
      "Iteration 1997, loss = 0.07966375\n",
      "Iteration 1998, loss = 0.07999563\n",
      "Iteration 1999, loss = 0.07992324\n",
      "Iteration 2000, loss = 0.07988381\n",
      "Iteration 2001, loss = 0.07988435\n",
      "Iteration 2002, loss = 0.08000100\n",
      "Iteration 2003, loss = 0.07976314\n",
      "Iteration 2004, loss = 0.07991326\n",
      "Iteration 2005, loss = 0.07983018\n",
      "Iteration 2006, loss = 0.08032613\n",
      "Iteration 2007, loss = 0.07982900\n",
      "Iteration 2008, loss = 0.07978425\n",
      "Iteration 2009, loss = 0.07960305\n",
      "Iteration 2010, loss = 0.08000503\n",
      "Iteration 2011, loss = 0.07998769\n",
      "Iteration 2012, loss = 0.07983091\n",
      "Iteration 2013, loss = 0.07974173\n",
      "Iteration 2014, loss = 0.08017018\n",
      "Iteration 2015, loss = 0.08025431\n",
      "Iteration 2016, loss = 0.08009069\n",
      "Iteration 2017, loss = 0.07989679\n",
      "Iteration 2018, loss = 0.07984333\n",
      "Iteration 2019, loss = 0.08004539\n",
      "Iteration 2020, loss = 0.07983441\n",
      "Iteration 2021, loss = 0.07994360\n",
      "Iteration 2022, loss = 0.07961271\n",
      "Iteration 2023, loss = 0.07982842\n",
      "Iteration 2024, loss = 0.08024184\n",
      "Iteration 2025, loss = 0.07988781\n",
      "Iteration 2026, loss = 0.07969771\n",
      "Iteration 2027, loss = 0.08029979\n",
      "Iteration 2028, loss = 0.07961806\n",
      "Iteration 2029, loss = 0.07965222\n",
      "Iteration 2030, loss = 0.07972050\n",
      "Iteration 2031, loss = 0.07955318\n",
      "Iteration 2032, loss = 0.07995772\n",
      "Iteration 2033, loss = 0.08021008\n",
      "Iteration 2034, loss = 0.07948655\n",
      "Iteration 2035, loss = 0.07972402\n",
      "Iteration 2036, loss = 0.07976336\n",
      "Iteration 2037, loss = 0.07981220\n",
      "Iteration 2038, loss = 0.07986159\n",
      "Iteration 2039, loss = 0.07986680\n",
      "Iteration 2040, loss = 0.08001581\n",
      "Iteration 2041, loss = 0.08025632\n",
      "Iteration 2042, loss = 0.07963552\n",
      "Iteration 2043, loss = 0.07995768\n",
      "Iteration 2044, loss = 0.07994460\n",
      "Iteration 2045, loss = 0.08036313\n",
      "Iteration 2046, loss = 0.07932442\n",
      "Iteration 2047, loss = 0.07968028\n",
      "Iteration 2048, loss = 0.07964812\n",
      "Iteration 2049, loss = 0.07974631\n",
      "Iteration 2050, loss = 0.07991147\n",
      "Iteration 2051, loss = 0.07973328\n",
      "Iteration 2052, loss = 0.07960292\n",
      "Iteration 2053, loss = 0.07965599\n",
      "Iteration 2054, loss = 0.07973387\n",
      "Iteration 2055, loss = 0.07991207\n",
      "Iteration 2056, loss = 0.07961747\n",
      "Iteration 2057, loss = 0.07971490\n",
      "Iteration 2058, loss = 0.07993846\n",
      "Iteration 2059, loss = 0.07998285\n",
      "Iteration 2060, loss = 0.07964853\n",
      "Iteration 2061, loss = 0.07964495\n",
      "Iteration 2062, loss = 0.08005169\n",
      "Iteration 2063, loss = 0.07990775\n",
      "Iteration 2064, loss = 0.07955567\n",
      "Iteration 2065, loss = 0.07991194\n",
      "Iteration 2066, loss = 0.08019282\n",
      "Iteration 2067, loss = 0.07971605\n",
      "Iteration 2068, loss = 0.07965626\n",
      "Iteration 2069, loss = 0.07962481\n",
      "Iteration 2070, loss = 0.07964936\n",
      "Iteration 2071, loss = 0.07958675\n",
      "Iteration 2072, loss = 0.07986420\n",
      "Iteration 2073, loss = 0.07971731\n",
      "Iteration 2074, loss = 0.07958495\n",
      "Iteration 2075, loss = 0.07969129\n",
      "Iteration 2076, loss = 0.07967088\n",
      "Iteration 2077, loss = 0.07961996\n",
      "Iteration 2078, loss = 0.07978001\n",
      "Iteration 2079, loss = 0.08076993\n",
      "Iteration 2080, loss = 0.07943323\n",
      "Iteration 2081, loss = 0.07963937\n",
      "Iteration 2082, loss = 0.07966050\n",
      "Iteration 2083, loss = 0.07938306\n",
      "Iteration 2084, loss = 0.07970351\n",
      "Iteration 2085, loss = 0.07929446\n",
      "Iteration 2086, loss = 0.07949567\n",
      "Iteration 2087, loss = 0.07970214\n",
      "Iteration 2088, loss = 0.07955018\n",
      "Iteration 2089, loss = 0.07983144\n",
      "Iteration 2090, loss = 0.07953803\n",
      "Iteration 2091, loss = 0.07951454\n",
      "Iteration 2092, loss = 0.07971877\n",
      "Iteration 2093, loss = 0.07959807\n",
      "Iteration 2094, loss = 0.07947387\n",
      "Iteration 2095, loss = 0.07972967\n",
      "Iteration 2096, loss = 0.07950707\n",
      "Iteration 2097, loss = 0.07936202\n",
      "Iteration 2098, loss = 0.07958381\n",
      "Iteration 2099, loss = 0.07944969\n",
      "Iteration 2100, loss = 0.07953443\n",
      "Iteration 2101, loss = 0.07984979\n",
      "Iteration 2102, loss = 0.07960753\n",
      "Iteration 2103, loss = 0.07930271\n",
      "Iteration 2104, loss = 0.07986545\n",
      "Iteration 2105, loss = 0.07923545\n",
      "Iteration 2106, loss = 0.07968306\n",
      "Iteration 2107, loss = 0.07944137\n",
      "Iteration 2108, loss = 0.07996052\n",
      "Iteration 2109, loss = 0.07979206\n",
      "Iteration 2110, loss = 0.07943115\n",
      "Iteration 2111, loss = 0.07913760\n",
      "Iteration 2112, loss = 0.07987627\n",
      "Iteration 2113, loss = 0.08016754\n",
      "Iteration 2114, loss = 0.07937047\n",
      "Iteration 2115, loss = 0.07935513\n",
      "Iteration 2116, loss = 0.07956530\n",
      "Iteration 2117, loss = 0.07928850\n",
      "Iteration 2118, loss = 0.07924550\n",
      "Iteration 2119, loss = 0.07944640\n",
      "Iteration 2120, loss = 0.07936052\n",
      "Iteration 2121, loss = 0.07940945\n",
      "Iteration 2122, loss = 0.07983987\n",
      "Iteration 2123, loss = 0.07921518\n",
      "Iteration 2124, loss = 0.07935984\n",
      "Iteration 2125, loss = 0.07921845\n",
      "Iteration 2126, loss = 0.07923856\n",
      "Iteration 2127, loss = 0.07966918\n",
      "Iteration 2128, loss = 0.07920275\n",
      "Iteration 2129, loss = 0.07926949\n",
      "Iteration 2130, loss = 0.07928915\n",
      "Iteration 2131, loss = 0.07947856\n",
      "Iteration 2132, loss = 0.07935247\n",
      "Iteration 2133, loss = 0.07927468\n",
      "Iteration 2134, loss = 0.07934148\n",
      "Iteration 2135, loss = 0.07941696\n",
      "Iteration 2136, loss = 0.07945845\n",
      "Iteration 2137, loss = 0.07934836\n",
      "Iteration 2138, loss = 0.07973080\n",
      "Iteration 2139, loss = 0.07942329\n",
      "Iteration 2140, loss = 0.07950461\n",
      "Iteration 2141, loss = 0.07942658\n",
      "Iteration 2142, loss = 0.08445158\n",
      "Iteration 2143, loss = 0.08002424\n",
      "Iteration 2144, loss = 0.07892730\n",
      "Iteration 2145, loss = 0.07882199\n",
      "Iteration 2146, loss = 0.07882003\n",
      "Iteration 2147, loss = 0.07886203\n",
      "Iteration 2148, loss = 0.07882617\n",
      "Iteration 2149, loss = 0.07917099\n",
      "Iteration 2150, loss = 0.07917136\n",
      "Iteration 2151, loss = 0.07965226\n",
      "Iteration 2152, loss = 0.07957592\n",
      "Iteration 2153, loss = 0.07943198\n",
      "Iteration 2154, loss = 0.07917045\n",
      "Iteration 2155, loss = 0.07937771\n",
      "Iteration 2156, loss = 0.07924787\n",
      "Iteration 2157, loss = 0.07921339\n",
      "Iteration 2158, loss = 0.07924315\n",
      "Iteration 2159, loss = 0.07959750\n",
      "Iteration 2160, loss = 0.07926773\n",
      "Iteration 2161, loss = 0.07926440\n",
      "Iteration 2162, loss = 0.07907380\n",
      "Iteration 2163, loss = 0.07903882\n",
      "Iteration 2164, loss = 0.07941902\n",
      "Iteration 2165, loss = 0.07924636\n",
      "Iteration 2166, loss = 0.07923719\n",
      "Iteration 2167, loss = 0.07928609\n",
      "Iteration 2168, loss = 0.07937973\n",
      "Iteration 2169, loss = 0.07936399\n",
      "Iteration 2170, loss = 0.07916878\n",
      "Iteration 2171, loss = 0.07949631\n",
      "Iteration 2172, loss = 0.07910873\n",
      "Iteration 2173, loss = 0.07947157\n",
      "Iteration 2174, loss = 0.07923380\n",
      "Iteration 2175, loss = 0.07944329\n",
      "Iteration 2176, loss = 0.07914556\n",
      "Iteration 2177, loss = 0.07900532\n",
      "Iteration 2178, loss = 0.07916426\n",
      "Iteration 2179, loss = 0.07912300\n",
      "Iteration 2180, loss = 0.07951731\n",
      "Iteration 2181, loss = 0.07937554\n",
      "Iteration 2182, loss = 0.07929204\n",
      "Iteration 2183, loss = 0.07931943\n",
      "Iteration 2184, loss = 0.07923063\n",
      "Iteration 2185, loss = 0.07905382\n",
      "Iteration 2186, loss = 0.07899488\n",
      "Iteration 2187, loss = 0.07948594\n",
      "Iteration 2188, loss = 0.07902075\n",
      "Iteration 2189, loss = 0.07924976\n",
      "Iteration 2190, loss = 0.07915843\n",
      "Iteration 2191, loss = 0.07942217\n",
      "Iteration 2192, loss = 0.07905776\n",
      "Iteration 2193, loss = 0.07927880\n",
      "Iteration 2194, loss = 0.07912868\n",
      "Iteration 2195, loss = 0.07923411\n",
      "Iteration 2196, loss = 0.07922796\n",
      "Iteration 2197, loss = 0.07926595\n",
      "Iteration 2198, loss = 0.07929371\n",
      "Iteration 2199, loss = 0.07934829\n",
      "Iteration 2200, loss = 0.07900478\n",
      "Iteration 2201, loss = 0.07888247\n",
      "Iteration 2202, loss = 0.07894631\n",
      "Iteration 2203, loss = 0.07943021\n",
      "Iteration 2204, loss = 0.07997107\n",
      "Iteration 2205, loss = 0.07906118\n",
      "Iteration 2206, loss = 0.07897836\n",
      "Iteration 2207, loss = 0.07891760\n",
      "Iteration 2208, loss = 0.07949585\n",
      "Iteration 2209, loss = 0.07896416\n",
      "Iteration 2210, loss = 0.07898783\n",
      "Iteration 2211, loss = 0.07941957\n",
      "Iteration 2212, loss = 0.07893930\n",
      "Iteration 2213, loss = 0.07896627\n",
      "Iteration 2214, loss = 0.07916139\n",
      "Iteration 2215, loss = 0.07931293\n",
      "Iteration 2216, loss = 0.07907935\n",
      "Iteration 2217, loss = 0.07894029\n",
      "Iteration 2218, loss = 0.07933103\n",
      "Iteration 2219, loss = 0.08058996\n",
      "Iteration 2220, loss = 0.07878139\n",
      "Iteration 2221, loss = 0.07868821\n",
      "Iteration 2222, loss = 0.07863842\n",
      "Iteration 2223, loss = 0.07887266\n",
      "Iteration 2224, loss = 0.07888481\n",
      "Iteration 2225, loss = 0.07928388\n",
      "Iteration 2226, loss = 0.07947493\n",
      "Iteration 2227, loss = 0.07885792\n",
      "Iteration 2228, loss = 0.07905571\n",
      "Iteration 2229, loss = 0.07912841\n",
      "Iteration 2230, loss = 0.07931214\n",
      "Iteration 2231, loss = 0.07902078\n",
      "Iteration 2232, loss = 0.07886020\n",
      "Iteration 2233, loss = 0.07893577\n",
      "Iteration 2234, loss = 0.07884515\n",
      "Iteration 2235, loss = 0.07893669\n",
      "Iteration 2236, loss = 0.07908285\n",
      "Iteration 2237, loss = 0.07919672\n",
      "Iteration 2238, loss = 0.07887047\n",
      "Iteration 2239, loss = 0.07895139\n",
      "Iteration 2240, loss = 0.07934100\n",
      "Iteration 2241, loss = 0.07907092\n",
      "Iteration 2242, loss = 0.07890201\n",
      "Iteration 2243, loss = 0.07901474\n",
      "Iteration 2244, loss = 0.07893608\n",
      "Iteration 2245, loss = 0.07867856\n",
      "Iteration 2246, loss = 0.08054150\n",
      "Iteration 2247, loss = 0.07893894\n",
      "Iteration 2248, loss = 0.07878509\n",
      "Iteration 2249, loss = 0.07896997\n",
      "Iteration 2250, loss = 0.07907994\n",
      "Iteration 2251, loss = 0.07893186\n",
      "Iteration 2252, loss = 0.07887174\n",
      "Iteration 2253, loss = 0.07886029\n",
      "Iteration 2254, loss = 0.07881051\n",
      "Iteration 2255, loss = 0.07875942\n",
      "Iteration 2256, loss = 0.07908163\n",
      "Iteration 2257, loss = 0.07892695\n",
      "Iteration 2258, loss = 0.07856399\n",
      "Iteration 2259, loss = 0.07915960\n",
      "Iteration 2260, loss = 0.07905579\n",
      "Iteration 2261, loss = 0.07878496\n",
      "Iteration 2262, loss = 0.07887670\n",
      "Iteration 2263, loss = 0.07929847\n",
      "Iteration 2264, loss = 0.07886754\n",
      "Iteration 2265, loss = 0.07875894\n",
      "Iteration 2266, loss = 0.07849949\n",
      "Iteration 2267, loss = 0.07890921\n",
      "Iteration 2268, loss = 0.07903094\n",
      "Iteration 2269, loss = 0.07874065\n",
      "Iteration 2270, loss = 0.07875756\n",
      "Iteration 2271, loss = 0.07889116\n",
      "Iteration 2272, loss = 0.07912854\n",
      "Iteration 2273, loss = 0.07880370\n",
      "Iteration 2274, loss = 0.07891037\n",
      "Iteration 2275, loss = 0.07873268\n",
      "Iteration 2276, loss = 0.07901355\n",
      "Iteration 2277, loss = 0.07891888\n",
      "Iteration 2278, loss = 0.07897377\n",
      "Iteration 2279, loss = 0.07898614\n",
      "Iteration 2280, loss = 0.07880122\n",
      "Iteration 2281, loss = 0.07875184\n",
      "Iteration 2282, loss = 0.07902342\n",
      "Iteration 2283, loss = 0.07903071\n",
      "Iteration 2284, loss = 0.07871427\n",
      "Iteration 2285, loss = 0.07879978\n",
      "Iteration 2286, loss = 0.07893822\n",
      "Iteration 2287, loss = 0.07895109\n",
      "Iteration 2288, loss = 0.07925412\n",
      "Iteration 2289, loss = 0.07884227\n",
      "Iteration 2290, loss = 0.07925838\n",
      "Iteration 2291, loss = 0.07903784\n",
      "Iteration 2292, loss = 0.07884351\n",
      "Iteration 2293, loss = 0.07851316\n",
      "Iteration 2294, loss = 0.07879203\n",
      "Iteration 2295, loss = 0.07859620\n",
      "Iteration 2296, loss = 0.07926332\n",
      "Iteration 2297, loss = 0.07850808\n",
      "Iteration 2298, loss = 0.07858650\n",
      "Iteration 2299, loss = 0.07860423\n",
      "Iteration 2300, loss = 0.07932615\n",
      "Iteration 2301, loss = 0.07867583\n",
      "Iteration 2302, loss = 0.07932346\n",
      "Iteration 2303, loss = 0.07871516\n",
      "Iteration 2304, loss = 0.07847888\n",
      "Iteration 2305, loss = 0.07913331\n",
      "Iteration 2306, loss = 0.07881833\n",
      "Iteration 2307, loss = 0.07864589\n",
      "Iteration 2308, loss = 0.07874744\n",
      "Iteration 2309, loss = 0.07857794\n",
      "Iteration 2310, loss = 0.07902730\n",
      "Iteration 2311, loss = 0.07870011\n",
      "Iteration 2312, loss = 0.07868165\n",
      "Iteration 2313, loss = 0.07908225\n",
      "Iteration 2314, loss = 0.07867003\n",
      "Iteration 2315, loss = 0.07863532\n",
      "Iteration 2316, loss = 0.07886171\n",
      "Iteration 2317, loss = 0.07830980\n",
      "Iteration 2318, loss = 0.07936054\n",
      "Iteration 2319, loss = 0.07933667\n",
      "Iteration 2320, loss = 0.07857590\n",
      "Iteration 2321, loss = 0.07869089\n",
      "Iteration 2322, loss = 0.07879058\n",
      "Iteration 2323, loss = 0.07878197\n",
      "Iteration 2324, loss = 0.07874409\n",
      "Iteration 2325, loss = 0.07856301\n",
      "Iteration 2326, loss = 0.07854846\n",
      "Iteration 2327, loss = 0.07844350\n",
      "Iteration 2328, loss = 0.07881541\n",
      "Iteration 2329, loss = 0.07877875\n",
      "Iteration 2330, loss = 0.07898446\n",
      "Iteration 2331, loss = 0.07898493\n",
      "Iteration 2332, loss = 0.07852324\n",
      "Iteration 2333, loss = 0.07873695\n",
      "Iteration 2334, loss = 0.07878781\n",
      "Iteration 2335, loss = 0.07853682\n",
      "Iteration 2336, loss = 0.07895426\n",
      "Iteration 2337, loss = 0.07863005\n",
      "Iteration 2338, loss = 0.07891991\n",
      "Iteration 2339, loss = 0.07881553\n",
      "Iteration 2340, loss = 0.07907161\n",
      "Iteration 2341, loss = 0.07874463\n",
      "Iteration 2342, loss = 0.07836695\n",
      "Iteration 2343, loss = 0.07900797\n",
      "Iteration 2344, loss = 0.07895497\n",
      "Iteration 2345, loss = 0.07911166\n",
      "Iteration 2346, loss = 0.07854308\n",
      "Iteration 2347, loss = 0.07869135\n",
      "Iteration 2348, loss = 0.07876139\n",
      "Iteration 2349, loss = 0.07856153\n",
      "Iteration 2350, loss = 0.07855598\n",
      "Iteration 2351, loss = 0.07849285\n",
      "Iteration 2352, loss = 0.07901778\n",
      "Iteration 2353, loss = 0.07858263\n",
      "Iteration 2354, loss = 0.07888164\n",
      "Iteration 2355, loss = 0.07828706\n",
      "Iteration 2356, loss = 0.07913803\n",
      "Iteration 2357, loss = 0.07850248\n",
      "Iteration 2358, loss = 0.07866229\n",
      "Iteration 2359, loss = 0.07872431\n",
      "Iteration 2360, loss = 0.07886530\n",
      "Iteration 2361, loss = 0.07839978\n",
      "Iteration 2362, loss = 0.07873198\n",
      "Iteration 2363, loss = 0.07857158\n",
      "Iteration 2364, loss = 0.07837518\n",
      "Iteration 2365, loss = 0.07871328\n",
      "Iteration 2366, loss = 0.07843272\n",
      "Iteration 2367, loss = 0.07840287\n",
      "Iteration 2368, loss = 0.07877783\n",
      "Iteration 2369, loss = 0.07909626\n",
      "Iteration 2370, loss = 0.07913324\n",
      "Iteration 2371, loss = 0.07859416\n",
      "Iteration 2372, loss = 0.07852796\n",
      "Iteration 2373, loss = 0.07838226\n",
      "Iteration 2374, loss = 0.07830633\n",
      "Iteration 2375, loss = 0.07843030\n",
      "Iteration 2376, loss = 0.07837335\n",
      "Iteration 2377, loss = 0.07847453\n",
      "Iteration 2378, loss = 0.07838745\n",
      "Iteration 2379, loss = 0.07890507\n",
      "Iteration 2380, loss = 0.07894123\n",
      "Iteration 2381, loss = 0.07920673\n",
      "Iteration 2382, loss = 0.07904561\n",
      "Iteration 2383, loss = 0.07828800\n",
      "Iteration 2384, loss = 0.07860733\n",
      "Iteration 2385, loss = 0.07837697\n",
      "Iteration 2386, loss = 0.07855740\n",
      "Iteration 2387, loss = 0.07871807\n",
      "Iteration 2388, loss = 0.07871674\n",
      "Iteration 2389, loss = 0.07827465\n",
      "Iteration 2390, loss = 0.07829693\n",
      "Iteration 2391, loss = 0.07871363\n",
      "Iteration 2392, loss = 0.07834810\n",
      "Iteration 2393, loss = 0.07871597\n",
      "Iteration 2394, loss = 0.07832459\n",
      "Iteration 2395, loss = 0.07965290\n",
      "Iteration 2396, loss = 0.07846497\n",
      "Iteration 2397, loss = 0.07826767\n",
      "Iteration 2398, loss = 0.07812940\n",
      "Iteration 2399, loss = 0.07881593\n",
      "Iteration 2400, loss = 0.07863275\n",
      "Iteration 2401, loss = 0.07850541\n",
      "Iteration 2402, loss = 0.07830192\n",
      "Iteration 2403, loss = 0.07843431\n",
      "Iteration 2404, loss = 0.07882370\n",
      "Iteration 2405, loss = 0.07842190\n",
      "Iteration 2406, loss = 0.07878985\n",
      "Iteration 2407, loss = 0.07835477\n",
      "Iteration 2408, loss = 0.07861199\n",
      "Iteration 2409, loss = 0.07833540\n",
      "Iteration 2410, loss = 0.07823245\n",
      "Iteration 2411, loss = 0.07850661\n",
      "Iteration 2412, loss = 0.07817632\n",
      "Iteration 2413, loss = 0.07837583\n",
      "Iteration 2414, loss = 0.07860570\n",
      "Iteration 2415, loss = 0.07890603\n",
      "Iteration 2416, loss = 0.07847586\n",
      "Iteration 2417, loss = 0.07830696\n",
      "Iteration 2418, loss = 0.07828193\n",
      "Iteration 2419, loss = 0.07833074\n",
      "Iteration 2420, loss = 0.07825515\n",
      "Iteration 2421, loss = 0.07846355\n",
      "Iteration 2422, loss = 0.07822721\n",
      "Iteration 2423, loss = 0.07828556\n",
      "Iteration 2424, loss = 0.07877607\n",
      "Iteration 2425, loss = 0.07856330\n",
      "Iteration 2426, loss = 0.07838709\n",
      "Iteration 2427, loss = 0.07820828\n",
      "Iteration 2428, loss = 0.07846418\n",
      "Iteration 2429, loss = 0.07854580\n",
      "Iteration 2430, loss = 0.07891728\n",
      "Iteration 2431, loss = 0.07956579\n",
      "Iteration 2432, loss = 0.07810669\n",
      "Iteration 2433, loss = 0.07815618\n",
      "Iteration 2434, loss = 0.07819732\n",
      "Iteration 2435, loss = 0.07822284\n",
      "Iteration 2436, loss = 0.07842737\n",
      "Iteration 2437, loss = 0.07841585\n",
      "Iteration 2438, loss = 0.07828785\n",
      "Iteration 2439, loss = 0.07822934\n",
      "Iteration 2440, loss = 0.07837944\n",
      "Iteration 2441, loss = 0.07838446\n",
      "Iteration 2442, loss = 0.07842996\n",
      "Iteration 2443, loss = 0.07872410\n",
      "Iteration 2444, loss = 0.07827558\n",
      "Iteration 2445, loss = 0.07841950\n",
      "Iteration 2446, loss = 0.07810355\n",
      "Iteration 2447, loss = 0.07869132\n",
      "Iteration 2448, loss = 0.07839462\n",
      "Iteration 2449, loss = 0.07829507\n",
      "Iteration 2450, loss = 0.07811927\n",
      "Iteration 2451, loss = 0.07823301\n",
      "Iteration 2452, loss = 0.07858099\n",
      "Iteration 2453, loss = 0.07816699\n",
      "Iteration 2454, loss = 0.07818887\n",
      "Iteration 2455, loss = 0.07912159\n",
      "Iteration 2456, loss = 0.07824305\n",
      "Iteration 2457, loss = 0.07827121\n",
      "Iteration 2458, loss = 0.07842400\n",
      "Iteration 2459, loss = 0.07822950\n",
      "Iteration 2460, loss = 0.07843482\n",
      "Iteration 2461, loss = 0.07803155\n",
      "Iteration 2462, loss = 0.07816366\n",
      "Iteration 2463, loss = 0.07809230\n",
      "Iteration 2464, loss = 0.07835388\n",
      "Iteration 2465, loss = 0.07825538\n",
      "Iteration 2466, loss = 0.07846518\n",
      "Iteration 2467, loss = 0.07813535\n",
      "Iteration 2468, loss = 0.07852344\n",
      "Iteration 2469, loss = 0.07821704\n",
      "Iteration 2470, loss = 0.07818404\n",
      "Iteration 2471, loss = 0.07834128\n",
      "Iteration 2472, loss = 0.07826037\n",
      "Iteration 2473, loss = 0.07835933\n",
      "Iteration 2474, loss = 0.07847520\n",
      "Iteration 2475, loss = 0.07830615\n",
      "Iteration 2476, loss = 0.07804259\n",
      "Iteration 2477, loss = 0.07806372\n",
      "Iteration 2478, loss = 0.07811974\n",
      "Iteration 2479, loss = 0.07814400\n",
      "Iteration 2480, loss = 0.07826875\n",
      "Iteration 2481, loss = 0.07820334\n",
      "Iteration 2482, loss = 0.07867705\n",
      "Iteration 2483, loss = 0.07883766\n",
      "Iteration 2484, loss = 0.07937366\n",
      "Iteration 2485, loss = 0.07830265\n",
      "Iteration 2486, loss = 0.07826042\n",
      "Iteration 2487, loss = 0.07822106\n",
      "Iteration 2488, loss = 0.07808114\n",
      "Iteration 2489, loss = 0.07809362\n",
      "Iteration 2490, loss = 0.07800747\n",
      "Iteration 2491, loss = 0.07850273\n",
      "Iteration 2492, loss = 0.07818928\n",
      "Iteration 2493, loss = 0.07835780\n",
      "Iteration 2494, loss = 0.07815145\n",
      "Iteration 2495, loss = 0.07801216\n",
      "Iteration 2496, loss = 0.07838179\n",
      "Iteration 2497, loss = 0.07805094\n",
      "Iteration 2498, loss = 0.07816856\n",
      "Iteration 2499, loss = 0.07831139\n",
      "Iteration 2500, loss = 0.07832030\n",
      "Iteration 2501, loss = 0.07826233\n",
      "Iteration 2502, loss = 0.07819176\n",
      "Iteration 2503, loss = 0.07836440\n",
      "Iteration 2504, loss = 0.07827145\n",
      "Iteration 2505, loss = 0.07794013\n",
      "Iteration 2506, loss = 0.07850324\n",
      "Iteration 2507, loss = 0.07796384\n",
      "Iteration 2508, loss = 0.07826207\n",
      "Iteration 2509, loss = 0.07799728\n",
      "Iteration 2510, loss = 0.07804174\n",
      "Iteration 2511, loss = 0.07809980\n",
      "Iteration 2512, loss = 0.07839001\n",
      "Iteration 2513, loss = 0.07838559\n",
      "Iteration 2514, loss = 0.07866876\n",
      "Iteration 2515, loss = 0.07819221\n",
      "Iteration 2516, loss = 0.07809804\n",
      "Iteration 2517, loss = 0.07790704\n",
      "Iteration 2518, loss = 0.07819389\n",
      "Iteration 2519, loss = 0.07784331\n",
      "Iteration 2520, loss = 0.07885309\n",
      "Iteration 2521, loss = 0.07859273\n",
      "Iteration 2522, loss = 0.07806889\n",
      "Iteration 2523, loss = 0.07794392\n",
      "Iteration 2524, loss = 0.07800555\n",
      "Iteration 2525, loss = 0.07802126\n",
      "Iteration 2526, loss = 0.07796375\n",
      "Iteration 2527, loss = 0.07845201\n",
      "Iteration 2528, loss = 0.07836101\n",
      "Iteration 2529, loss = 0.07809891\n",
      "Iteration 2530, loss = 0.07810331\n",
      "Iteration 2531, loss = 0.07833044\n",
      "Iteration 2532, loss = 0.07799466\n",
      "Iteration 2533, loss = 0.07864604\n",
      "Iteration 2534, loss = 0.07814056\n",
      "Iteration 2535, loss = 0.07806245\n",
      "Iteration 2536, loss = 0.07831319\n",
      "Iteration 2537, loss = 0.07839676\n",
      "Iteration 2538, loss = 0.07792971\n",
      "Iteration 2539, loss = 0.07785783\n",
      "Iteration 2540, loss = 0.07857818\n",
      "Iteration 2541, loss = 0.07784339\n",
      "Iteration 2542, loss = 0.07830508\n",
      "Iteration 2543, loss = 0.07831220\n",
      "Iteration 2544, loss = 0.07799622\n",
      "Iteration 2545, loss = 0.07798277\n",
      "Iteration 2546, loss = 0.07805800\n",
      "Iteration 2547, loss = 0.07821299\n",
      "Iteration 2548, loss = 0.07805766\n",
      "Iteration 2549, loss = 0.07817517\n",
      "Iteration 2550, loss = 0.07806710\n",
      "Iteration 2551, loss = 0.07844411\n",
      "Iteration 2552, loss = 0.07781847\n",
      "Iteration 2553, loss = 0.07870836\n",
      "Iteration 2554, loss = 0.07809158\n",
      "Iteration 2555, loss = 0.07784341\n",
      "Iteration 2556, loss = 0.07802540\n",
      "Iteration 2557, loss = 0.07826167\n",
      "Iteration 2558, loss = 0.07792296\n",
      "Iteration 2559, loss = 0.07792790\n",
      "Iteration 2560, loss = 0.07784605\n",
      "Iteration 2561, loss = 0.07805313\n",
      "Iteration 2562, loss = 0.07812691\n",
      "Iteration 2563, loss = 0.07785994\n",
      "Iteration 2564, loss = 0.07858817\n",
      "Iteration 2565, loss = 0.07804829\n",
      "Iteration 2566, loss = 0.07814012\n",
      "Iteration 2567, loss = 0.07808680\n",
      "Iteration 2568, loss = 0.07808439\n",
      "Iteration 2569, loss = 0.07787039\n",
      "Iteration 2570, loss = 0.07794768\n",
      "Iteration 2571, loss = 0.07792452\n",
      "Iteration 2572, loss = 0.07803538\n",
      "Iteration 2573, loss = 0.07824650\n",
      "Iteration 2574, loss = 0.07832023\n",
      "Iteration 2575, loss = 0.07801542\n",
      "Iteration 2576, loss = 0.07807651\n",
      "Iteration 2577, loss = 0.07811522\n",
      "Iteration 2578, loss = 0.07802456\n",
      "Iteration 2579, loss = 0.07781494\n",
      "Iteration 2580, loss = 0.07835207\n",
      "Iteration 2581, loss = 0.07804231\n",
      "Iteration 2582, loss = 0.07781567\n",
      "Iteration 2583, loss = 0.07782186\n",
      "Iteration 2584, loss = 0.07783686\n",
      "Iteration 2585, loss = 0.07809138\n",
      "Iteration 2586, loss = 0.07808188\n",
      "Iteration 2587, loss = 0.07806948\n",
      "Iteration 2588, loss = 0.07824329\n",
      "Iteration 2589, loss = 0.07815504\n",
      "Iteration 2590, loss = 0.07801446\n",
      "Iteration 2591, loss = 0.07796589\n",
      "Iteration 2592, loss = 0.07793935\n",
      "Iteration 2593, loss = 0.07766125\n",
      "Iteration 2594, loss = 0.07787533\n",
      "Iteration 2595, loss = 0.07789398\n",
      "Iteration 2596, loss = 0.07790402\n",
      "Iteration 2597, loss = 0.07789891\n",
      "Iteration 2598, loss = 0.07818585\n",
      "Iteration 2599, loss = 0.07803629\n",
      "Iteration 2600, loss = 0.07793969\n",
      "Iteration 2601, loss = 0.07785945\n",
      "Iteration 2602, loss = 0.07770946\n",
      "Iteration 2603, loss = 0.07806877\n",
      "Iteration 2604, loss = 0.07801185\n",
      "Iteration 2605, loss = 0.07777645\n",
      "Iteration 2606, loss = 0.07789225\n",
      "Iteration 2607, loss = 0.07780865\n",
      "Iteration 2608, loss = 0.07811584\n",
      "Iteration 2609, loss = 0.07797537\n",
      "Iteration 2610, loss = 0.07770550\n",
      "Iteration 2611, loss = 0.07801979\n",
      "Iteration 2612, loss = 0.07815700\n",
      "Iteration 2613, loss = 0.07811230\n",
      "Iteration 2614, loss = 0.07795904\n",
      "Iteration 2615, loss = 0.07789460\n",
      "Iteration 2616, loss = 0.07765446\n",
      "Iteration 2617, loss = 0.07796208\n",
      "Iteration 2618, loss = 0.07803464\n",
      "Iteration 2619, loss = 0.07784585\n",
      "Iteration 2620, loss = 0.07851965\n",
      "Iteration 2621, loss = 0.07948783\n",
      "Iteration 2622, loss = 0.07796535\n",
      "Iteration 2623, loss = 0.07771220\n",
      "Iteration 2624, loss = 0.07769087\n",
      "Iteration 2625, loss = 0.07766198\n",
      "Iteration 2626, loss = 0.07769703\n",
      "Iteration 2627, loss = 0.07773011\n",
      "Iteration 2628, loss = 0.07770610\n",
      "Iteration 2629, loss = 0.07771155\n",
      "Iteration 2630, loss = 0.07779664\n",
      "Iteration 2631, loss = 0.07832452\n",
      "Iteration 2632, loss = 0.07795904\n",
      "Iteration 2633, loss = 0.07764951\n",
      "Iteration 2634, loss = 0.07775865\n",
      "Iteration 2635, loss = 0.07780030\n",
      "Iteration 2636, loss = 0.07772563\n",
      "Iteration 2637, loss = 0.07801405\n",
      "Iteration 2638, loss = 0.07789190\n",
      "Iteration 2639, loss = 0.07808212\n",
      "Iteration 2640, loss = 0.07790654\n",
      "Iteration 2641, loss = 0.07823961\n",
      "Iteration 2642, loss = 0.07784180\n",
      "Iteration 2643, loss = 0.07770696\n",
      "Iteration 2644, loss = 0.07799180\n",
      "Iteration 2645, loss = 0.07773895\n",
      "Iteration 2646, loss = 0.07774529\n",
      "Iteration 2647, loss = 0.07770723\n",
      "Iteration 2648, loss = 0.07786869\n",
      "Iteration 2649, loss = 0.07793760\n",
      "Iteration 2650, loss = 0.07781329\n",
      "Iteration 2651, loss = 0.07778981\n",
      "Iteration 2652, loss = 0.07769731\n",
      "Iteration 2653, loss = 0.07807883\n",
      "Iteration 2654, loss = 0.07792176\n",
      "Iteration 2655, loss = 0.07760201\n",
      "Iteration 2656, loss = 0.07776983\n",
      "Iteration 2657, loss = 0.07782312\n",
      "Iteration 2658, loss = 0.07790605\n",
      "Iteration 2659, loss = 0.07765965\n",
      "Iteration 2660, loss = 0.07779920\n",
      "Iteration 2661, loss = 0.07796158\n",
      "Iteration 2662, loss = 0.07796202\n",
      "Iteration 2663, loss = 0.07771345\n",
      "Iteration 2664, loss = 0.07783647\n",
      "Iteration 2665, loss = 0.07795370\n",
      "Iteration 2666, loss = 0.07800448\n",
      "Iteration 2667, loss = 0.07755021\n",
      "Iteration 2668, loss = 0.07779489\n",
      "Iteration 2669, loss = 0.07780775\n",
      "Iteration 2670, loss = 0.07779346\n",
      "Iteration 2671, loss = 0.07793213\n",
      "Iteration 2672, loss = 0.07810581\n",
      "Iteration 2673, loss = 0.07757846\n",
      "Iteration 2674, loss = 0.07806850\n",
      "Iteration 2675, loss = 0.07795100\n",
      "Iteration 2676, loss = 0.07752496\n",
      "Iteration 2677, loss = 0.07765735\n",
      "Iteration 2678, loss = 0.07755612\n",
      "Iteration 2679, loss = 0.07753993\n",
      "Iteration 2680, loss = 0.07754330\n",
      "Iteration 2681, loss = 0.07755716\n",
      "Iteration 2682, loss = 0.07750819\n",
      "Iteration 2683, loss = 0.07792434\n",
      "Iteration 2684, loss = 0.07770666\n",
      "Iteration 2685, loss = 0.07796876\n",
      "Iteration 2686, loss = 0.07833809\n",
      "Iteration 2687, loss = 0.07763428\n",
      "Iteration 2688, loss = 0.07753233\n",
      "Iteration 2689, loss = 0.07754637\n",
      "Iteration 2690, loss = 0.07814124\n",
      "Iteration 2691, loss = 0.07760898\n",
      "Iteration 2692, loss = 0.07764207\n",
      "Iteration 2693, loss = 0.07785109\n",
      "Iteration 2694, loss = 0.07748992\n",
      "Iteration 2695, loss = 0.07758518\n",
      "Iteration 2696, loss = 0.07777429\n",
      "Iteration 2697, loss = 0.07765388\n",
      "Iteration 2698, loss = 0.07773877\n",
      "Iteration 2699, loss = 0.07762812\n",
      "Iteration 2700, loss = 0.07795830\n",
      "Iteration 2701, loss = 0.07766227\n",
      "Iteration 2702, loss = 0.07784739\n",
      "Iteration 2703, loss = 0.07771472\n",
      "Iteration 2704, loss = 0.07765581\n",
      "Iteration 2705, loss = 0.07753600\n",
      "Iteration 2706, loss = 0.07765951\n",
      "Iteration 2707, loss = 0.07749043\n",
      "Iteration 2708, loss = 0.07805530\n",
      "Iteration 2709, loss = 0.07762682\n",
      "Iteration 2710, loss = 0.07787413\n",
      "Iteration 2711, loss = 0.07782549\n",
      "Iteration 2712, loss = 0.07758384\n",
      "Iteration 2713, loss = 0.07761513\n",
      "Iteration 2714, loss = 0.07756323\n",
      "Iteration 2715, loss = 0.07741047\n",
      "Iteration 2716, loss = 0.07787472\n",
      "Iteration 2717, loss = 0.07763395\n",
      "Iteration 2718, loss = 0.07764070\n",
      "Iteration 2719, loss = 0.07777138\n",
      "Iteration 2720, loss = 0.07743471\n",
      "Iteration 2721, loss = 0.07766662\n",
      "Iteration 2722, loss = 0.07768812\n",
      "Iteration 2723, loss = 0.07769086\n",
      "Iteration 2724, loss = 0.07751485\n",
      "Iteration 2725, loss = 0.07780361\n",
      "Iteration 2726, loss = 0.07814701\n",
      "Iteration 2727, loss = 0.07768261\n",
      "Iteration 2728, loss = 0.07769336\n",
      "Iteration 2729, loss = 0.07796767\n",
      "Iteration 2730, loss = 0.07746192\n",
      "Iteration 2731, loss = 0.07741385\n",
      "Iteration 2732, loss = 0.07762846\n",
      "Iteration 2733, loss = 0.07755225\n",
      "Iteration 2734, loss = 0.07758163\n",
      "Iteration 2735, loss = 0.07760759\n",
      "Iteration 2736, loss = 0.07801373\n",
      "Iteration 2737, loss = 0.07825576\n",
      "Iteration 2738, loss = 0.07770727\n",
      "Iteration 2739, loss = 0.07767883\n",
      "Iteration 2740, loss = 0.07735143\n",
      "Iteration 2741, loss = 0.07758728\n",
      "Iteration 2742, loss = 0.07749150\n",
      "Iteration 2743, loss = 0.07767902\n",
      "Iteration 2744, loss = 0.07747258\n",
      "Iteration 2745, loss = 0.07806827\n",
      "Iteration 2746, loss = 0.07782268\n",
      "Iteration 2747, loss = 0.07736194\n",
      "Iteration 2748, loss = 0.07760021\n",
      "Iteration 2749, loss = 0.07777535\n",
      "Iteration 2750, loss = 0.07758065\n",
      "Iteration 2751, loss = 0.07772680\n",
      "Iteration 2752, loss = 0.07768031\n",
      "Iteration 2753, loss = 0.07786561\n",
      "Iteration 2754, loss = 0.07749329\n",
      "Iteration 2755, loss = 0.07738263\n",
      "Iteration 2756, loss = 0.07757621\n",
      "Iteration 2757, loss = 0.07728763\n",
      "Iteration 2758, loss = 0.07736713\n",
      "Iteration 2759, loss = 0.07787740\n",
      "Iteration 2760, loss = 0.07786715\n",
      "Iteration 2761, loss = 0.07741160\n",
      "Iteration 2762, loss = 0.07777410\n",
      "Iteration 2763, loss = 0.07748946\n",
      "Iteration 2764, loss = 0.07758537\n",
      "Iteration 2765, loss = 0.07754803\n",
      "Iteration 2766, loss = 0.07754262\n",
      "Iteration 2767, loss = 0.07736842\n",
      "Iteration 2768, loss = 0.07754805\n",
      "Iteration 2769, loss = 0.07758653\n",
      "Iteration 2770, loss = 0.07734134\n",
      "Iteration 2771, loss = 0.07751844\n",
      "Iteration 2772, loss = 0.07764433\n",
      "Iteration 2773, loss = 0.07763042\n",
      "Iteration 2774, loss = 0.07756751\n",
      "Iteration 2775, loss = 0.07741821\n",
      "Iteration 2776, loss = 0.07757293\n",
      "Iteration 2777, loss = 0.07749119\n",
      "Iteration 2778, loss = 0.07740567\n",
      "Iteration 2779, loss = 0.07863935\n",
      "Iteration 2780, loss = 0.07745164\n",
      "Iteration 2781, loss = 0.07739932\n",
      "Iteration 2782, loss = 0.07739051\n",
      "Iteration 2783, loss = 0.07763888\n",
      "Iteration 2784, loss = 0.07751438\n",
      "Iteration 2785, loss = 0.07740917\n",
      "Iteration 2786, loss = 0.07733677\n",
      "Iteration 2787, loss = 0.07734905\n",
      "Iteration 2788, loss = 0.07789443\n",
      "Iteration 2789, loss = 0.07808490\n",
      "Iteration 2790, loss = 0.07757115\n",
      "Iteration 2791, loss = 0.07750113\n",
      "Iteration 2792, loss = 0.07739523\n",
      "Iteration 2793, loss = 0.07740012\n",
      "Iteration 2794, loss = 0.07741754\n",
      "Training loss did not improve more than tol=0.000100 for 200 consecutive epochs. Stopping.\n",
      "\n",
      "Final Scores:\n",
      "Training Accuracy: 1.0000\n",
      "Validation Accuracy: 0.4112\n",
      "Test Accuracy: 0.4217\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x, y, and format string must not be None",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[135], line 164\u001B[0m\n\u001B[0;32m    161\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n\u001B[0;32m    163\u001B[0m \u001B[38;5;66;03m# 运行完整实验\u001B[39;00m\n\u001B[1;32m--> 164\u001B[0m model \u001B[38;5;241m=\u001B[39m run_experiment(xTr, yTr)\n\u001B[0;32m    166\u001B[0m \u001B[38;5;66;03m# 或者分步执行\u001B[39;00m\n\u001B[0;32m    167\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(xTr, yTr\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mravel(), test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m13\u001B[39m)\n",
      "Cell \u001B[1;32mIn[135], line 159\u001B[0m, in \u001B[0;36mrun_experiment\u001B[1;34m(xTr, yTr, test_size, random_state)\u001B[0m\n\u001B[0;32m    156\u001B[0m model \u001B[38;5;241m=\u001B[39m train_and_evaluate_model(X_train, y_train, X_test, y_test)\n\u001B[0;32m    158\u001B[0m \u001B[38;5;66;03m# 绘制学习曲线\u001B[39;00m\n\u001B[1;32m--> 159\u001B[0m plot_learning_curves(model)\n\u001B[0;32m    161\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "Cell \u001B[1;32mIn[135], line 127\u001B[0m, in \u001B[0;36mplot_learning_curves\u001B[1;34m(model)\u001B[0m\n\u001B[0;32m    125\u001B[0m plt\u001B[38;5;241m.\u001B[39msubplot(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    126\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(model\u001B[38;5;241m.\u001B[39mtrain_scores_, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTraining Score\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 127\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(model\u001B[38;5;241m.\u001B[39mvalidation_scores_, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mValidation Score\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    128\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIterations\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    129\u001B[0m plt\u001B[38;5;241m.\u001B[39mylabel(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mScore\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mD:\\Anaconda3\\Lib\\site-packages\\matplotlib\\pyplot.py:3578\u001B[0m, in \u001B[0;36mplot\u001B[1;34m(scalex, scaley, data, *args, **kwargs)\u001B[0m\n\u001B[0;32m   3570\u001B[0m \u001B[38;5;129m@_copy_docstring_and_deprecators\u001B[39m(Axes\u001B[38;5;241m.\u001B[39mplot)\n\u001B[0;32m   3571\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplot\u001B[39m(\n\u001B[0;32m   3572\u001B[0m     \u001B[38;5;241m*\u001B[39margs: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m ArrayLike \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3576\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3577\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[Line2D]:\n\u001B[1;32m-> 3578\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m gca()\u001B[38;5;241m.\u001B[39mplot(\n\u001B[0;32m   3579\u001B[0m         \u001B[38;5;241m*\u001B[39margs,\n\u001B[0;32m   3580\u001B[0m         scalex\u001B[38;5;241m=\u001B[39mscalex,\n\u001B[0;32m   3581\u001B[0m         scaley\u001B[38;5;241m=\u001B[39mscaley,\n\u001B[0;32m   3582\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m: data} \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m {}),\n\u001B[0;32m   3583\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3584\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1721\u001B[0m, in \u001B[0;36mAxes.plot\u001B[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1478\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1479\u001B[0m \u001B[38;5;124;03mPlot y versus x as lines and/or markers.\u001B[39;00m\n\u001B[0;32m   1480\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1718\u001B[0m \u001B[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001B[39;00m\n\u001B[0;32m   1719\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1720\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m cbook\u001B[38;5;241m.\u001B[39mnormalize_kwargs(kwargs, mlines\u001B[38;5;241m.\u001B[39mLine2D)\n\u001B[1;32m-> 1721\u001B[0m lines \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_lines(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, data\u001B[38;5;241m=\u001B[39mdata, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)]\n\u001B[0;32m   1722\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m lines:\n\u001B[0;32m   1723\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_line(line)\n",
      "File \u001B[1;32mD:\\Anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:303\u001B[0m, in \u001B[0;36m_process_plot_var_args.__call__\u001B[1;34m(self, axes, data, *args, **kwargs)\u001B[0m\n\u001B[0;32m    301\u001B[0m     this \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m    302\u001B[0m     args \u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m1\u001B[39m:]\n\u001B[1;32m--> 303\u001B[0m \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plot_args(\n\u001B[0;32m    304\u001B[0m     axes, this, kwargs, ambiguous_fmt_datakey\u001B[38;5;241m=\u001B[39mambiguous_fmt_datakey)\n",
      "File \u001B[1;32mD:\\Anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:460\u001B[0m, in \u001B[0;36m_process_plot_var_args._plot_args\u001B[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001B[0m\n\u001B[0;32m    457\u001B[0m \u001B[38;5;66;03m# Don't allow any None value; these would be up-converted to one\u001B[39;00m\n\u001B[0;32m    458\u001B[0m \u001B[38;5;66;03m# element array of None which causes problems downstream.\u001B[39;00m\n\u001B[0;32m    459\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(v \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m tup):\n\u001B[1;32m--> 460\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx, y, and format string must not be None\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    462\u001B[0m kw \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    463\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m prop_name, val \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m((\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlinestyle\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmarker\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcolor\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[0;32m    464\u001B[0m                           (linestyle, marker, color)):\n",
      "\u001B[1;31mValueError\u001B[0m: x, y, and format string must not be None"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 135
  },
  {
   "cell_type": "markdown",
   "id": "edee8858",
   "metadata": {},
   "source": [
    "### 3.6 Simple Neural Network ####"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T05:33:43.136920Z",
     "start_time": "2025-02-04T05:30:21.452682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.neural_network._stochastic_optimizers import AdamOptimizer\n",
    "from sklearn.base import clone\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "class MonitorMLPClassifier(MLPClassifier):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_scores_ = []\n",
    "        self.val_scores_ = []\n",
    "        self.loss_curve_ = []\n",
    "        self.iter_ = 0\n",
    "    \n",
    "    def _partial_fit(self, X, y, *args, **kwargs):\n",
    "        if self.early_stopping:\n",
    "            # 分割验证集\n",
    "            n_samples = X.shape[0]\n",
    "            n_val = int(n_samples * self.validation_fraction)\n",
    "            X_val = X[-n_val:]\n",
    "            y_val = y[-n_val:]\n",
    "            X_train = X[:-n_val]\n",
    "            y_train = y[:-n_val]\n",
    "            \n",
    "            # 训练\n",
    "            super()._partial_fit(X_train, y_train, *args, **kwargs)\n",
    "            \n",
    "            # 记录分数\n",
    "            train_score = accuracy_score(y_train, self.predict(X_train))\n",
    "            val_score = accuracy_score(y_val, self.predict(X_val))\n",
    "            \n",
    "            self.train_scores_.append(train_score)\n",
    "            self.val_scores_.append(val_score)\n",
    "            self.iter_ += 1\n",
    "            if self.verbose & self.iter_ % 10 == 0:\n",
    "                print(f\"Epoch {len(self.train_scores_)}\")\n",
    "                print(f\"Training Score: {train_score:.4f}\")\n",
    "                print(f\"Validation Score: {val_score:.4f}\")\n",
    "                print(f\"Loss: {self.loss_curve_[-1]:.4f}\")\n",
    "                print(\"-\" * 50)\n",
    "        else:\n",
    "            super()._partial_fit(X, y, *args, **kwargs)\n",
    "            \n",
    "        return self\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "monitor_clf = MonitorMLPClassifier(\n",
    "    hidden_layer_sizes=(64,32,16),\n",
    "    activation='relu',\n",
    "    batch_size=128,\n",
    "    max_iter=10000,\n",
    "    learning_rate_init=1e-4,\n",
    "    early_stopping=False,\n",
    "    alpha=1e-3,\n",
    "    verbose=True,\n",
    "    tol=1e-4,\n",
    "    random_state=13,\n",
    "    learning_rate='adaptive',\n",
    "    n_iter_no_change=10,\n",
    "    validation_fraction=0.2\n",
    ").fit(xTr, yTr.values.ravel())\n",
    "\n",
    "# 绘制训练过程\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(monitor_clf.train_scores_, label='Training Score')\n",
    "plt.plot(monitor_clf.validation_scores_, label='Validation Score')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Training and Validation Scores Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "d0743111542e7096",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.23070062\n",
      "Iteration 2, loss = 1.12545994\n",
      "Iteration 3, loss = 1.04178538\n",
      "Iteration 4, loss = 1.00074267\n",
      "Iteration 5, loss = 0.98849254\n",
      "Iteration 6, loss = 0.98196331\n",
      "Iteration 7, loss = 0.97823506\n",
      "Iteration 8, loss = 0.97545673\n",
      "Iteration 9, loss = 0.97357442\n",
      "Iteration 10, loss = 0.97163676\n",
      "Iteration 11, loss = 0.97024414\n",
      "Iteration 12, loss = 0.96882293\n",
      "Iteration 13, loss = 0.96744006\n",
      "Iteration 14, loss = 0.96592732\n",
      "Iteration 15, loss = 0.96489714\n",
      "Iteration 16, loss = 0.96380466\n",
      "Iteration 17, loss = 0.96220817\n",
      "Iteration 18, loss = 0.96099659\n",
      "Iteration 19, loss = 0.96003688\n",
      "Iteration 20, loss = 0.95908802\n",
      "Iteration 21, loss = 0.95767026\n",
      "Iteration 22, loss = 0.95644549\n",
      "Iteration 23, loss = 0.95535147\n",
      "Iteration 24, loss = 0.95420171\n",
      "Iteration 25, loss = 0.95285765\n",
      "Iteration 26, loss = 0.95174647\n",
      "Iteration 27, loss = 0.95060868\n",
      "Iteration 28, loss = 0.94963681\n",
      "Iteration 29, loss = 0.94808009\n",
      "Iteration 30, loss = 0.94676124\n",
      "Iteration 31, loss = 0.94547406\n",
      "Iteration 32, loss = 0.94457769\n",
      "Iteration 33, loss = 0.94293914\n",
      "Iteration 34, loss = 0.94180363\n",
      "Iteration 35, loss = 0.94044193\n",
      "Iteration 36, loss = 0.93876969\n",
      "Iteration 37, loss = 0.93754626\n",
      "Iteration 38, loss = 0.93590986\n",
      "Iteration 39, loss = 0.93447346\n",
      "Iteration 40, loss = 0.93331167\n",
      "Iteration 41, loss = 0.93159833\n",
      "Iteration 42, loss = 0.93021757\n",
      "Iteration 43, loss = 0.92864542\n",
      "Iteration 44, loss = 0.92730105\n",
      "Iteration 45, loss = 0.92570275\n",
      "Iteration 46, loss = 0.92398816\n",
      "Iteration 47, loss = 0.92272097\n",
      "Iteration 48, loss = 0.92114293\n",
      "Iteration 49, loss = 0.91961782\n",
      "Iteration 50, loss = 0.91820745\n",
      "Iteration 51, loss = 0.91640396\n",
      "Iteration 52, loss = 0.91503921\n",
      "Iteration 53, loss = 0.91322968\n",
      "Iteration 54, loss = 0.91194496\n",
      "Iteration 55, loss = 0.91008525\n",
      "Iteration 56, loss = 0.90897169\n",
      "Iteration 57, loss = 0.90715900\n",
      "Iteration 58, loss = 0.90598692\n",
      "Iteration 59, loss = 0.90417372\n",
      "Iteration 60, loss = 0.90261877\n",
      "Iteration 61, loss = 0.90129298\n",
      "Iteration 62, loss = 0.89926105\n",
      "Iteration 63, loss = 0.89764925\n",
      "Iteration 64, loss = 0.89594497\n",
      "Iteration 65, loss = 0.89451852\n",
      "Iteration 66, loss = 0.89276821\n",
      "Iteration 67, loss = 0.89107239\n",
      "Iteration 68, loss = 0.88970161\n",
      "Iteration 69, loss = 0.88796644\n",
      "Iteration 70, loss = 0.88604468\n",
      "Iteration 71, loss = 0.88437704\n",
      "Iteration 72, loss = 0.88299737\n",
      "Iteration 73, loss = 0.88072453\n",
      "Iteration 74, loss = 0.87962137\n",
      "Iteration 75, loss = 0.87745239\n",
      "Iteration 76, loss = 0.87644164\n",
      "Iteration 77, loss = 0.87417181\n",
      "Iteration 78, loss = 0.87206869\n",
      "Iteration 79, loss = 0.86982921\n",
      "Iteration 80, loss = 0.86924482\n",
      "Iteration 81, loss = 0.86676988\n",
      "Iteration 82, loss = 0.86452886\n",
      "Iteration 83, loss = 0.86292594\n",
      "Iteration 84, loss = 0.86083117\n",
      "Iteration 85, loss = 0.85948037\n",
      "Iteration 86, loss = 0.85712057\n",
      "Iteration 87, loss = 0.85537858\n",
      "Iteration 88, loss = 0.85375902\n",
      "Iteration 89, loss = 0.85165429\n",
      "Iteration 90, loss = 0.84967712\n",
      "Iteration 91, loss = 0.84793756\n",
      "Iteration 92, loss = 0.84566541\n",
      "Iteration 93, loss = 0.84353808\n",
      "Iteration 94, loss = 0.84170729\n",
      "Iteration 95, loss = 0.83988400\n",
      "Iteration 96, loss = 0.83762730\n",
      "Iteration 97, loss = 0.83554536\n",
      "Iteration 98, loss = 0.83420292\n",
      "Iteration 99, loss = 0.83100711\n",
      "Iteration 100, loss = 0.82990934\n",
      "Iteration 101, loss = 0.82782118\n",
      "Iteration 102, loss = 0.82546177\n",
      "Iteration 103, loss = 0.82370673\n",
      "Iteration 104, loss = 0.82112576\n",
      "Iteration 105, loss = 0.81919077\n",
      "Iteration 106, loss = 0.81750990\n",
      "Iteration 107, loss = 0.81478625\n",
      "Iteration 108, loss = 0.81353283\n",
      "Iteration 109, loss = 0.81048385\n",
      "Iteration 110, loss = 0.80876134\n",
      "Iteration 111, loss = 0.80752101\n",
      "Iteration 112, loss = 0.80458158\n",
      "Iteration 113, loss = 0.80202334\n",
      "Iteration 114, loss = 0.80124442\n",
      "Iteration 115, loss = 0.79902925\n",
      "Iteration 116, loss = 0.79623468\n",
      "Iteration 117, loss = 0.79398704\n",
      "Iteration 118, loss = 0.79152245\n",
      "Iteration 119, loss = 0.79003316\n",
      "Iteration 120, loss = 0.78741880\n",
      "Iteration 121, loss = 0.78604163\n",
      "Iteration 122, loss = 0.78271923\n",
      "Iteration 123, loss = 0.78040482\n",
      "Iteration 124, loss = 0.77835927\n",
      "Iteration 125, loss = 0.77624039\n",
      "Iteration 126, loss = 0.77476804\n",
      "Iteration 127, loss = 0.77177192\n",
      "Iteration 128, loss = 0.76974413\n",
      "Iteration 129, loss = 0.76750565\n",
      "Iteration 130, loss = 0.76628238\n",
      "Iteration 131, loss = 0.76318575\n",
      "Iteration 132, loss = 0.76101268\n",
      "Iteration 133, loss = 0.75986646\n",
      "Iteration 134, loss = 0.75703995\n",
      "Iteration 135, loss = 0.75509482\n",
      "Iteration 136, loss = 0.75264829\n",
      "Iteration 137, loss = 0.75061634\n",
      "Iteration 138, loss = 0.74807464\n",
      "Iteration 139, loss = 0.74583731\n",
      "Iteration 140, loss = 0.74386597\n",
      "Iteration 141, loss = 0.74270497\n",
      "Iteration 142, loss = 0.74079542\n",
      "Iteration 143, loss = 0.73682424\n",
      "Iteration 144, loss = 0.73540730\n",
      "Iteration 145, loss = 0.73263580\n",
      "Iteration 146, loss = 0.73090290\n",
      "Iteration 147, loss = 0.72938950\n",
      "Iteration 148, loss = 0.72631388\n",
      "Iteration 149, loss = 0.72383909\n",
      "Iteration 150, loss = 0.72252337\n",
      "Iteration 151, loss = 0.72001884\n",
      "Iteration 152, loss = 0.71834950\n",
      "Iteration 153, loss = 0.71552843\n",
      "Iteration 154, loss = 0.71376493\n",
      "Iteration 155, loss = 0.71168210\n",
      "Iteration 156, loss = 0.70842310\n",
      "Iteration 157, loss = 0.70699734\n",
      "Iteration 158, loss = 0.70486538\n",
      "Iteration 159, loss = 0.70277091\n",
      "Iteration 160, loss = 0.70075492\n",
      "Iteration 161, loss = 0.69820428\n",
      "Iteration 162, loss = 0.69620013\n",
      "Iteration 163, loss = 0.69319202\n",
      "Iteration 164, loss = 0.69191908\n",
      "Iteration 165, loss = 0.69021051\n",
      "Iteration 166, loss = 0.68729888\n",
      "Iteration 167, loss = 0.68501945\n",
      "Iteration 168, loss = 0.68220169\n",
      "Iteration 169, loss = 0.67991217\n",
      "Iteration 170, loss = 0.67867859\n",
      "Iteration 171, loss = 0.67625130\n",
      "Iteration 172, loss = 0.67355509\n",
      "Iteration 173, loss = 0.67204443\n",
      "Iteration 174, loss = 0.66940801\n",
      "Iteration 175, loss = 0.66718049\n",
      "Iteration 176, loss = 0.66483572\n",
      "Iteration 177, loss = 0.66237638\n",
      "Iteration 178, loss = 0.66072759\n",
      "Iteration 179, loss = 0.65876202\n",
      "Iteration 180, loss = 0.65591155\n",
      "Iteration 181, loss = 0.65414711\n",
      "Iteration 182, loss = 0.65116718\n",
      "Iteration 183, loss = 0.64897455\n",
      "Iteration 184, loss = 0.64746724\n",
      "Iteration 185, loss = 0.64577309\n",
      "Iteration 186, loss = 0.64286428\n",
      "Iteration 187, loss = 0.64030516\n",
      "Iteration 188, loss = 0.63822263\n",
      "Iteration 189, loss = 0.63641374\n",
      "Iteration 190, loss = 0.63375899\n",
      "Iteration 191, loss = 0.63070451\n",
      "Iteration 192, loss = 0.62923528\n",
      "Iteration 193, loss = 0.62677832\n",
      "Iteration 194, loss = 0.62425903\n",
      "Iteration 195, loss = 0.62321761\n",
      "Iteration 196, loss = 0.62030170\n",
      "Iteration 197, loss = 0.61899974\n",
      "Iteration 198, loss = 0.61631014\n",
      "Iteration 199, loss = 0.61411202\n",
      "Iteration 200, loss = 0.61246153\n",
      "Iteration 201, loss = 0.60956145\n",
      "Iteration 202, loss = 0.60802381\n",
      "Iteration 203, loss = 0.60593266\n",
      "Iteration 204, loss = 0.60321037\n",
      "Iteration 205, loss = 0.60043566\n",
      "Iteration 206, loss = 0.59851763\n",
      "Iteration 207, loss = 0.59632303\n",
      "Iteration 208, loss = 0.59450330\n",
      "Iteration 209, loss = 0.59246735\n",
      "Iteration 210, loss = 0.59038244\n",
      "Iteration 211, loss = 0.58752484\n",
      "Iteration 212, loss = 0.58543816\n",
      "Iteration 213, loss = 0.58299733\n",
      "Iteration 214, loss = 0.58211000\n",
      "Iteration 215, loss = 0.57860933\n",
      "Iteration 216, loss = 0.57684454\n",
      "Iteration 217, loss = 0.57421562\n",
      "Iteration 218, loss = 0.57224607\n",
      "Iteration 219, loss = 0.56908366\n",
      "Iteration 220, loss = 0.56909242\n",
      "Iteration 221, loss = 0.56573953\n",
      "Iteration 222, loss = 0.56347732\n",
      "Iteration 223, loss = 0.56141956\n",
      "Iteration 224, loss = 0.55857376\n",
      "Iteration 225, loss = 0.55743815\n",
      "Iteration 226, loss = 0.55580740\n",
      "Iteration 227, loss = 0.55265357\n",
      "Iteration 228, loss = 0.55069644\n",
      "Iteration 229, loss = 0.54810863\n",
      "Iteration 230, loss = 0.54584885\n",
      "Iteration 231, loss = 0.54307536\n",
      "Iteration 232, loss = 0.54221115\n",
      "Iteration 233, loss = 0.54071468\n",
      "Iteration 234, loss = 0.53886609\n",
      "Iteration 235, loss = 0.53565686\n",
      "Iteration 236, loss = 0.53389588\n",
      "Iteration 237, loss = 0.53177080\n",
      "Iteration 238, loss = 0.53006542\n",
      "Iteration 239, loss = 0.52828825\n",
      "Iteration 240, loss = 0.52488766\n",
      "Iteration 241, loss = 0.52386321\n",
      "Iteration 242, loss = 0.52207113\n",
      "Iteration 243, loss = 0.51852598\n",
      "Iteration 244, loss = 0.51636123\n",
      "Iteration 245, loss = 0.51497759\n",
      "Iteration 246, loss = 0.51395798\n",
      "Iteration 247, loss = 0.51064264\n",
      "Iteration 248, loss = 0.50867930\n",
      "Iteration 249, loss = 0.50688670\n",
      "Iteration 250, loss = 0.50443937\n",
      "Iteration 251, loss = 0.50282147\n",
      "Iteration 252, loss = 0.50053020\n",
      "Iteration 253, loss = 0.49921393\n",
      "Iteration 254, loss = 0.49706999\n",
      "Iteration 255, loss = 0.49484853\n",
      "Iteration 256, loss = 0.49344139\n",
      "Iteration 257, loss = 0.49034538\n",
      "Iteration 258, loss = 0.48923904\n",
      "Iteration 259, loss = 0.48737956\n",
      "Iteration 260, loss = 0.48335105\n",
      "Iteration 261, loss = 0.48174641\n",
      "Iteration 262, loss = 0.48012178\n",
      "Iteration 263, loss = 0.47835228\n",
      "Iteration 264, loss = 0.47804457\n",
      "Iteration 265, loss = 0.47455032\n",
      "Iteration 266, loss = 0.47170794\n",
      "Iteration 267, loss = 0.47007112\n",
      "Iteration 268, loss = 0.47005719\n",
      "Iteration 269, loss = 0.46655564\n",
      "Iteration 270, loss = 0.46429136\n",
      "Iteration 271, loss = 0.46364369\n",
      "Iteration 272, loss = 0.46032599\n",
      "Iteration 273, loss = 0.45826289\n",
      "Iteration 274, loss = 0.45707311\n",
      "Iteration 275, loss = 0.45758222\n",
      "Iteration 276, loss = 0.45598854\n",
      "Iteration 277, loss = 0.45116961\n",
      "Iteration 278, loss = 0.44885377\n",
      "Iteration 279, loss = 0.44663524\n",
      "Iteration 280, loss = 0.44553558\n",
      "Iteration 281, loss = 0.44338281\n",
      "Iteration 282, loss = 0.44244537\n",
      "Iteration 283, loss = 0.44008124\n",
      "Iteration 284, loss = 0.43705185\n",
      "Iteration 285, loss = 0.43723032\n",
      "Iteration 286, loss = 0.43464352\n",
      "Iteration 287, loss = 0.43152863\n",
      "Iteration 288, loss = 0.43056466\n",
      "Iteration 289, loss = 0.42897370\n",
      "Iteration 290, loss = 0.42744297\n",
      "Iteration 291, loss = 0.42435488\n",
      "Iteration 292, loss = 0.42167198\n",
      "Iteration 293, loss = 0.42108700\n",
      "Iteration 294, loss = 0.41814885\n",
      "Iteration 295, loss = 0.41866008\n",
      "Iteration 296, loss = 0.41609011\n",
      "Iteration 297, loss = 0.41427188\n",
      "Iteration 298, loss = 0.41218771\n",
      "Iteration 299, loss = 0.41047027\n",
      "Iteration 300, loss = 0.40893358\n",
      "Iteration 301, loss = 0.40611328\n",
      "Iteration 302, loss = 0.40635779\n",
      "Iteration 303, loss = 0.40319798\n",
      "Iteration 304, loss = 0.40102992\n",
      "Iteration 305, loss = 0.39971292\n",
      "Iteration 306, loss = 0.39746186\n",
      "Iteration 307, loss = 0.39623447\n",
      "Iteration 308, loss = 0.39436811\n",
      "Iteration 309, loss = 0.39130622\n",
      "Iteration 310, loss = 0.39149548\n",
      "Iteration 311, loss = 0.38853572\n",
      "Iteration 312, loss = 0.38735591\n",
      "Iteration 313, loss = 0.38810722\n",
      "Iteration 314, loss = 0.38441724\n",
      "Iteration 315, loss = 0.38199754\n",
      "Iteration 316, loss = 0.38060692\n",
      "Iteration 317, loss = 0.37969858\n",
      "Iteration 318, loss = 0.37694973\n",
      "Iteration 319, loss = 0.37645259\n",
      "Iteration 320, loss = 0.37406107\n",
      "Iteration 321, loss = 0.37228216\n",
      "Iteration 322, loss = 0.37024914\n",
      "Iteration 323, loss = 0.37133715\n",
      "Iteration 324, loss = 0.36847945\n",
      "Iteration 325, loss = 0.36493115\n",
      "Iteration 326, loss = 0.36372255\n",
      "Iteration 327, loss = 0.36399265\n",
      "Iteration 328, loss = 0.35988546\n",
      "Iteration 329, loss = 0.35821104\n",
      "Iteration 330, loss = 0.35609168\n",
      "Iteration 331, loss = 0.35577549\n",
      "Iteration 332, loss = 0.35400928\n",
      "Iteration 333, loss = 0.35247130\n",
      "Iteration 334, loss = 0.35313035\n",
      "Iteration 335, loss = 0.35160985\n",
      "Iteration 336, loss = 0.34677356\n",
      "Iteration 337, loss = 0.34466058\n",
      "Iteration 338, loss = 0.34451625\n",
      "Iteration 339, loss = 0.34252965\n",
      "Iteration 340, loss = 0.34166459\n",
      "Iteration 341, loss = 0.33964607\n",
      "Iteration 342, loss = 0.33993949\n",
      "Iteration 343, loss = 0.33605674\n",
      "Iteration 344, loss = 0.33412915\n",
      "Iteration 345, loss = 0.33317915\n",
      "Iteration 346, loss = 0.33158667\n",
      "Iteration 347, loss = 0.32995398\n",
      "Iteration 348, loss = 0.32865845\n",
      "Iteration 349, loss = 0.32774125\n",
      "Iteration 350, loss = 0.32608896\n",
      "Iteration 351, loss = 0.32723756\n",
      "Iteration 352, loss = 0.32248310\n",
      "Iteration 353, loss = 0.32225315\n",
      "Iteration 354, loss = 0.32049839\n",
      "Iteration 355, loss = 0.31895521\n",
      "Iteration 356, loss = 0.31620389\n",
      "Iteration 357, loss = 0.31447072\n",
      "Iteration 358, loss = 0.31182387\n",
      "Iteration 359, loss = 0.31101488\n",
      "Iteration 360, loss = 0.31134257\n",
      "Iteration 361, loss = 0.30998881\n",
      "Iteration 362, loss = 0.30660235\n",
      "Iteration 363, loss = 0.30636999\n",
      "Iteration 364, loss = 0.30377196\n",
      "Iteration 365, loss = 0.30226952\n",
      "Iteration 366, loss = 0.30172793\n",
      "Iteration 367, loss = 0.29969580\n",
      "Iteration 368, loss = 0.29922273\n",
      "Iteration 369, loss = 0.29647791\n",
      "Iteration 370, loss = 0.29657460\n",
      "Iteration 371, loss = 0.29400667\n",
      "Iteration 372, loss = 0.29153958\n",
      "Iteration 373, loss = 0.29038840\n",
      "Iteration 374, loss = 0.28962102\n",
      "Iteration 375, loss = 0.28777378\n",
      "Iteration 376, loss = 0.28615031\n",
      "Iteration 377, loss = 0.28461951\n",
      "Iteration 378, loss = 0.28390895\n",
      "Iteration 379, loss = 0.28281947\n",
      "Iteration 380, loss = 0.28085534\n",
      "Iteration 381, loss = 0.27837467\n",
      "Iteration 382, loss = 0.27726568\n",
      "Iteration 383, loss = 0.27877951\n",
      "Iteration 384, loss = 0.27717489\n",
      "Iteration 385, loss = 0.27456183\n",
      "Iteration 386, loss = 0.27325906\n",
      "Iteration 387, loss = 0.27139655\n",
      "Iteration 388, loss = 0.27078302\n",
      "Iteration 389, loss = 0.27066966\n",
      "Iteration 390, loss = 0.26606169\n",
      "Iteration 391, loss = 0.26464186\n",
      "Iteration 392, loss = 0.26387679\n",
      "Iteration 393, loss = 0.26207777\n",
      "Iteration 394, loss = 0.26060883\n",
      "Iteration 395, loss = 0.25954706\n",
      "Iteration 396, loss = 0.25702998\n",
      "Iteration 397, loss = 0.25633312\n",
      "Iteration 398, loss = 0.25523191\n",
      "Iteration 399, loss = 0.25575222\n",
      "Iteration 400, loss = 0.25358947\n",
      "Iteration 401, loss = 0.25288113\n",
      "Iteration 402, loss = 0.25407105\n",
      "Iteration 403, loss = 0.24936561\n",
      "Iteration 404, loss = 0.24776574\n",
      "Iteration 405, loss = 0.24651905\n",
      "Iteration 406, loss = 0.24633495\n",
      "Iteration 407, loss = 0.24359940\n",
      "Iteration 408, loss = 0.24366198\n",
      "Iteration 409, loss = 0.24226742\n",
      "Iteration 410, loss = 0.23909154\n",
      "Iteration 411, loss = 0.23726859\n",
      "Iteration 412, loss = 0.23775468\n",
      "Iteration 413, loss = 0.23554369\n",
      "Iteration 414, loss = 0.23405663\n",
      "Iteration 415, loss = 0.23269155\n",
      "Iteration 416, loss = 0.23178082\n",
      "Iteration 417, loss = 0.23090950\n",
      "Iteration 418, loss = 0.23020191\n",
      "Iteration 419, loss = 0.22975136\n",
      "Iteration 420, loss = 0.22681027\n",
      "Iteration 421, loss = 0.22658539\n",
      "Iteration 422, loss = 0.22396257\n",
      "Iteration 423, loss = 0.22346292\n",
      "Iteration 424, loss = 0.22340248\n",
      "Iteration 425, loss = 0.22197532\n",
      "Iteration 426, loss = 0.22054870\n",
      "Iteration 427, loss = 0.21807537\n",
      "Iteration 428, loss = 0.21591497\n",
      "Iteration 429, loss = 0.21583992\n",
      "Iteration 430, loss = 0.21658276\n",
      "Iteration 431, loss = 0.21365061\n",
      "Iteration 432, loss = 0.21344859\n",
      "Iteration 433, loss = 0.21193451\n",
      "Iteration 434, loss = 0.21046198\n",
      "Iteration 435, loss = 0.20949294\n",
      "Iteration 436, loss = 0.20726807\n",
      "Iteration 437, loss = 0.20619701\n",
      "Iteration 438, loss = 0.20611027\n",
      "Iteration 439, loss = 0.20472203\n",
      "Iteration 440, loss = 0.20277895\n",
      "Iteration 441, loss = 0.20131000\n",
      "Iteration 442, loss = 0.20044250\n",
      "Iteration 443, loss = 0.20003647\n",
      "Iteration 444, loss = 0.19858232\n",
      "Iteration 445, loss = 0.19662547\n",
      "Iteration 446, loss = 0.19598623\n",
      "Iteration 447, loss = 0.19438202\n",
      "Iteration 448, loss = 0.19290135\n",
      "Iteration 449, loss = 0.19464626\n",
      "Iteration 450, loss = 0.19035817\n",
      "Iteration 451, loss = 0.18998608\n",
      "Iteration 452, loss = 0.19075524\n",
      "Iteration 453, loss = 0.18654487\n",
      "Iteration 454, loss = 0.18977584\n",
      "Iteration 455, loss = 0.18503043\n",
      "Iteration 456, loss = 0.18488995\n",
      "Iteration 457, loss = 0.18379445\n",
      "Iteration 458, loss = 0.18175979\n",
      "Iteration 459, loss = 0.18131744\n",
      "Iteration 460, loss = 0.18047607\n",
      "Iteration 461, loss = 0.18123770\n",
      "Iteration 462, loss = 0.17929330\n",
      "Iteration 463, loss = 0.17747491\n",
      "Iteration 464, loss = 0.17607049\n",
      "Iteration 465, loss = 0.17469852\n",
      "Iteration 466, loss = 0.17368996\n",
      "Iteration 467, loss = 0.17210463\n",
      "Iteration 468, loss = 0.17473816\n",
      "Iteration 469, loss = 0.17008897\n",
      "Iteration 470, loss = 0.17059001\n",
      "Iteration 471, loss = 0.17056071\n",
      "Iteration 472, loss = 0.16934562\n",
      "Iteration 473, loss = 0.16743509\n",
      "Iteration 474, loss = 0.16562710\n",
      "Iteration 475, loss = 0.16427425\n",
      "Iteration 476, loss = 0.16302619\n",
      "Iteration 477, loss = 0.16216790\n",
      "Iteration 478, loss = 0.16226094\n",
      "Iteration 479, loss = 0.16016284\n",
      "Iteration 480, loss = 0.16009927\n",
      "Iteration 481, loss = 0.15783978\n",
      "Iteration 482, loss = 0.15857981\n",
      "Iteration 483, loss = 0.15716467\n",
      "Iteration 484, loss = 0.15675150\n",
      "Iteration 485, loss = 0.15381551\n",
      "Iteration 486, loss = 0.15418415\n",
      "Iteration 487, loss = 0.15381676\n",
      "Iteration 488, loss = 0.15214259\n",
      "Iteration 489, loss = 0.15145640\n",
      "Iteration 490, loss = 0.15295086\n",
      "Iteration 491, loss = 0.15073781\n",
      "Iteration 492, loss = 0.14719029\n",
      "Iteration 493, loss = 0.14766566\n",
      "Iteration 494, loss = 0.14676453\n",
      "Iteration 495, loss = 0.14466395\n",
      "Iteration 496, loss = 0.14786317\n",
      "Iteration 497, loss = 0.14340943\n",
      "Iteration 498, loss = 0.14302460\n",
      "Iteration 499, loss = 0.14500187\n",
      "Iteration 500, loss = 0.14105237\n",
      "Iteration 501, loss = 0.14016865\n",
      "Iteration 502, loss = 0.13826615\n",
      "Iteration 503, loss = 0.13848663\n",
      "Iteration 504, loss = 0.13693443\n",
      "Iteration 505, loss = 0.13625965\n",
      "Iteration 506, loss = 0.13775983\n",
      "Iteration 507, loss = 0.13421576\n",
      "Iteration 508, loss = 0.13562664\n",
      "Iteration 509, loss = 0.13250955\n",
      "Iteration 510, loss = 0.13065222\n",
      "Iteration 511, loss = 0.13427777\n",
      "Iteration 512, loss = 0.13159762\n",
      "Iteration 513, loss = 0.12868305\n",
      "Iteration 514, loss = 0.12792621\n",
      "Iteration 515, loss = 0.12723230\n",
      "Iteration 516, loss = 0.12701882\n",
      "Iteration 517, loss = 0.12623829\n",
      "Iteration 518, loss = 0.12445085\n",
      "Iteration 519, loss = 0.12370247\n",
      "Iteration 520, loss = 0.12287983\n",
      "Iteration 521, loss = 0.12221519\n",
      "Iteration 522, loss = 0.12175816\n",
      "Iteration 523, loss = 0.12088975\n",
      "Iteration 524, loss = 0.11901278\n",
      "Iteration 525, loss = 0.11837883\n",
      "Iteration 526, loss = 0.11950229\n",
      "Iteration 527, loss = 0.11854489\n",
      "Iteration 528, loss = 0.11737256\n",
      "Iteration 529, loss = 0.11534737\n",
      "Iteration 530, loss = 0.11416084\n",
      "Iteration 531, loss = 0.11324357\n",
      "Iteration 532, loss = 0.11306598\n",
      "Iteration 533, loss = 0.11382350\n",
      "Iteration 534, loss = 0.11200462\n",
      "Iteration 535, loss = 0.11068674\n",
      "Iteration 536, loss = 0.11083818\n",
      "Iteration 537, loss = 0.11052678\n",
      "Iteration 538, loss = 0.10849612\n",
      "Iteration 539, loss = 0.10823805\n",
      "Iteration 540, loss = 0.10847429\n",
      "Iteration 541, loss = 0.10738358\n",
      "Iteration 542, loss = 0.10626010\n",
      "Iteration 543, loss = 0.10611397\n",
      "Iteration 544, loss = 0.10437086\n",
      "Iteration 545, loss = 0.10342644\n",
      "Iteration 546, loss = 0.10297152\n",
      "Iteration 547, loss = 0.10247145\n",
      "Iteration 548, loss = 0.10117620\n",
      "Iteration 549, loss = 0.10074475\n",
      "Iteration 550, loss = 0.09952324\n",
      "Iteration 551, loss = 0.09966747\n",
      "Iteration 552, loss = 0.09835985\n",
      "Iteration 553, loss = 0.09878456\n",
      "Iteration 554, loss = 0.09818877\n",
      "Iteration 555, loss = 0.09725412\n",
      "Iteration 556, loss = 0.09597951\n",
      "Iteration 557, loss = 0.09888183\n",
      "Iteration 558, loss = 0.09540685\n",
      "Iteration 559, loss = 0.09304749\n",
      "Iteration 560, loss = 0.09248462\n",
      "Iteration 561, loss = 0.09327330\n",
      "Iteration 562, loss = 0.09172913\n",
      "Iteration 563, loss = 0.09154476\n",
      "Iteration 564, loss = 0.09047412\n",
      "Iteration 565, loss = 0.09039508\n",
      "Iteration 566, loss = 0.08940493\n",
      "Iteration 567, loss = 0.08850060\n",
      "Iteration 568, loss = 0.08792591\n",
      "Iteration 569, loss = 0.08747842\n",
      "Iteration 570, loss = 0.08740885\n",
      "Iteration 571, loss = 0.08745782\n",
      "Iteration 572, loss = 0.08589241\n",
      "Iteration 573, loss = 0.08515013\n",
      "Iteration 574, loss = 0.08513489\n",
      "Iteration 575, loss = 0.08258786\n",
      "Iteration 576, loss = 0.08414479\n",
      "Iteration 577, loss = 0.08194694\n",
      "Iteration 578, loss = 0.08052343\n",
      "Iteration 579, loss = 0.08023364\n",
      "Iteration 580, loss = 0.08084295\n",
      "Iteration 581, loss = 0.07940640\n",
      "Iteration 582, loss = 0.08039454\n",
      "Iteration 583, loss = 0.07909848\n",
      "Iteration 584, loss = 0.07797248\n",
      "Iteration 585, loss = 0.07814560\n",
      "Iteration 586, loss = 0.07705404\n",
      "Iteration 587, loss = 0.07695024\n",
      "Iteration 588, loss = 0.07520645\n",
      "Iteration 589, loss = 0.07468064\n",
      "Iteration 590, loss = 0.07409374\n",
      "Iteration 591, loss = 0.07304078\n",
      "Iteration 592, loss = 0.07521359\n",
      "Iteration 593, loss = 0.07224770\n",
      "Iteration 594, loss = 0.07269491\n",
      "Iteration 595, loss = 0.07207490\n",
      "Iteration 596, loss = 0.07066858\n",
      "Iteration 597, loss = 0.06992442\n",
      "Iteration 598, loss = 0.06889479\n",
      "Iteration 599, loss = 0.06934146\n",
      "Iteration 600, loss = 0.06867881\n",
      "Iteration 601, loss = 0.06923736\n",
      "Iteration 602, loss = 0.06801924\n",
      "Iteration 603, loss = 0.06871137\n",
      "Iteration 604, loss = 0.06661229\n",
      "Iteration 605, loss = 0.06601100\n",
      "Iteration 606, loss = 0.06514406\n",
      "Iteration 607, loss = 0.06531603\n",
      "Iteration 608, loss = 0.06435621\n",
      "Iteration 609, loss = 0.06333837\n",
      "Iteration 610, loss = 0.06275477\n",
      "Iteration 611, loss = 0.06221000\n",
      "Iteration 612, loss = 0.06202707\n",
      "Iteration 613, loss = 0.05992387\n",
      "Iteration 614, loss = 0.06132526\n",
      "Iteration 615, loss = 0.06056449\n",
      "Iteration 616, loss = 0.06005826\n",
      "Iteration 617, loss = 0.06171766\n",
      "Iteration 618, loss = 0.05967589\n",
      "Iteration 619, loss = 0.05895767\n",
      "Iteration 620, loss = 0.05768034\n",
      "Iteration 621, loss = 0.05769983\n",
      "Iteration 622, loss = 0.05826950\n",
      "Iteration 623, loss = 0.05703530\n",
      "Iteration 624, loss = 0.05652722\n",
      "Iteration 625, loss = 0.05567365\n",
      "Iteration 626, loss = 0.05616042\n",
      "Iteration 627, loss = 0.05462393\n",
      "Iteration 628, loss = 0.05508190\n",
      "Iteration 629, loss = 0.05376387\n",
      "Iteration 630, loss = 0.05376843\n",
      "Iteration 631, loss = 0.05239049\n",
      "Iteration 632, loss = 0.05262700\n",
      "Iteration 633, loss = 0.05110643\n",
      "Iteration 634, loss = 0.05318752\n",
      "Iteration 635, loss = 0.05189047\n",
      "Iteration 636, loss = 0.05052193\n",
      "Iteration 637, loss = 0.04993900\n",
      "Iteration 638, loss = 0.04959859\n",
      "Iteration 639, loss = 0.04932923\n",
      "Iteration 640, loss = 0.04832347\n",
      "Iteration 641, loss = 0.04868958\n",
      "Iteration 642, loss = 0.04960236\n",
      "Iteration 643, loss = 0.04775380\n",
      "Iteration 644, loss = 0.04836796\n",
      "Iteration 645, loss = 0.04591880\n",
      "Iteration 646, loss = 0.04571409\n",
      "Iteration 647, loss = 0.04562473\n",
      "Iteration 648, loss = 0.04533974\n",
      "Iteration 649, loss = 0.04488870\n",
      "Iteration 650, loss = 0.04460271\n",
      "Iteration 651, loss = 0.04432709\n",
      "Iteration 652, loss = 0.04476847\n",
      "Iteration 653, loss = 0.04394053\n",
      "Iteration 654, loss = 0.04368003\n",
      "Iteration 655, loss = 0.04250295\n",
      "Iteration 656, loss = 0.04233854\n",
      "Iteration 657, loss = 0.04298873\n",
      "Iteration 658, loss = 0.04174722\n",
      "Iteration 659, loss = 0.04160242\n",
      "Iteration 660, loss = 0.04121282\n",
      "Iteration 661, loss = 0.04068100\n",
      "Iteration 662, loss = 0.03905309\n",
      "Iteration 663, loss = 0.03900769\n",
      "Iteration 664, loss = 0.03963049\n",
      "Iteration 665, loss = 0.03961100\n",
      "Iteration 666, loss = 0.03803792\n",
      "Iteration 667, loss = 0.03809155\n",
      "Iteration 668, loss = 0.03802833\n",
      "Iteration 669, loss = 0.03763489\n",
      "Iteration 670, loss = 0.03688057\n",
      "Iteration 671, loss = 0.03936742\n",
      "Iteration 672, loss = 0.03730604\n",
      "Iteration 673, loss = 0.03562456\n",
      "Iteration 674, loss = 0.03505467\n",
      "Iteration 675, loss = 0.03522138\n",
      "Iteration 676, loss = 0.03428673\n",
      "Iteration 677, loss = 0.03528702\n",
      "Iteration 678, loss = 0.03460421\n",
      "Iteration 679, loss = 0.03356173\n",
      "Iteration 680, loss = 0.03370416\n",
      "Iteration 681, loss = 0.03348206\n",
      "Iteration 682, loss = 0.03281049\n",
      "Iteration 683, loss = 0.03445025\n",
      "Iteration 684, loss = 0.03170284\n",
      "Iteration 685, loss = 0.03149739\n",
      "Iteration 686, loss = 0.03153194\n",
      "Iteration 687, loss = 0.03122877\n",
      "Iteration 688, loss = 0.03117556\n",
      "Iteration 689, loss = 0.03124506\n",
      "Iteration 690, loss = 0.03020659\n",
      "Iteration 691, loss = 0.03019315\n",
      "Iteration 692, loss = 0.02943991\n",
      "Iteration 693, loss = 0.02910516\n",
      "Iteration 694, loss = 0.03011446\n",
      "Iteration 695, loss = 0.02913155\n",
      "Iteration 696, loss = 0.02920917\n",
      "Iteration 697, loss = 0.02936322\n",
      "Iteration 698, loss = 0.02861850\n",
      "Iteration 699, loss = 0.02820743\n",
      "Iteration 700, loss = 0.02752032\n",
      "Iteration 701, loss = 0.02788149\n",
      "Iteration 702, loss = 0.02748084\n",
      "Iteration 703, loss = 0.02714650\n",
      "Iteration 704, loss = 0.02719591\n",
      "Iteration 705, loss = 0.02649909\n",
      "Iteration 706, loss = 0.02620419\n",
      "Iteration 707, loss = 0.02610075\n",
      "Iteration 708, loss = 0.02587415\n",
      "Iteration 709, loss = 0.02500777\n",
      "Iteration 710, loss = 0.02485749\n",
      "Iteration 711, loss = 0.02453597\n",
      "Iteration 712, loss = 0.02597970\n",
      "Iteration 713, loss = 0.02424085\n",
      "Iteration 714, loss = 0.02627618\n",
      "Iteration 715, loss = 0.02506176\n",
      "Iteration 716, loss = 0.02469643\n",
      "Iteration 717, loss = 0.02412129\n",
      "Iteration 718, loss = 0.02306240\n",
      "Iteration 719, loss = 0.02322300\n",
      "Iteration 720, loss = 0.02296822\n",
      "Iteration 721, loss = 0.02304122\n",
      "Iteration 722, loss = 0.02303936\n",
      "Iteration 723, loss = 0.02294816\n",
      "Iteration 724, loss = 0.02184473\n",
      "Iteration 725, loss = 0.02204441\n",
      "Iteration 726, loss = 0.02123721\n",
      "Iteration 727, loss = 0.02099704\n",
      "Iteration 728, loss = 0.02106644\n",
      "Iteration 729, loss = 0.02282933\n",
      "Iteration 730, loss = 0.02351868\n",
      "Iteration 731, loss = 0.03039394\n",
      "Iteration 732, loss = 0.02420826\n",
      "Iteration 733, loss = 0.02040312\n",
      "Iteration 734, loss = 0.01978750\n",
      "Iteration 735, loss = 0.01954225\n",
      "Iteration 736, loss = 0.01947108\n",
      "Iteration 737, loss = 0.01929414\n",
      "Iteration 738, loss = 0.01926171\n",
      "Iteration 739, loss = 0.01883367\n",
      "Iteration 740, loss = 0.01866786\n",
      "Iteration 741, loss = 0.01905776\n",
      "Iteration 742, loss = 0.01917452\n",
      "Iteration 743, loss = 0.01905424\n",
      "Iteration 744, loss = 0.01835009\n",
      "Iteration 745, loss = 0.01823001\n",
      "Iteration 746, loss = 0.01827890\n",
      "Iteration 747, loss = 0.01810106\n",
      "Iteration 748, loss = 0.01880166\n",
      "Iteration 749, loss = 0.01769995\n",
      "Iteration 750, loss = 0.01746771\n",
      "Iteration 751, loss = 0.01760416\n",
      "Iteration 752, loss = 0.01806974\n",
      "Iteration 753, loss = 0.01720182\n",
      "Iteration 754, loss = 0.01653354\n",
      "Iteration 755, loss = 0.01660382\n",
      "Iteration 756, loss = 0.01635873\n",
      "Iteration 757, loss = 0.01672925\n",
      "Iteration 758, loss = 0.01626537\n",
      "Iteration 759, loss = 0.01588531\n",
      "Iteration 760, loss = 0.01573934\n",
      "Iteration 761, loss = 0.01661315\n",
      "Iteration 762, loss = 0.01714687\n",
      "Iteration 763, loss = 0.01574673\n",
      "Iteration 764, loss = 0.01639783\n",
      "Iteration 765, loss = 0.01553069\n",
      "Iteration 766, loss = 0.01557182\n",
      "Iteration 767, loss = 0.01557196\n",
      "Iteration 768, loss = 0.01492007\n",
      "Iteration 769, loss = 0.01578773\n",
      "Iteration 770, loss = 0.01544797\n",
      "Iteration 771, loss = 0.01555411\n",
      "Iteration 772, loss = 0.01477364\n",
      "Iteration 773, loss = 0.01424916\n",
      "Iteration 774, loss = 0.01427089\n",
      "Iteration 775, loss = 0.01414943\n",
      "Iteration 776, loss = 0.01410053\n",
      "Iteration 777, loss = 0.01360355\n",
      "Iteration 778, loss = 0.01356465\n",
      "Iteration 779, loss = 0.01354065\n",
      "Iteration 780, loss = 0.01403762\n",
      "Iteration 781, loss = 0.01785750\n",
      "Iteration 782, loss = 0.02062980\n",
      "Iteration 783, loss = 0.01656650\n",
      "Iteration 784, loss = 0.01502068\n",
      "Iteration 785, loss = 0.01308646\n",
      "Iteration 786, loss = 0.01253221\n",
      "Iteration 787, loss = 0.01272330\n",
      "Iteration 788, loss = 0.01239742\n",
      "Iteration 789, loss = 0.01208954\n",
      "Iteration 790, loss = 0.01262198\n",
      "Iteration 791, loss = 0.01178418\n",
      "Iteration 792, loss = 0.01227713\n",
      "Iteration 793, loss = 0.01194203\n",
      "Iteration 794, loss = 0.01152175\n",
      "Iteration 795, loss = 0.01126004\n",
      "Iteration 796, loss = 0.01129442\n",
      "Iteration 797, loss = 0.01155662\n",
      "Iteration 798, loss = 0.01148664\n",
      "Iteration 799, loss = 0.01118223\n",
      "Iteration 800, loss = 0.01211546\n",
      "Iteration 801, loss = 0.01144410\n",
      "Iteration 802, loss = 0.01137616\n",
      "Iteration 803, loss = 0.01114773\n",
      "Iteration 804, loss = 0.01110607\n",
      "Iteration 805, loss = 0.01172188\n",
      "Iteration 806, loss = 0.01119920\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x, y, and format string must not be None",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[136], line 67\u001B[0m\n\u001B[0;32m     65\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m6\u001B[39m))\n\u001B[0;32m     66\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(monitor_clf\u001B[38;5;241m.\u001B[39mtrain_scores_, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTraining Score\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 67\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(monitor_clf\u001B[38;5;241m.\u001B[39mvalidation_scores_, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mValidation Score\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     68\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIterations\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     69\u001B[0m plt\u001B[38;5;241m.\u001B[39mylabel(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mScore\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mD:\\Anaconda3\\Lib\\site-packages\\matplotlib\\pyplot.py:3578\u001B[0m, in \u001B[0;36mplot\u001B[1;34m(scalex, scaley, data, *args, **kwargs)\u001B[0m\n\u001B[0;32m   3570\u001B[0m \u001B[38;5;129m@_copy_docstring_and_deprecators\u001B[39m(Axes\u001B[38;5;241m.\u001B[39mplot)\n\u001B[0;32m   3571\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplot\u001B[39m(\n\u001B[0;32m   3572\u001B[0m     \u001B[38;5;241m*\u001B[39margs: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m ArrayLike \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3576\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3577\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[Line2D]:\n\u001B[1;32m-> 3578\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m gca()\u001B[38;5;241m.\u001B[39mplot(\n\u001B[0;32m   3579\u001B[0m         \u001B[38;5;241m*\u001B[39margs,\n\u001B[0;32m   3580\u001B[0m         scalex\u001B[38;5;241m=\u001B[39mscalex,\n\u001B[0;32m   3581\u001B[0m         scaley\u001B[38;5;241m=\u001B[39mscaley,\n\u001B[0;32m   3582\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m: data} \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m {}),\n\u001B[0;32m   3583\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3584\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1721\u001B[0m, in \u001B[0;36mAxes.plot\u001B[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1478\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1479\u001B[0m \u001B[38;5;124;03mPlot y versus x as lines and/or markers.\u001B[39;00m\n\u001B[0;32m   1480\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1718\u001B[0m \u001B[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001B[39;00m\n\u001B[0;32m   1719\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1720\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m cbook\u001B[38;5;241m.\u001B[39mnormalize_kwargs(kwargs, mlines\u001B[38;5;241m.\u001B[39mLine2D)\n\u001B[1;32m-> 1721\u001B[0m lines \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_lines(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, data\u001B[38;5;241m=\u001B[39mdata, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)]\n\u001B[0;32m   1722\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m lines:\n\u001B[0;32m   1723\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_line(line)\n",
      "File \u001B[1;32mD:\\Anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:303\u001B[0m, in \u001B[0;36m_process_plot_var_args.__call__\u001B[1;34m(self, axes, data, *args, **kwargs)\u001B[0m\n\u001B[0;32m    301\u001B[0m     this \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m    302\u001B[0m     args \u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m1\u001B[39m:]\n\u001B[1;32m--> 303\u001B[0m \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plot_args(\n\u001B[0;32m    304\u001B[0m     axes, this, kwargs, ambiguous_fmt_datakey\u001B[38;5;241m=\u001B[39mambiguous_fmt_datakey)\n",
      "File \u001B[1;32mD:\\Anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:460\u001B[0m, in \u001B[0;36m_process_plot_var_args._plot_args\u001B[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001B[0m\n\u001B[0;32m    457\u001B[0m \u001B[38;5;66;03m# Don't allow any None value; these would be up-converted to one\u001B[39;00m\n\u001B[0;32m    458\u001B[0m \u001B[38;5;66;03m# element array of None which causes problems downstream.\u001B[39;00m\n\u001B[0;32m    459\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(v \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m tup):\n\u001B[1;32m--> 460\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx, y, and format string must not be None\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    462\u001B[0m kw \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    463\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m prop_name, val \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m((\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlinestyle\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmarker\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcolor\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[0;32m    464\u001B[0m                           (linestyle, marker, color)):\n",
      "\u001B[1;31mValueError\u001B[0m: x, y, and format string must not be None"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T05:36:54.751638Z",
     "start_time": "2025-02-04T05:36:54.599035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, monitor_clf.predict(xTr))\n"
   ],
   "id": "bcc996a324bbf3bd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9995523724261415"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T05:36:56.501168Z",
     "start_time": "2025-02-04T05:36:56.425439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# testing score\n",
    "accuracy_score(yTe.Result.values, monitor_clf.predict(xTe))"
   ],
   "id": "f2166d80432cbe0f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4344344344344344"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 138
  },
  {
   "cell_type": "code",
   "id": "d0505f91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T05:39:53.207502Z",
     "start_time": "2025-02-04T05:36:58.680262Z"
    }
   },
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(512,128,32),\n",
    "#                     activation='relu',\n",
    "#                     batch_size=512,\n",
    "#                     max_iter=10000,\n",
    "#                     learning_rate_init=1e-4,\n",
    "#                     early_stopping=True,\n",
    "#                     alpha=1e-3,\n",
    "#                    ).fit(xTr, yTr.values.ravel())\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(256,128,32),\n",
    "                    activation='relu',\n",
    "                    batch_size=512,\n",
    "                    max_iter=10000,\n",
    "                    learning_rate_init=1e-4,\n",
    "                    early_stopping=False,\n",
    "                    alpha=4e-1,\n",
    "                    verbose=True,\n",
    "                    tol=1e-4,\n",
    "                    random_state=13,\n",
    "                    learning_rate='adaptive',\n",
    "                    n_iter_no_change=10\n",
    "                   ).fit(xTr, yTr.values.ravel())\n",
    "mlp"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.29084881\n",
      "Iteration 2, loss = 1.23276688\n",
      "Iteration 3, loss = 1.20295836\n",
      "Iteration 4, loss = 1.18639847\n",
      "Iteration 5, loss = 1.17621765\n",
      "Iteration 6, loss = 1.16786812\n",
      "Iteration 7, loss = 1.16036627\n",
      "Iteration 8, loss = 1.15446902\n",
      "Iteration 9, loss = 1.14793161\n",
      "Iteration 10, loss = 1.14202886\n",
      "Iteration 11, loss = 1.13669942\n",
      "Iteration 12, loss = 1.13169831\n",
      "Iteration 13, loss = 1.12712690\n",
      "Iteration 14, loss = 1.12260015\n",
      "Iteration 15, loss = 1.11734331\n",
      "Iteration 16, loss = 1.11280242\n",
      "Iteration 17, loss = 1.10882528\n",
      "Iteration 18, loss = 1.10425953\n",
      "Iteration 19, loss = 1.10057407\n",
      "Iteration 20, loss = 1.09679473\n",
      "Iteration 21, loss = 1.09270228\n",
      "Iteration 22, loss = 1.08916697\n",
      "Iteration 23, loss = 1.08617631\n",
      "Iteration 24, loss = 1.08368252\n",
      "Iteration 25, loss = 1.07911685\n",
      "Iteration 26, loss = 1.07627040\n",
      "Iteration 27, loss = 1.07416420\n",
      "Iteration 28, loss = 1.06964682\n",
      "Iteration 29, loss = 1.06673172\n",
      "Iteration 30, loss = 1.06453443\n",
      "Iteration 31, loss = 1.06187085\n",
      "Iteration 32, loss = 1.05846918\n",
      "Iteration 33, loss = 1.05575959\n",
      "Iteration 34, loss = 1.05374247\n",
      "Iteration 35, loss = 1.05033849\n",
      "Iteration 36, loss = 1.04810847\n",
      "Iteration 37, loss = 1.04564931\n",
      "Iteration 38, loss = 1.04432851\n",
      "Iteration 39, loss = 1.03996215\n",
      "Iteration 40, loss = 1.03812077\n",
      "Iteration 41, loss = 1.03590139\n",
      "Iteration 42, loss = 1.03375172\n",
      "Iteration 43, loss = 1.03118884\n",
      "Iteration 44, loss = 1.02931345\n",
      "Iteration 45, loss = 1.02777783\n",
      "Iteration 46, loss = 1.02483759\n",
      "Iteration 47, loss = 1.02197430\n",
      "Iteration 48, loss = 1.02007799\n",
      "Iteration 49, loss = 1.01843598\n",
      "Iteration 50, loss = 1.01610947\n",
      "Iteration 51, loss = 1.01352954\n",
      "Iteration 52, loss = 1.01142085\n",
      "Iteration 53, loss = 1.00876522\n",
      "Iteration 54, loss = 1.00594474\n",
      "Iteration 55, loss = 1.00448937\n",
      "Iteration 56, loss = 1.00151780\n",
      "Iteration 57, loss = 0.99888577\n",
      "Iteration 58, loss = 0.99716765\n",
      "Iteration 59, loss = 0.99430368\n",
      "Iteration 60, loss = 0.99090023\n",
      "Iteration 61, loss = 0.98808363\n",
      "Iteration 62, loss = 0.98561893\n",
      "Iteration 63, loss = 0.98420517\n",
      "Iteration 64, loss = 0.98195488\n",
      "Iteration 65, loss = 0.97899678\n",
      "Iteration 66, loss = 0.97628061\n",
      "Iteration 67, loss = 0.97465290\n",
      "Iteration 68, loss = 0.97178101\n",
      "Iteration 69, loss = 0.97004435\n",
      "Iteration 70, loss = 0.96765693\n",
      "Iteration 71, loss = 0.96409417\n",
      "Iteration 72, loss = 0.96161988\n",
      "Iteration 73, loss = 0.95937879\n",
      "Iteration 74, loss = 0.95814888\n",
      "Iteration 75, loss = 0.95389907\n",
      "Iteration 76, loss = 0.95110123\n",
      "Iteration 77, loss = 0.94967598\n",
      "Iteration 78, loss = 0.94615438\n",
      "Iteration 79, loss = 0.94357314\n",
      "Iteration 80, loss = 0.94076944\n",
      "Iteration 81, loss = 0.93797609\n",
      "Iteration 82, loss = 0.93593246\n",
      "Iteration 83, loss = 0.93264997\n",
      "Iteration 84, loss = 0.93115809\n",
      "Iteration 85, loss = 0.92773123\n",
      "Iteration 86, loss = 0.92399917\n",
      "Iteration 87, loss = 0.92145380\n",
      "Iteration 88, loss = 0.91864843\n",
      "Iteration 89, loss = 0.91455626\n",
      "Iteration 90, loss = 0.91183146\n",
      "Iteration 91, loss = 0.90778731\n",
      "Iteration 92, loss = 0.90714294\n",
      "Iteration 93, loss = 0.90191727\n",
      "Iteration 94, loss = 0.89908362\n",
      "Iteration 95, loss = 0.89688174\n",
      "Iteration 96, loss = 0.89308040\n",
      "Iteration 97, loss = 0.88814851\n",
      "Iteration 98, loss = 0.88437741\n",
      "Iteration 99, loss = 0.88032092\n",
      "Iteration 100, loss = 0.87738411\n",
      "Iteration 101, loss = 0.87358779\n",
      "Iteration 102, loss = 0.86908176\n",
      "Iteration 103, loss = 0.86619660\n",
      "Iteration 104, loss = 0.86262465\n",
      "Iteration 105, loss = 0.85742237\n",
      "Iteration 106, loss = 0.85411328\n",
      "Iteration 107, loss = 0.85164533\n",
      "Iteration 108, loss = 0.84785719\n",
      "Iteration 109, loss = 0.84418075\n",
      "Iteration 110, loss = 0.83871651\n",
      "Iteration 111, loss = 0.83684310\n",
      "Iteration 112, loss = 0.83448819\n",
      "Iteration 113, loss = 0.82980971\n",
      "Iteration 114, loss = 0.82332878\n",
      "Iteration 115, loss = 0.81891718\n",
      "Iteration 116, loss = 0.81520948\n",
      "Iteration 117, loss = 0.81290342\n",
      "Iteration 118, loss = 0.80710805\n",
      "Iteration 119, loss = 0.80284048\n",
      "Iteration 120, loss = 0.80033639\n",
      "Iteration 121, loss = 0.79568664\n",
      "Iteration 122, loss = 0.78873180\n",
      "Iteration 123, loss = 0.78599866\n",
      "Iteration 124, loss = 0.78126579\n",
      "Iteration 125, loss = 0.77587631\n",
      "Iteration 126, loss = 0.77226168\n",
      "Iteration 127, loss = 0.76841815\n",
      "Iteration 128, loss = 0.76520919\n",
      "Iteration 129, loss = 0.76401490\n",
      "Iteration 130, loss = 0.75577555\n",
      "Iteration 131, loss = 0.75133248\n",
      "Iteration 132, loss = 0.74652351\n",
      "Iteration 133, loss = 0.74433636\n",
      "Iteration 134, loss = 0.74233244\n",
      "Iteration 135, loss = 0.73407825\n",
      "Iteration 136, loss = 0.73012897\n",
      "Iteration 137, loss = 0.72471937\n",
      "Iteration 138, loss = 0.72182002\n",
      "Iteration 139, loss = 0.71732734\n",
      "Iteration 140, loss = 0.71193188\n",
      "Iteration 141, loss = 0.70930416\n",
      "Iteration 142, loss = 0.70466178\n",
      "Iteration 143, loss = 0.70090666\n",
      "Iteration 144, loss = 0.69630771\n",
      "Iteration 145, loss = 0.69066967\n",
      "Iteration 146, loss = 0.68628180\n",
      "Iteration 147, loss = 0.68360940\n",
      "Iteration 148, loss = 0.67893930\n",
      "Iteration 149, loss = 0.67456785\n",
      "Iteration 150, loss = 0.67049940\n",
      "Iteration 151, loss = 0.66470300\n",
      "Iteration 152, loss = 0.66036617\n",
      "Iteration 153, loss = 0.65521321\n",
      "Iteration 154, loss = 0.65210588\n",
      "Iteration 155, loss = 0.64912916\n",
      "Iteration 156, loss = 0.64568003\n",
      "Iteration 157, loss = 0.63980315\n",
      "Iteration 158, loss = 0.63487811\n",
      "Iteration 159, loss = 0.63111532\n",
      "Iteration 160, loss = 0.62635059\n",
      "Iteration 161, loss = 0.62313647\n",
      "Iteration 162, loss = 0.62026245\n",
      "Iteration 163, loss = 0.61620537\n",
      "Iteration 164, loss = 0.61224157\n",
      "Iteration 165, loss = 0.60940857\n",
      "Iteration 166, loss = 0.60309683\n",
      "Iteration 167, loss = 0.59899008\n",
      "Iteration 168, loss = 0.59463611\n",
      "Iteration 169, loss = 0.58926378\n",
      "Iteration 170, loss = 0.58834023\n",
      "Iteration 171, loss = 0.58527844\n",
      "Iteration 172, loss = 0.58066209\n",
      "Iteration 173, loss = 0.57478580\n",
      "Iteration 174, loss = 0.57242772\n",
      "Iteration 175, loss = 0.56787334\n",
      "Iteration 176, loss = 0.56353600\n",
      "Iteration 177, loss = 0.56133073\n",
      "Iteration 178, loss = 0.55552626\n",
      "Iteration 179, loss = 0.55257540\n",
      "Iteration 180, loss = 0.55039810\n",
      "Iteration 181, loss = 0.54834563\n",
      "Iteration 182, loss = 0.54174896\n",
      "Iteration 183, loss = 0.53988455\n",
      "Iteration 184, loss = 0.53603204\n",
      "Iteration 185, loss = 0.53051569\n",
      "Iteration 186, loss = 0.52734572\n",
      "Iteration 187, loss = 0.52695369\n",
      "Iteration 188, loss = 0.52131244\n",
      "Iteration 189, loss = 0.51875577\n",
      "Iteration 190, loss = 0.51479702\n",
      "Iteration 191, loss = 0.51741873\n",
      "Iteration 192, loss = 0.50803821\n",
      "Iteration 193, loss = 0.50257049\n",
      "Iteration 194, loss = 0.50112452\n",
      "Iteration 195, loss = 0.49778902\n",
      "Iteration 196, loss = 0.49648643\n",
      "Iteration 197, loss = 0.49065845\n",
      "Iteration 198, loss = 0.48854942\n",
      "Iteration 199, loss = 0.48675740\n",
      "Iteration 200, loss = 0.48137272\n",
      "Iteration 201, loss = 0.47679721\n",
      "Iteration 202, loss = 0.47500411\n",
      "Iteration 203, loss = 0.47132086\n",
      "Iteration 204, loss = 0.46877827\n",
      "Iteration 205, loss = 0.46530185\n",
      "Iteration 206, loss = 0.46315932\n",
      "Iteration 207, loss = 0.46056311\n",
      "Iteration 208, loss = 0.45888474\n",
      "Iteration 209, loss = 0.45428022\n",
      "Iteration 210, loss = 0.45050627\n",
      "Iteration 211, loss = 0.44948304\n",
      "Iteration 212, loss = 0.44664063\n",
      "Iteration 213, loss = 0.44184093\n",
      "Iteration 214, loss = 0.43885406\n",
      "Iteration 215, loss = 0.43598413\n",
      "Iteration 216, loss = 0.43391054\n",
      "Iteration 217, loss = 0.43155898\n",
      "Iteration 218, loss = 0.43435379\n",
      "Iteration 219, loss = 0.42730385\n",
      "Iteration 220, loss = 0.42602505\n",
      "Iteration 221, loss = 0.42066888\n",
      "Iteration 222, loss = 0.41682905\n",
      "Iteration 223, loss = 0.41473336\n",
      "Iteration 224, loss = 0.41161463\n",
      "Iteration 225, loss = 0.40876948\n",
      "Iteration 226, loss = 0.40743918\n",
      "Iteration 227, loss = 0.40566557\n",
      "Iteration 228, loss = 0.40217526\n",
      "Iteration 229, loss = 0.40045757\n",
      "Iteration 230, loss = 0.39843613\n",
      "Iteration 231, loss = 0.39391745\n",
      "Iteration 232, loss = 0.39214062\n",
      "Iteration 233, loss = 0.39117667\n",
      "Iteration 234, loss = 0.39020312\n",
      "Iteration 235, loss = 0.38574100\n",
      "Iteration 236, loss = 0.38595735\n",
      "Iteration 237, loss = 0.38202851\n",
      "Iteration 238, loss = 0.37864051\n",
      "Iteration 239, loss = 0.37597610\n",
      "Iteration 240, loss = 0.37312235\n",
      "Iteration 241, loss = 0.37397591\n",
      "Iteration 242, loss = 0.37151797\n",
      "Iteration 243, loss = 0.36709036\n",
      "Iteration 244, loss = 0.36396790\n",
      "Iteration 245, loss = 0.36384323\n",
      "Iteration 246, loss = 0.36149099\n",
      "Iteration 247, loss = 0.35925928\n",
      "Iteration 248, loss = 0.35649094\n",
      "Iteration 249, loss = 0.35500048\n",
      "Iteration 250, loss = 0.35279869\n",
      "Iteration 251, loss = 0.34975616\n",
      "Iteration 252, loss = 0.34803966\n",
      "Iteration 253, loss = 0.34566398\n",
      "Iteration 254, loss = 0.34426124\n",
      "Iteration 255, loss = 0.34169605\n",
      "Iteration 256, loss = 0.34104322\n",
      "Iteration 257, loss = 0.33754440\n",
      "Iteration 258, loss = 0.33686273\n",
      "Iteration 259, loss = 0.33554857\n",
      "Iteration 260, loss = 0.33342832\n",
      "Iteration 261, loss = 0.33125698\n",
      "Iteration 262, loss = 0.32918550\n",
      "Iteration 263, loss = 0.32822560\n",
      "Iteration 264, loss = 0.32729603\n",
      "Iteration 265, loss = 0.32419511\n",
      "Iteration 266, loss = 0.32307486\n",
      "Iteration 267, loss = 0.32246772\n",
      "Iteration 268, loss = 0.31985482\n",
      "Iteration 269, loss = 0.31707473\n",
      "Iteration 270, loss = 0.31562668\n",
      "Iteration 271, loss = 0.31836172\n",
      "Iteration 272, loss = 0.31459706\n",
      "Iteration 273, loss = 0.31164635\n",
      "Iteration 274, loss = 0.30984031\n",
      "Iteration 275, loss = 0.30942339\n",
      "Iteration 276, loss = 0.30817925\n",
      "Iteration 277, loss = 0.30352740\n",
      "Iteration 278, loss = 0.30200229\n",
      "Iteration 279, loss = 0.30030965\n",
      "Iteration 280, loss = 0.29899233\n",
      "Iteration 281, loss = 0.29723647\n",
      "Iteration 282, loss = 0.29699338\n",
      "Iteration 283, loss = 0.29572562\n",
      "Iteration 284, loss = 0.29698417\n",
      "Iteration 285, loss = 0.29555011\n",
      "Iteration 286, loss = 0.29059204\n",
      "Iteration 287, loss = 0.28904879\n",
      "Iteration 288, loss = 0.28839900\n",
      "Iteration 289, loss = 0.28686813\n",
      "Iteration 290, loss = 0.28590409\n",
      "Iteration 291, loss = 0.28524214\n",
      "Iteration 292, loss = 0.28318867\n",
      "Iteration 293, loss = 0.28286686\n",
      "Iteration 294, loss = 0.28086683\n",
      "Iteration 295, loss = 0.27841296\n",
      "Iteration 296, loss = 0.27668829\n",
      "Iteration 297, loss = 0.27602257\n",
      "Iteration 298, loss = 0.27672339\n",
      "Iteration 299, loss = 0.27665135\n",
      "Iteration 300, loss = 0.27365154\n",
      "Iteration 301, loss = 0.27230950\n",
      "Iteration 302, loss = 0.27100883\n",
      "Iteration 303, loss = 0.27051863\n",
      "Iteration 304, loss = 0.26774743\n",
      "Iteration 305, loss = 0.26708414\n",
      "Iteration 306, loss = 0.26471640\n",
      "Iteration 307, loss = 0.26357314\n",
      "Iteration 308, loss = 0.26430687\n",
      "Iteration 309, loss = 0.26304908\n",
      "Iteration 310, loss = 0.26326611\n",
      "Iteration 311, loss = 0.25966895\n",
      "Iteration 312, loss = 0.25872390\n",
      "Iteration 313, loss = 0.25766602\n",
      "Iteration 314, loss = 0.25697641\n",
      "Iteration 315, loss = 0.25806298\n",
      "Iteration 316, loss = 0.25685340\n",
      "Iteration 317, loss = 0.25599436\n",
      "Iteration 318, loss = 0.25318688\n",
      "Iteration 319, loss = 0.25175449\n",
      "Iteration 320, loss = 0.25060276\n",
      "Iteration 321, loss = 0.25051772\n",
      "Iteration 322, loss = 0.24927946\n",
      "Iteration 323, loss = 0.24926710\n",
      "Iteration 324, loss = 0.24831867\n",
      "Iteration 325, loss = 0.24654694\n",
      "Iteration 326, loss = 0.24616619\n",
      "Iteration 327, loss = 0.24454273\n",
      "Iteration 328, loss = 0.24434481\n",
      "Iteration 329, loss = 0.24388197\n",
      "Iteration 330, loss = 0.24210425\n",
      "Iteration 331, loss = 0.24111349\n",
      "Iteration 332, loss = 0.23996542\n",
      "Iteration 333, loss = 0.24023553\n",
      "Iteration 334, loss = 0.23918926\n",
      "Iteration 335, loss = 0.23786498\n",
      "Iteration 336, loss = 0.23716365\n",
      "Iteration 337, loss = 0.23591384\n",
      "Iteration 338, loss = 0.23596157\n",
      "Iteration 339, loss = 0.23481626\n",
      "Iteration 340, loss = 0.23371276\n",
      "Iteration 341, loss = 0.23326010\n",
      "Iteration 342, loss = 0.23373254\n",
      "Iteration 343, loss = 0.23362457\n",
      "Iteration 344, loss = 0.23163457\n",
      "Iteration 345, loss = 0.22984557\n",
      "Iteration 346, loss = 0.22910499\n",
      "Iteration 347, loss = 0.22905743\n",
      "Iteration 348, loss = 0.23019728\n",
      "Iteration 349, loss = 0.22755762\n",
      "Iteration 350, loss = 0.22738395\n",
      "Iteration 351, loss = 0.22621067\n",
      "Iteration 352, loss = 0.22734145\n",
      "Iteration 353, loss = 0.22693562\n",
      "Iteration 354, loss = 0.22601183\n",
      "Iteration 355, loss = 0.22482188\n",
      "Iteration 356, loss = 0.22348526\n",
      "Iteration 357, loss = 0.22311629\n",
      "Iteration 358, loss = 0.22241825\n",
      "Iteration 359, loss = 0.22098460\n",
      "Iteration 360, loss = 0.22090248\n",
      "Iteration 361, loss = 0.22087276\n",
      "Iteration 362, loss = 0.22055052\n",
      "Iteration 363, loss = 0.21905456\n",
      "Iteration 364, loss = 0.21838083\n",
      "Iteration 365, loss = 0.21828365\n",
      "Iteration 366, loss = 0.21813586\n",
      "Iteration 367, loss = 0.21742691\n",
      "Iteration 368, loss = 0.21691267\n",
      "Iteration 369, loss = 0.21556525\n",
      "Iteration 370, loss = 0.21575724\n",
      "Iteration 371, loss = 0.21583606\n",
      "Iteration 372, loss = 0.21424296\n",
      "Iteration 373, loss = 0.21297649\n",
      "Iteration 374, loss = 0.21294202\n",
      "Iteration 375, loss = 0.21177734\n",
      "Iteration 376, loss = 0.21279731\n",
      "Iteration 377, loss = 0.21166192\n",
      "Iteration 378, loss = 0.21156613\n",
      "Iteration 379, loss = 0.21074188\n",
      "Iteration 380, loss = 0.21046788\n",
      "Iteration 381, loss = 0.20933891\n",
      "Iteration 382, loss = 0.20873284\n",
      "Iteration 383, loss = 0.20909305\n",
      "Iteration 384, loss = 0.20920739\n",
      "Iteration 385, loss = 0.20799359\n",
      "Iteration 386, loss = 0.20686086\n",
      "Iteration 387, loss = 0.20642070\n",
      "Iteration 388, loss = 0.20640699\n",
      "Iteration 389, loss = 0.20681321\n",
      "Iteration 390, loss = 0.20526053\n",
      "Iteration 391, loss = 0.20522769\n",
      "Iteration 392, loss = 0.20514252\n",
      "Iteration 393, loss = 0.20487495\n",
      "Iteration 394, loss = 0.20374496\n",
      "Iteration 395, loss = 0.20292267\n",
      "Iteration 396, loss = 0.20334430\n",
      "Iteration 397, loss = 0.20244314\n",
      "Iteration 398, loss = 0.20147311\n",
      "Iteration 399, loss = 0.20097349\n",
      "Iteration 400, loss = 0.20170154\n",
      "Iteration 401, loss = 0.20071692\n",
      "Iteration 402, loss = 0.20043085\n",
      "Iteration 403, loss = 0.19963600\n",
      "Iteration 404, loss = 0.19949929\n",
      "Iteration 405, loss = 0.19915454\n",
      "Iteration 406, loss = 0.19912734\n",
      "Iteration 407, loss = 0.19822431\n",
      "Iteration 408, loss = 0.19822041\n",
      "Iteration 409, loss = 0.19799643\n",
      "Iteration 410, loss = 0.19816996\n",
      "Iteration 411, loss = 0.19707099\n",
      "Iteration 412, loss = 0.19643882\n",
      "Iteration 413, loss = 0.19582431\n",
      "Iteration 414, loss = 0.19664827\n",
      "Iteration 415, loss = 0.19547117\n",
      "Iteration 416, loss = 0.19540443\n",
      "Iteration 417, loss = 0.19464270\n",
      "Iteration 418, loss = 0.19416681\n",
      "Iteration 419, loss = 0.19400786\n",
      "Iteration 420, loss = 0.19352191\n",
      "Iteration 421, loss = 0.19467676\n",
      "Iteration 422, loss = 0.19386552\n",
      "Iteration 423, loss = 0.19328230\n",
      "Iteration 424, loss = 0.19202986\n",
      "Iteration 425, loss = 0.19202092\n",
      "Iteration 426, loss = 0.19176943\n",
      "Iteration 427, loss = 0.19159815\n",
      "Iteration 428, loss = 0.19148887\n",
      "Iteration 429, loss = 0.19090744\n",
      "Iteration 430, loss = 0.19048981\n",
      "Iteration 431, loss = 0.18989929\n",
      "Iteration 432, loss = 0.19011984\n",
      "Iteration 433, loss = 0.18981792\n",
      "Iteration 434, loss = 0.18915333\n",
      "Iteration 435, loss = 0.18910020\n",
      "Iteration 436, loss = 0.18863462\n",
      "Iteration 437, loss = 0.18821584\n",
      "Iteration 438, loss = 0.18818049\n",
      "Iteration 439, loss = 0.18775022\n",
      "Iteration 440, loss = 0.18733006\n",
      "Iteration 441, loss = 0.18734161\n",
      "Iteration 442, loss = 0.18747054\n",
      "Iteration 443, loss = 0.18678768\n",
      "Iteration 444, loss = 0.18633927\n",
      "Iteration 445, loss = 0.18612210\n",
      "Iteration 446, loss = 0.18554180\n",
      "Iteration 447, loss = 0.18619384\n",
      "Iteration 448, loss = 0.18574702\n",
      "Iteration 449, loss = 0.18495237\n",
      "Iteration 450, loss = 0.18563689\n",
      "Iteration 451, loss = 0.18441295\n",
      "Iteration 452, loss = 0.18402821\n",
      "Iteration 453, loss = 0.18427869\n",
      "Iteration 454, loss = 0.18427741\n",
      "Iteration 455, loss = 0.18352000\n",
      "Iteration 456, loss = 0.18306817\n",
      "Iteration 457, loss = 0.18373290\n",
      "Iteration 458, loss = 0.18251635\n",
      "Iteration 459, loss = 0.18232050\n",
      "Iteration 460, loss = 0.18201915\n",
      "Iteration 461, loss = 0.18163622\n",
      "Iteration 462, loss = 0.18112261\n",
      "Iteration 463, loss = 0.18113616\n",
      "Iteration 464, loss = 0.18054249\n",
      "Iteration 465, loss = 0.18036925\n",
      "Iteration 466, loss = 0.18022357\n",
      "Iteration 467, loss = 0.17995339\n",
      "Iteration 468, loss = 0.18009664\n",
      "Iteration 469, loss = 0.18000325\n",
      "Iteration 470, loss = 0.18003098\n",
      "Iteration 471, loss = 0.17960949\n",
      "Iteration 472, loss = 0.17863428\n",
      "Iteration 473, loss = 0.17823710\n",
      "Iteration 474, loss = 0.17841285\n",
      "Iteration 475, loss = 0.17818125\n",
      "Iteration 476, loss = 0.17865010\n",
      "Iteration 477, loss = 0.17787490\n",
      "Iteration 478, loss = 0.17736019\n",
      "Iteration 479, loss = 0.17702752\n",
      "Iteration 480, loss = 0.17684745\n",
      "Iteration 481, loss = 0.17670946\n",
      "Iteration 482, loss = 0.17645097\n",
      "Iteration 483, loss = 0.17603320\n",
      "Iteration 484, loss = 0.17583672\n",
      "Iteration 485, loss = 0.17568674\n",
      "Iteration 486, loss = 0.17520521\n",
      "Iteration 487, loss = 0.17528203\n",
      "Iteration 488, loss = 0.17499653\n",
      "Iteration 489, loss = 0.17497960\n",
      "Iteration 490, loss = 0.17451131\n",
      "Iteration 491, loss = 0.17436483\n",
      "Iteration 492, loss = 0.17419847\n",
      "Iteration 493, loss = 0.17403492\n",
      "Iteration 494, loss = 0.17347505\n",
      "Iteration 495, loss = 0.17363872\n",
      "Iteration 496, loss = 0.17294872\n",
      "Iteration 497, loss = 0.17328719\n",
      "Iteration 498, loss = 0.17312321\n",
      "Iteration 499, loss = 0.17265077\n",
      "Iteration 500, loss = 0.17254425\n",
      "Iteration 501, loss = 0.17183650\n",
      "Iteration 502, loss = 0.17167993\n",
      "Iteration 503, loss = 0.17162959\n",
      "Iteration 504, loss = 0.17147504\n",
      "Iteration 505, loss = 0.17110245\n",
      "Iteration 506, loss = 0.17086413\n",
      "Iteration 507, loss = 0.17068876\n",
      "Iteration 508, loss = 0.17041416\n",
      "Iteration 509, loss = 0.17021129\n",
      "Iteration 510, loss = 0.16990155\n",
      "Iteration 511, loss = 0.17018022\n",
      "Iteration 512, loss = 0.16966592\n",
      "Iteration 513, loss = 0.17050479\n",
      "Iteration 514, loss = 0.16965809\n",
      "Iteration 515, loss = 0.16918340\n",
      "Iteration 516, loss = 0.16886506\n",
      "Iteration 517, loss = 0.16871157\n",
      "Iteration 518, loss = 0.16810147\n",
      "Iteration 519, loss = 0.16793413\n",
      "Iteration 520, loss = 0.16789379\n",
      "Iteration 521, loss = 0.16805272\n",
      "Iteration 522, loss = 0.16797742\n",
      "Iteration 523, loss = 0.16776682\n",
      "Iteration 524, loss = 0.16737866\n",
      "Iteration 525, loss = 0.16704842\n",
      "Iteration 526, loss = 0.16674317\n",
      "Iteration 527, loss = 0.16754923\n",
      "Iteration 528, loss = 0.16675191\n",
      "Iteration 529, loss = 0.16612989\n",
      "Iteration 530, loss = 0.16611372\n",
      "Iteration 531, loss = 0.16556937\n",
      "Iteration 532, loss = 0.16599595\n",
      "Iteration 533, loss = 0.16556929\n",
      "Iteration 534, loss = 0.16577639\n",
      "Iteration 535, loss = 0.16507609\n",
      "Iteration 536, loss = 0.16479619\n",
      "Iteration 537, loss = 0.16494921\n",
      "Iteration 538, loss = 0.16424129\n",
      "Iteration 539, loss = 0.16414868\n",
      "Iteration 540, loss = 0.16386641\n",
      "Iteration 541, loss = 0.16352377\n",
      "Iteration 542, loss = 0.16386527\n",
      "Iteration 543, loss = 0.16377562\n",
      "Iteration 544, loss = 0.16317908\n",
      "Iteration 545, loss = 0.16322804\n",
      "Iteration 546, loss = 0.16287109\n",
      "Iteration 547, loss = 0.16297249\n",
      "Iteration 548, loss = 0.16255873\n",
      "Iteration 549, loss = 0.16234351\n",
      "Iteration 550, loss = 0.16206857\n",
      "Iteration 551, loss = 0.16183932\n",
      "Iteration 552, loss = 0.16178553\n",
      "Iteration 553, loss = 0.16149835\n",
      "Iteration 554, loss = 0.16166027\n",
      "Iteration 555, loss = 0.16126061\n",
      "Iteration 556, loss = 0.16101926\n",
      "Iteration 557, loss = 0.16093923\n",
      "Iteration 558, loss = 0.16085494\n",
      "Iteration 559, loss = 0.16091926\n",
      "Iteration 560, loss = 0.16047816\n",
      "Iteration 561, loss = 0.16014646\n",
      "Iteration 562, loss = 0.16008969\n",
      "Iteration 563, loss = 0.16001141\n",
      "Iteration 564, loss = 0.15968763\n",
      "Iteration 565, loss = 0.15951940\n",
      "Iteration 566, loss = 0.15932786\n",
      "Iteration 567, loss = 0.15912708\n",
      "Iteration 568, loss = 0.15925615\n",
      "Iteration 569, loss = 0.15862606\n",
      "Iteration 570, loss = 0.15858521\n",
      "Iteration 571, loss = 0.15899154\n",
      "Iteration 572, loss = 0.15850254\n",
      "Iteration 573, loss = 0.15809996\n",
      "Iteration 574, loss = 0.15806881\n",
      "Iteration 575, loss = 0.15741733\n",
      "Iteration 576, loss = 0.15747958\n",
      "Iteration 577, loss = 0.15795099\n",
      "Iteration 578, loss = 0.15737800\n",
      "Iteration 579, loss = 0.15679731\n",
      "Iteration 580, loss = 0.15682015\n",
      "Iteration 581, loss = 0.15652687\n",
      "Iteration 582, loss = 0.15646308\n",
      "Iteration 583, loss = 0.15625454\n",
      "Iteration 584, loss = 0.15623091\n",
      "Iteration 585, loss = 0.15637448\n",
      "Iteration 586, loss = 0.15611333\n",
      "Iteration 587, loss = 0.15545066\n",
      "Iteration 588, loss = 0.15537293\n",
      "Iteration 589, loss = 0.15522952\n",
      "Iteration 590, loss = 0.15520112\n",
      "Iteration 591, loss = 0.15499662\n",
      "Iteration 592, loss = 0.15515961\n",
      "Iteration 593, loss = 0.15483160\n",
      "Iteration 594, loss = 0.15438441\n",
      "Iteration 595, loss = 0.15430383\n",
      "Iteration 596, loss = 0.15417913\n",
      "Iteration 597, loss = 0.15414787\n",
      "Iteration 598, loss = 0.15388748\n",
      "Iteration 599, loss = 0.15387370\n",
      "Iteration 600, loss = 0.15406907\n",
      "Iteration 601, loss = 0.15348682\n",
      "Iteration 602, loss = 0.15353074\n",
      "Iteration 603, loss = 0.15354850\n",
      "Iteration 604, loss = 0.15290450\n",
      "Iteration 605, loss = 0.15264166\n",
      "Iteration 606, loss = 0.15256339\n",
      "Iteration 607, loss = 0.15262740\n",
      "Iteration 608, loss = 0.15275798\n",
      "Iteration 609, loss = 0.15257979\n",
      "Iteration 610, loss = 0.15210156\n",
      "Iteration 611, loss = 0.15270870\n",
      "Iteration 612, loss = 0.15258808\n",
      "Iteration 613, loss = 0.15175207\n",
      "Iteration 614, loss = 0.15133035\n",
      "Iteration 615, loss = 0.15108259\n",
      "Iteration 616, loss = 0.15110507\n",
      "Iteration 617, loss = 0.15124061\n",
      "Iteration 618, loss = 0.15120090\n",
      "Iteration 619, loss = 0.15104597\n",
      "Iteration 620, loss = 0.15052872\n",
      "Iteration 621, loss = 0.15058536\n",
      "Iteration 622, loss = 0.15038268\n",
      "Iteration 623, loss = 0.15018792\n",
      "Iteration 624, loss = 0.15006372\n",
      "Iteration 625, loss = 0.14982168\n",
      "Iteration 626, loss = 0.15005151\n",
      "Iteration 627, loss = 0.14949792\n",
      "Iteration 628, loss = 0.14953955\n",
      "Iteration 629, loss = 0.14931833\n",
      "Iteration 630, loss = 0.14888763\n",
      "Iteration 631, loss = 0.14872466\n",
      "Iteration 632, loss = 0.14875157\n",
      "Iteration 633, loss = 0.14887763\n",
      "Iteration 634, loss = 0.14852173\n",
      "Iteration 635, loss = 0.14826165\n",
      "Iteration 636, loss = 0.14860367\n",
      "Iteration 637, loss = 0.14826921\n",
      "Iteration 638, loss = 0.14803491\n",
      "Iteration 639, loss = 0.14778821\n",
      "Iteration 640, loss = 0.14769912\n",
      "Iteration 641, loss = 0.14772831\n",
      "Iteration 642, loss = 0.14754285\n",
      "Iteration 643, loss = 0.14809245\n",
      "Iteration 644, loss = 0.14726278\n",
      "Iteration 645, loss = 0.14729110\n",
      "Iteration 646, loss = 0.14696156\n",
      "Iteration 647, loss = 0.14684109\n",
      "Iteration 648, loss = 0.14653640\n",
      "Iteration 649, loss = 0.14676824\n",
      "Iteration 650, loss = 0.14621567\n",
      "Iteration 651, loss = 0.14609358\n",
      "Iteration 652, loss = 0.14644337\n",
      "Iteration 653, loss = 0.14603778\n",
      "Iteration 654, loss = 0.14607684\n",
      "Iteration 655, loss = 0.14561461\n",
      "Iteration 656, loss = 0.14560303\n",
      "Iteration 657, loss = 0.14581531\n",
      "Iteration 658, loss = 0.14530949\n",
      "Iteration 659, loss = 0.14494986\n",
      "Iteration 660, loss = 0.14515349\n",
      "Iteration 661, loss = 0.14499065\n",
      "Iteration 662, loss = 0.14498892\n",
      "Iteration 663, loss = 0.14463520\n",
      "Iteration 664, loss = 0.14441085\n",
      "Iteration 665, loss = 0.14422966\n",
      "Iteration 666, loss = 0.14427686\n",
      "Iteration 667, loss = 0.14442761\n",
      "Iteration 668, loss = 0.14423395\n",
      "Iteration 669, loss = 0.14376662\n",
      "Iteration 670, loss = 0.14378522\n",
      "Iteration 671, loss = 0.14404116\n",
      "Iteration 672, loss = 0.14400245\n",
      "Iteration 673, loss = 0.14373634\n",
      "Iteration 674, loss = 0.14342723\n",
      "Iteration 675, loss = 0.14323918\n",
      "Iteration 676, loss = 0.14329947\n",
      "Iteration 677, loss = 0.14316710\n",
      "Iteration 678, loss = 0.14324700\n",
      "Iteration 679, loss = 0.14316748\n",
      "Iteration 680, loss = 0.14337913\n",
      "Iteration 681, loss = 0.14282688\n",
      "Iteration 682, loss = 0.14241556\n",
      "Iteration 683, loss = 0.14217074\n",
      "Iteration 684, loss = 0.14193448\n",
      "Iteration 685, loss = 0.14214259\n",
      "Iteration 686, loss = 0.14194385\n",
      "Iteration 687, loss = 0.14203717\n",
      "Iteration 688, loss = 0.14159253\n",
      "Iteration 689, loss = 0.14123612\n",
      "Iteration 690, loss = 0.14132042\n",
      "Iteration 691, loss = 0.14129537\n",
      "Iteration 692, loss = 0.14113489\n",
      "Iteration 693, loss = 0.14088086\n",
      "Iteration 694, loss = 0.14092705\n",
      "Iteration 695, loss = 0.14105128\n",
      "Iteration 696, loss = 0.14117608\n",
      "Iteration 697, loss = 0.14104927\n",
      "Iteration 698, loss = 0.14032025\n",
      "Iteration 699, loss = 0.14065841\n",
      "Iteration 700, loss = 0.14020724\n",
      "Iteration 701, loss = 0.13996923\n",
      "Iteration 702, loss = 0.14022412\n",
      "Iteration 703, loss = 0.14012599\n",
      "Iteration 704, loss = 0.14023661\n",
      "Iteration 705, loss = 0.13964688\n",
      "Iteration 706, loss = 0.13956468\n",
      "Iteration 707, loss = 0.13967036\n",
      "Iteration 708, loss = 0.13949755\n",
      "Iteration 709, loss = 0.13955306\n",
      "Iteration 710, loss = 0.13927190\n",
      "Iteration 711, loss = 0.13911725\n",
      "Iteration 712, loss = 0.13889282\n",
      "Iteration 713, loss = 0.13890794\n",
      "Iteration 714, loss = 0.13914058\n",
      "Iteration 715, loss = 0.13946223\n",
      "Iteration 716, loss = 0.13970916\n",
      "Iteration 717, loss = 0.13886854\n",
      "Iteration 718, loss = 0.13860484\n",
      "Iteration 719, loss = 0.13808617\n",
      "Iteration 720, loss = 0.13831261\n",
      "Iteration 721, loss = 0.13834977\n",
      "Iteration 722, loss = 0.13792699\n",
      "Iteration 723, loss = 0.13774574\n",
      "Iteration 724, loss = 0.13763695\n",
      "Iteration 725, loss = 0.13782773\n",
      "Iteration 726, loss = 0.13762744\n",
      "Iteration 727, loss = 0.13736683\n",
      "Iteration 728, loss = 0.13782539\n",
      "Iteration 729, loss = 0.13719164\n",
      "Iteration 730, loss = 0.13682635\n",
      "Iteration 731, loss = 0.13702379\n",
      "Iteration 732, loss = 0.13702844\n",
      "Iteration 733, loss = 0.13668831\n",
      "Iteration 734, loss = 0.13678366\n",
      "Iteration 735, loss = 0.13670181\n",
      "Iteration 736, loss = 0.13658244\n",
      "Iteration 737, loss = 0.13633112\n",
      "Iteration 738, loss = 0.13630918\n",
      "Iteration 739, loss = 0.13684333\n",
      "Iteration 740, loss = 0.13614910\n",
      "Iteration 741, loss = 0.13595973\n",
      "Iteration 742, loss = 0.13614769\n",
      "Iteration 743, loss = 0.13578506\n",
      "Iteration 744, loss = 0.13616429\n",
      "Iteration 745, loss = 0.13587733\n",
      "Iteration 746, loss = 0.13583224\n",
      "Iteration 747, loss = 0.13535190\n",
      "Iteration 748, loss = 0.13542483\n",
      "Iteration 749, loss = 0.13507766\n",
      "Iteration 750, loss = 0.13534491\n",
      "Iteration 751, loss = 0.13517168\n",
      "Iteration 752, loss = 0.13498967\n",
      "Iteration 753, loss = 0.13490177\n",
      "Iteration 754, loss = 0.13501364\n",
      "Iteration 755, loss = 0.13503902\n",
      "Iteration 756, loss = 0.13459805\n",
      "Iteration 757, loss = 0.13466492\n",
      "Iteration 758, loss = 0.13460726\n",
      "Iteration 759, loss = 0.13448265\n",
      "Iteration 760, loss = 0.13417194\n",
      "Iteration 761, loss = 0.13434153\n",
      "Iteration 762, loss = 0.13461535\n",
      "Iteration 763, loss = 0.13390213\n",
      "Iteration 764, loss = 0.13401850\n",
      "Iteration 765, loss = 0.13381754\n",
      "Iteration 766, loss = 0.13376776\n",
      "Iteration 767, loss = 0.13402120\n",
      "Iteration 768, loss = 0.13343953\n",
      "Iteration 769, loss = 0.13344948\n",
      "Iteration 770, loss = 0.13333948\n",
      "Iteration 771, loss = 0.13314392\n",
      "Iteration 772, loss = 0.13311002\n",
      "Iteration 773, loss = 0.13296721\n",
      "Iteration 774, loss = 0.13305912\n",
      "Iteration 775, loss = 0.13305423\n",
      "Iteration 776, loss = 0.13300924\n",
      "Iteration 777, loss = 0.13305386\n",
      "Iteration 778, loss = 0.13290855\n",
      "Iteration 779, loss = 0.13288513\n",
      "Iteration 780, loss = 0.13353714\n",
      "Iteration 781, loss = 0.13286128\n",
      "Iteration 782, loss = 0.13331913\n",
      "Iteration 783, loss = 0.13284214\n",
      "Iteration 784, loss = 0.13249642\n",
      "Iteration 785, loss = 0.13199737\n",
      "Iteration 786, loss = 0.13182311\n",
      "Iteration 787, loss = 0.13180958\n",
      "Iteration 788, loss = 0.13197263\n",
      "Iteration 789, loss = 0.13171215\n",
      "Iteration 790, loss = 0.13180272\n",
      "Iteration 791, loss = 0.13174691\n",
      "Iteration 792, loss = 0.13178146\n",
      "Iteration 793, loss = 0.13134954\n",
      "Iteration 794, loss = 0.13154757\n",
      "Iteration 795, loss = 0.13132781\n",
      "Iteration 796, loss = 0.13125687\n",
      "Iteration 797, loss = 0.13097278\n",
      "Iteration 798, loss = 0.13099906\n",
      "Iteration 799, loss = 0.13111614\n",
      "Iteration 800, loss = 0.13133030\n",
      "Iteration 801, loss = 0.13109377\n",
      "Iteration 802, loss = 0.13108166\n",
      "Iteration 803, loss = 0.13077750\n",
      "Iteration 804, loss = 0.13040045\n",
      "Iteration 805, loss = 0.13111446\n",
      "Iteration 806, loss = 0.13168191\n",
      "Iteration 807, loss = 0.13054411\n",
      "Iteration 808, loss = 0.13097211\n",
      "Iteration 809, loss = 0.13097380\n",
      "Iteration 810, loss = 0.13021020\n",
      "Iteration 811, loss = 0.12998027\n",
      "Iteration 812, loss = 0.13088764\n",
      "Iteration 813, loss = 0.13132800\n",
      "Iteration 814, loss = 0.13036870\n",
      "Iteration 815, loss = 0.13026167\n",
      "Iteration 816, loss = 0.13006612\n",
      "Iteration 817, loss = 0.12973610\n",
      "Iteration 818, loss = 0.13023727\n",
      "Iteration 819, loss = 0.12955047\n",
      "Iteration 820, loss = 0.12963182\n",
      "Iteration 821, loss = 0.12931273\n",
      "Iteration 822, loss = 0.12912453\n",
      "Iteration 823, loss = 0.12901039\n",
      "Iteration 824, loss = 0.12904813\n",
      "Iteration 825, loss = 0.12912467\n",
      "Iteration 826, loss = 0.12882368\n",
      "Iteration 827, loss = 0.12886904\n",
      "Iteration 828, loss = 0.12912102\n",
      "Iteration 829, loss = 0.12905521\n",
      "Iteration 830, loss = 0.12941657\n",
      "Iteration 831, loss = 0.12895447\n",
      "Iteration 832, loss = 0.12873433\n",
      "Iteration 833, loss = 0.12851502\n",
      "Iteration 834, loss = 0.12855934\n",
      "Iteration 835, loss = 0.12837584\n",
      "Iteration 836, loss = 0.12821621\n",
      "Iteration 837, loss = 0.12859323\n",
      "Iteration 838, loss = 0.12887616\n",
      "Iteration 839, loss = 0.12839500\n",
      "Iteration 840, loss = 0.12847944\n",
      "Iteration 841, loss = 0.12805429\n",
      "Iteration 842, loss = 0.12792999\n",
      "Iteration 843, loss = 0.12770987\n",
      "Iteration 844, loss = 0.12784787\n",
      "Iteration 845, loss = 0.12823808\n",
      "Iteration 846, loss = 0.12806016\n",
      "Iteration 847, loss = 0.12820321\n",
      "Iteration 848, loss = 0.12789790\n",
      "Iteration 849, loss = 0.12821371\n",
      "Iteration 850, loss = 0.12805250\n",
      "Iteration 851, loss = 0.12812947\n",
      "Iteration 852, loss = 0.12758978\n",
      "Iteration 853, loss = 0.12724694\n",
      "Iteration 854, loss = 0.12743781\n",
      "Iteration 855, loss = 0.12770518\n",
      "Iteration 856, loss = 0.12765946\n",
      "Iteration 857, loss = 0.12735822\n",
      "Iteration 858, loss = 0.12715958\n",
      "Iteration 859, loss = 0.12732240\n",
      "Iteration 860, loss = 0.12686650\n",
      "Iteration 861, loss = 0.12666772\n",
      "Iteration 862, loss = 0.12668361\n",
      "Iteration 863, loss = 0.12687493\n",
      "Iteration 864, loss = 0.12689554\n",
      "Iteration 865, loss = 0.12680282\n",
      "Iteration 866, loss = 0.12639983\n",
      "Iteration 867, loss = 0.12668047\n",
      "Iteration 868, loss = 0.12678897\n",
      "Iteration 869, loss = 0.12663367\n",
      "Iteration 870, loss = 0.12658485\n",
      "Iteration 871, loss = 0.12619834\n",
      "Iteration 872, loss = 0.12628853\n",
      "Iteration 873, loss = 0.12632570\n",
      "Iteration 874, loss = 0.12629972\n",
      "Iteration 875, loss = 0.12902507\n",
      "Iteration 876, loss = 0.12694975\n",
      "Iteration 877, loss = 0.12666504\n",
      "Iteration 878, loss = 0.12602950\n",
      "Iteration 879, loss = 0.12611487\n",
      "Iteration 880, loss = 0.12578277\n",
      "Iteration 881, loss = 0.12579059\n",
      "Iteration 882, loss = 0.12575298\n",
      "Iteration 883, loss = 0.12564449\n",
      "Iteration 884, loss = 0.12540772\n",
      "Iteration 885, loss = 0.12536494\n",
      "Iteration 886, loss = 0.12552872\n",
      "Iteration 887, loss = 0.12541005\n",
      "Iteration 888, loss = 0.12572240\n",
      "Iteration 889, loss = 0.12526002\n",
      "Iteration 890, loss = 0.12531292\n",
      "Iteration 891, loss = 0.12497042\n",
      "Iteration 892, loss = 0.12506002\n",
      "Iteration 893, loss = 0.12505750\n",
      "Iteration 894, loss = 0.12490895\n",
      "Iteration 895, loss = 0.12482112\n",
      "Iteration 896, loss = 0.12484286\n",
      "Iteration 897, loss = 0.12485329\n",
      "Iteration 898, loss = 0.12498397\n",
      "Iteration 899, loss = 0.12471413\n",
      "Iteration 900, loss = 0.12461111\n",
      "Iteration 901, loss = 0.12483182\n",
      "Iteration 902, loss = 0.12462063\n",
      "Iteration 903, loss = 0.12438782\n",
      "Iteration 904, loss = 0.12445960\n",
      "Iteration 905, loss = 0.12441040\n",
      "Iteration 906, loss = 0.12483401\n",
      "Iteration 907, loss = 0.12511044\n",
      "Iteration 908, loss = 0.12490382\n",
      "Iteration 909, loss = 0.12423612\n",
      "Iteration 910, loss = 0.12430407\n",
      "Iteration 911, loss = 0.12416103\n",
      "Iteration 912, loss = 0.12422391\n",
      "Iteration 913, loss = 0.12394383\n",
      "Iteration 914, loss = 0.12393046\n",
      "Iteration 915, loss = 0.12390722\n",
      "Iteration 916, loss = 0.12388552\n",
      "Iteration 917, loss = 0.12389883\n",
      "Iteration 918, loss = 0.12408772\n",
      "Iteration 919, loss = 0.12421573\n",
      "Iteration 920, loss = 0.12359913\n",
      "Iteration 921, loss = 0.12358453\n",
      "Iteration 922, loss = 0.12371842\n",
      "Iteration 923, loss = 0.12364888\n",
      "Iteration 924, loss = 0.12345259\n",
      "Iteration 925, loss = 0.12394326\n",
      "Iteration 926, loss = 0.12351587\n",
      "Iteration 927, loss = 0.12375112\n",
      "Iteration 928, loss = 0.12342696\n",
      "Iteration 929, loss = 0.12317665\n",
      "Iteration 930, loss = 0.12328459\n",
      "Iteration 931, loss = 0.12331856\n",
      "Iteration 932, loss = 0.12303841\n",
      "Iteration 933, loss = 0.12323261\n",
      "Iteration 934, loss = 0.12357805\n",
      "Iteration 935, loss = 0.12308365\n",
      "Iteration 936, loss = 0.12320646\n",
      "Iteration 937, loss = 0.12297076\n",
      "Iteration 938, loss = 0.12285204\n",
      "Iteration 939, loss = 0.12282460\n",
      "Iteration 940, loss = 0.12271262\n",
      "Iteration 941, loss = 0.12336789\n",
      "Iteration 942, loss = 0.12336842\n",
      "Iteration 943, loss = 0.12279835\n",
      "Iteration 944, loss = 0.12273310\n",
      "Iteration 945, loss = 0.12267525\n",
      "Iteration 946, loss = 0.12229761\n",
      "Iteration 947, loss = 0.12228581\n",
      "Iteration 948, loss = 0.12241089\n",
      "Iteration 949, loss = 0.12218124\n",
      "Iteration 950, loss = 0.12239870\n",
      "Iteration 951, loss = 0.12217018\n",
      "Iteration 952, loss = 0.12229509\n",
      "Iteration 953, loss = 0.12202780\n",
      "Iteration 954, loss = 0.12235704\n",
      "Iteration 955, loss = 0.12236888\n",
      "Iteration 956, loss = 0.12219638\n",
      "Iteration 957, loss = 0.12184360\n",
      "Iteration 958, loss = 0.12191149\n",
      "Iteration 959, loss = 0.12179961\n",
      "Iteration 960, loss = 0.12184254\n",
      "Iteration 961, loss = 0.12164790\n",
      "Iteration 962, loss = 0.12155161\n",
      "Iteration 963, loss = 0.12160690\n",
      "Iteration 964, loss = 0.12216786\n",
      "Iteration 965, loss = 0.12256570\n",
      "Iteration 966, loss = 0.12229676\n",
      "Iteration 967, loss = 0.12188238\n",
      "Iteration 968, loss = 0.12175007\n",
      "Iteration 969, loss = 0.12132339\n",
      "Iteration 970, loss = 0.12138018\n",
      "Iteration 971, loss = 0.12123307\n",
      "Iteration 972, loss = 0.12129370\n",
      "Iteration 973, loss = 0.12165377\n",
      "Iteration 974, loss = 0.12158142\n",
      "Iteration 975, loss = 0.12129815\n",
      "Iteration 976, loss = 0.12126194\n",
      "Iteration 977, loss = 0.12143454\n",
      "Iteration 978, loss = 0.12106492\n",
      "Iteration 979, loss = 0.12095267\n",
      "Iteration 980, loss = 0.12115219\n",
      "Iteration 981, loss = 0.12142194\n",
      "Iteration 982, loss = 0.12095878\n",
      "Iteration 983, loss = 0.12099709\n",
      "Iteration 984, loss = 0.12090159\n",
      "Iteration 985, loss = 0.12109883\n",
      "Iteration 986, loss = 0.12064448\n",
      "Iteration 987, loss = 0.12065959\n",
      "Iteration 988, loss = 0.12070412\n",
      "Iteration 989, loss = 0.12048104\n",
      "Iteration 990, loss = 0.12029170\n",
      "Iteration 991, loss = 0.12065389\n",
      "Iteration 992, loss = 0.12043471\n",
      "Iteration 993, loss = 0.12038369\n",
      "Iteration 994, loss = 0.12039457\n",
      "Iteration 995, loss = 0.12019696\n",
      "Iteration 996, loss = 0.12019745\n",
      "Iteration 997, loss = 0.12006208\n",
      "Iteration 998, loss = 0.12021278\n",
      "Iteration 999, loss = 0.12024400\n",
      "Iteration 1000, loss = 0.12042743\n",
      "Iteration 1001, loss = 0.12039692\n",
      "Iteration 1002, loss = 0.12025659\n",
      "Iteration 1003, loss = 0.12033475\n",
      "Iteration 1004, loss = 0.12002001\n",
      "Iteration 1005, loss = 0.11974012\n",
      "Iteration 1006, loss = 0.11968028\n",
      "Iteration 1007, loss = 0.11973121\n",
      "Iteration 1008, loss = 0.11976880\n",
      "Iteration 1009, loss = 0.11958516\n",
      "Iteration 1010, loss = 0.11961417\n",
      "Iteration 1011, loss = 0.11968999\n",
      "Iteration 1012, loss = 0.11979394\n",
      "Iteration 1013, loss = 0.12077942\n",
      "Iteration 1014, loss = 0.12005114\n",
      "Iteration 1015, loss = 0.11960922\n",
      "Iteration 1016, loss = 0.11931816\n",
      "Iteration 1017, loss = 0.11957958\n",
      "Iteration 1018, loss = 0.11960881\n",
      "Iteration 1019, loss = 0.11936889\n",
      "Iteration 1020, loss = 0.11933908\n",
      "Iteration 1021, loss = 0.11936503\n",
      "Iteration 1022, loss = 0.11928111\n",
      "Iteration 1023, loss = 0.11931620\n",
      "Iteration 1024, loss = 0.11926993\n",
      "Iteration 1025, loss = 0.11923823\n",
      "Iteration 1026, loss = 0.11917463\n",
      "Iteration 1027, loss = 0.11934600\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.4, batch_size=512, hidden_layer_sizes=(256, 128, 32),\n",
       "              learning_rate='adaptive', learning_rate_init=0.0001,\n",
       "              max_iter=10000, random_state=13, verbose=True)"
      ],
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=0.4, batch_size=512, hidden_layer_sizes=(256, 128, 32),\n",
       "              learning_rate=&#x27;adaptive&#x27;, learning_rate_init=0.0001,\n",
       "              max_iter=10000, random_state=13, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.4, batch_size=512, hidden_layer_sizes=(256, 128, 32),\n",
       "              learning_rate=&#x27;adaptive&#x27;, learning_rate_init=0.0001,\n",
       "              max_iter=10000, random_state=13, verbose=True)</pre></div></div></div></div></div>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 139
  },
  {
   "cell_type": "code",
   "id": "5ca78c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T05:39:53.403752Z",
     "start_time": "2025-02-04T05:39:53.208508Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, mlp.predict(xTr))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 140
  },
  {
   "cell_type": "code",
   "id": "aa884592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T05:39:53.442245Z",
     "start_time": "2025-02-04T05:39:53.404756Z"
    }
   },
   "source": [
    "# testing score\n",
    "accuracy_score(yTe.Result.values, mlp.predict(xTe))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4574574574574575"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T05:39:53.469649Z",
     "start_time": "2025-02-04T05:39:53.443254Z"
    }
   },
   "cell_type": "code",
   "source": "xTr",
   "id": "f3b1aae2279970a7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          B365H     B365D     B365A   AHh  B365AHH  B365AHA  HomeTeamELO  \\\n",
       "27411 -0.900990  4.955212  4.460696 -2.25     1.96     1.94     2.950768   \n",
       "27412  0.121878 -0.629606 -0.579036  0.00     2.07     1.72    -1.032273   \n",
       "27413 -0.224577 -0.796317 -0.327504 -0.25     2.02     1.88    -1.096578   \n",
       "27414 -0.455547 -0.379540 -0.206284 -0.50     2.01     1.89     0.014493   \n",
       "27415 -0.224577 -0.712962 -0.357809 -0.25     2.05     1.85    -1.494997   \n",
       "...         ...       ...       ...   ...      ...      ...          ...   \n",
       "36342 -0.780005  0.995826  0.975614 -1.50     2.01     1.89     0.833673   \n",
       "36343 -0.747010  0.787437  0.596801 -1.25     1.95     1.95     0.780336   \n",
       "36344  0.671807 -0.046118 -0.751775  0.50     2.06     1.84    -0.120739   \n",
       "36345  1.496700 -0.046118 -0.833598  1.00     1.98     1.92    -0.470864   \n",
       "36346 -0.714014  0.579048  0.521038 -1.25     2.05     1.85     1.682098   \n",
       "\n",
       "       AwayTeamELO  balance_val  Div 0  ...  Month     Sin_Month  Cos_Month  \\\n",
       "27411    -0.480580           -2      0  ...      2  8.660254e-01   0.500000   \n",
       "27412     0.326923            0      0  ...      2  8.660254e-01   0.500000   \n",
       "27413    -1.111298           -1      0  ...      2  8.660254e-01   0.500000   \n",
       "27414    -0.572309           -1      0  ...      2  8.660254e-01   0.500000   \n",
       "27415    -1.621927           -1      0  ...      2  8.660254e-01   0.500000   \n",
       "...            ...          ...    ...  ...    ...           ...        ...   \n",
       "36342    -0.820631           -2      0  ...     11 -5.000000e-01   0.866025   \n",
       "36343    -0.632290           -1      0  ...     11 -5.000000e-01   0.866025   \n",
       "36344     1.501672            1      0  ...     11 -5.000000e-01   0.866025   \n",
       "36345     2.329773            1      0  ...     11 -5.000000e-01   0.866025   \n",
       "36346     0.585685           -1      0  ...     12 -2.449294e-16   1.000000   \n",
       "\n",
       "        Sin_Day   Cos_Day  Last Match Result  HomeWinStreak  AwayWinStreak  \\\n",
       "27411  0.635432  0.772157                  1              0              0   \n",
       "27412  0.635432  0.772157                  2              0              0   \n",
       "27413  0.648630  0.761104                  0              0              0   \n",
       "27414  0.648630  0.761104                  3              0              0   \n",
       "27415  0.648630  0.761104                  0              0              0   \n",
       "...         ...       ...                ...            ...            ...   \n",
       "36342 -0.566702  0.823923                  2              4              0   \n",
       "36343 -0.566702  0.823923                  1              0              0   \n",
       "36344 -0.566702  0.823923                  0              0              1   \n",
       "36345 -0.566702  0.823923                  0              0              1   \n",
       "36346 -0.478734  0.877960                  2              3              1   \n",
       "\n",
       "       HomeWinsToDate  AwayWinsToDate  \n",
       "27411             0.0             0.0  \n",
       "27412             0.0             0.0  \n",
       "27413             0.0             0.0  \n",
       "27414             0.0             0.0  \n",
       "27415             0.0             0.0  \n",
       "...               ...             ...  \n",
       "36342            10.0             2.0  \n",
       "36343             6.0             2.0  \n",
       "36344             6.0            10.0  \n",
       "36345             6.0            15.0  \n",
       "36346            13.0             6.0  \n",
       "\n",
       "[8936 rows x 439 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>AHh</th>\n",
       "      <th>B365AHH</th>\n",
       "      <th>B365AHA</th>\n",
       "      <th>HomeTeamELO</th>\n",
       "      <th>AwayTeamELO</th>\n",
       "      <th>balance_val</th>\n",
       "      <th>Div 0</th>\n",
       "      <th>...</th>\n",
       "      <th>Month</th>\n",
       "      <th>Sin_Month</th>\n",
       "      <th>Cos_Month</th>\n",
       "      <th>Sin_Day</th>\n",
       "      <th>Cos_Day</th>\n",
       "      <th>Last Match Result</th>\n",
       "      <th>HomeWinStreak</th>\n",
       "      <th>AwayWinStreak</th>\n",
       "      <th>HomeWinsToDate</th>\n",
       "      <th>AwayWinsToDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27411</th>\n",
       "      <td>-0.900990</td>\n",
       "      <td>4.955212</td>\n",
       "      <td>4.460696</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>1.96</td>\n",
       "      <td>1.94</td>\n",
       "      <td>2.950768</td>\n",
       "      <td>-0.480580</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.635432</td>\n",
       "      <td>0.772157</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27412</th>\n",
       "      <td>0.121878</td>\n",
       "      <td>-0.629606</td>\n",
       "      <td>-0.579036</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.72</td>\n",
       "      <td>-1.032273</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.635432</td>\n",
       "      <td>0.772157</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27413</th>\n",
       "      <td>-0.224577</td>\n",
       "      <td>-0.796317</td>\n",
       "      <td>-0.327504</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>2.02</td>\n",
       "      <td>1.88</td>\n",
       "      <td>-1.096578</td>\n",
       "      <td>-1.111298</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.648630</td>\n",
       "      <td>0.761104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27414</th>\n",
       "      <td>-0.455547</td>\n",
       "      <td>-0.379540</td>\n",
       "      <td>-0.206284</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>-0.572309</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.648630</td>\n",
       "      <td>0.761104</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27415</th>\n",
       "      <td>-0.224577</td>\n",
       "      <td>-0.712962</td>\n",
       "      <td>-0.357809</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.85</td>\n",
       "      <td>-1.494997</td>\n",
       "      <td>-1.621927</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.648630</td>\n",
       "      <td>0.761104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36342</th>\n",
       "      <td>-0.780005</td>\n",
       "      <td>0.995826</td>\n",
       "      <td>0.975614</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.833673</td>\n",
       "      <td>-0.820631</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.566702</td>\n",
       "      <td>0.823923</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36343</th>\n",
       "      <td>-0.747010</td>\n",
       "      <td>0.787437</td>\n",
       "      <td>0.596801</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.780336</td>\n",
       "      <td>-0.632290</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.566702</td>\n",
       "      <td>0.823923</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36344</th>\n",
       "      <td>0.671807</td>\n",
       "      <td>-0.046118</td>\n",
       "      <td>-0.751775</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2.06</td>\n",
       "      <td>1.84</td>\n",
       "      <td>-0.120739</td>\n",
       "      <td>1.501672</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.566702</td>\n",
       "      <td>0.823923</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36345</th>\n",
       "      <td>1.496700</td>\n",
       "      <td>-0.046118</td>\n",
       "      <td>-0.833598</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.92</td>\n",
       "      <td>-0.470864</td>\n",
       "      <td>2.329773</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.566702</td>\n",
       "      <td>0.823923</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36346</th>\n",
       "      <td>-0.714014</td>\n",
       "      <td>0.579048</td>\n",
       "      <td>0.521038</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.682098</td>\n",
       "      <td>0.585685</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.478734</td>\n",
       "      <td>0.877960</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8936 rows × 439 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T05:39:53.481452Z",
     "start_time": "2025-02-04T05:39:53.470658Z"
    }
   },
   "cell_type": "code",
   "source": "yTr",
   "id": "e4dd2c1c61d1079c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Result\n",
       "27411       2\n",
       "27412       0\n",
       "27413       2\n",
       "27414       1\n",
       "27415       1\n",
       "...       ...\n",
       "36342       1\n",
       "36343       1\n",
       "36344       0\n",
       "36345       1\n",
       "36346       0\n",
       "\n",
       "[8936 rows x 1 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27411</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27412</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27413</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27414</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27415</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36342</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36343</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36344</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36345</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36346</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8936 rows × 1 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T05:39:53.501513Z",
     "start_time": "2025-02-04T05:39:53.481452Z"
    }
   },
   "cell_type": "code",
   "source": "xTe",
   "id": "b2b865f37b49cf07",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          B365H     B365D     B365A   AHh  B365AHH  B365AHA  HomeTeamELO  \\\n",
       "36347 -0.714014 -0.046118  1.127139 -1.00     1.84     2.06     0.585328   \n",
       "36348 -0.235576 -0.546251 -0.388114 -0.25     2.06     1.84     0.131709   \n",
       "36349 -0.428051 -0.712962 -0.054758 -0.50     2.06     1.87    -0.332145   \n",
       "36350  0.011892 -0.671284 -0.500243  0.00     2.03     1.90    -0.865586   \n",
       "36351  2.321593  0.579048 -0.866934  1.25     1.95     1.98    -1.222583   \n",
       "...         ...       ...       ...   ...      ...      ...          ...   \n",
       "37341  0.286857 -0.629606 -0.615402  0.25     1.99     1.94    -0.216969   \n",
       "37342 -0.015604 -0.963028 -0.424480  0.00     1.91     1.99    -0.479657   \n",
       "37343 -0.081596 -0.879673 -0.388114  0.00     1.89     2.01    -0.590991   \n",
       "37344 -0.290569 -0.879673 -0.206284 -0.25     1.95     1.98    -0.091103   \n",
       "37345 -0.345562 -0.462895 -0.312351 -0.25     1.92     2.01    -0.973865   \n",
       "\n",
       "       AwayTeamELO  balance_val  Div 0  ...  Month  Sin_Month  Cos_Month  \\\n",
       "36347    -0.545998           -1      0  ...      2   0.866025   0.500000   \n",
       "36348     0.831459           -1      0  ...      2   0.866025   0.500000   \n",
       "36349    -0.391280           -1      0  ...      2   0.866025   0.500000   \n",
       "36350    -0.138709            0      0  ...      2   0.866025   0.500000   \n",
       "36351     1.549273            1      0  ...      2   0.866025   0.500000   \n",
       "...            ...          ...    ...  ...    ...        ...        ...   \n",
       "37341     0.652390            1      0  ...      7  -0.500000  -0.866025   \n",
       "37342     0.091215            0      0  ...      7  -0.500000  -0.866025   \n",
       "37343    -0.458062            0      0  ...      7  -0.500000  -0.866025   \n",
       "37344    -0.531554           -1      0  ...      7  -0.500000  -0.866025   \n",
       "37345    -0.636398           -1      0  ...      7  -0.500000  -0.866025   \n",
       "\n",
       "        Sin_Day   Cos_Day  Last Match Result  HomeWinStreak  AwayWinStreak  \\\n",
       "36347  0.711657  0.702527                  1              0              0   \n",
       "36348  0.711657  0.702527                  1              0              0   \n",
       "36349  0.723644  0.690173                  1              0              0   \n",
       "36350  0.723644  0.690173                  0              0              0   \n",
       "36351  0.723644  0.690173                  0              0              0   \n",
       "...         ...       ...                ...            ...            ...   \n",
       "37341 -0.425000 -0.905193                  0              0              0   \n",
       "37342 -0.425000 -0.905193                  0              1              1   \n",
       "37343 -0.440519 -0.897743                  2              0              0   \n",
       "37344 -0.440519 -0.897743                  0              1              0   \n",
       "37345 -0.440519 -0.897743                  0              0              0   \n",
       "\n",
       "       HomeWinsToDate  AwayWinsToDate  \n",
       "36347             0.0             0.0  \n",
       "36348             0.0             0.0  \n",
       "36349             0.0             0.0  \n",
       "36350             0.0             0.0  \n",
       "36351             0.0             0.0  \n",
       "...               ...             ...  \n",
       "37341             4.0             0.0  \n",
       "37342             3.0             3.0  \n",
       "37343             3.0             1.0  \n",
       "37344             1.0             1.0  \n",
       "37345             3.0             3.0  \n",
       "\n",
       "[999 rows x 439 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>AHh</th>\n",
       "      <th>B365AHH</th>\n",
       "      <th>B365AHA</th>\n",
       "      <th>HomeTeamELO</th>\n",
       "      <th>AwayTeamELO</th>\n",
       "      <th>balance_val</th>\n",
       "      <th>Div 0</th>\n",
       "      <th>...</th>\n",
       "      <th>Month</th>\n",
       "      <th>Sin_Month</th>\n",
       "      <th>Cos_Month</th>\n",
       "      <th>Sin_Day</th>\n",
       "      <th>Cos_Day</th>\n",
       "      <th>Last Match Result</th>\n",
       "      <th>HomeWinStreak</th>\n",
       "      <th>AwayWinStreak</th>\n",
       "      <th>HomeWinsToDate</th>\n",
       "      <th>AwayWinsToDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36347</th>\n",
       "      <td>-0.714014</td>\n",
       "      <td>-0.046118</td>\n",
       "      <td>1.127139</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.84</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.585328</td>\n",
       "      <td>-0.545998</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.711657</td>\n",
       "      <td>0.702527</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36348</th>\n",
       "      <td>-0.235576</td>\n",
       "      <td>-0.546251</td>\n",
       "      <td>-0.388114</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>2.06</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.131709</td>\n",
       "      <td>0.831459</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.711657</td>\n",
       "      <td>0.702527</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36349</th>\n",
       "      <td>-0.428051</td>\n",
       "      <td>-0.712962</td>\n",
       "      <td>-0.054758</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>2.06</td>\n",
       "      <td>1.87</td>\n",
       "      <td>-0.332145</td>\n",
       "      <td>-0.391280</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.723644</td>\n",
       "      <td>0.690173</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36350</th>\n",
       "      <td>0.011892</td>\n",
       "      <td>-0.671284</td>\n",
       "      <td>-0.500243</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.03</td>\n",
       "      <td>1.90</td>\n",
       "      <td>-0.865586</td>\n",
       "      <td>-0.138709</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.723644</td>\n",
       "      <td>0.690173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36351</th>\n",
       "      <td>2.321593</td>\n",
       "      <td>0.579048</td>\n",
       "      <td>-0.866934</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.98</td>\n",
       "      <td>-1.222583</td>\n",
       "      <td>1.549273</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.723644</td>\n",
       "      <td>0.690173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37341</th>\n",
       "      <td>0.286857</td>\n",
       "      <td>-0.629606</td>\n",
       "      <td>-0.615402</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.94</td>\n",
       "      <td>-0.216969</td>\n",
       "      <td>0.652390</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.425000</td>\n",
       "      <td>-0.905193</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37342</th>\n",
       "      <td>-0.015604</td>\n",
       "      <td>-0.963028</td>\n",
       "      <td>-0.424480</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.479657</td>\n",
       "      <td>0.091215</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.425000</td>\n",
       "      <td>-0.905193</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37343</th>\n",
       "      <td>-0.081596</td>\n",
       "      <td>-0.879673</td>\n",
       "      <td>-0.388114</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.01</td>\n",
       "      <td>-0.590991</td>\n",
       "      <td>-0.458062</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37344</th>\n",
       "      <td>-0.290569</td>\n",
       "      <td>-0.879673</td>\n",
       "      <td>-0.206284</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.98</td>\n",
       "      <td>-0.091103</td>\n",
       "      <td>-0.531554</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37345</th>\n",
       "      <td>-0.345562</td>\n",
       "      <td>-0.462895</td>\n",
       "      <td>-0.312351</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2.01</td>\n",
       "      <td>-0.973865</td>\n",
       "      <td>-0.636398</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.440519</td>\n",
       "      <td>-0.897743</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 439 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T05:39:53.518606Z",
     "start_time": "2025-02-04T05:39:53.501513Z"
    }
   },
   "cell_type": "code",
   "source": "yTe",
   "id": "a695d991d7cb690f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Result\n",
       "36347       1\n",
       "36348       1\n",
       "36349       2\n",
       "36350       1\n",
       "36351       0\n",
       "...       ...\n",
       "37341       0\n",
       "37342       2\n",
       "37343       1\n",
       "37344       2\n",
       "37345       1\n",
       "\n",
       "[999 rows x 1 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36347</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36348</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36349</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36350</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36351</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37341</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37342</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37343</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37344</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37345</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 1 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 145
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T05:39:53.563991Z",
     "start_time": "2025-02-04T05:39:53.519611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predict_val = mlp.predict(xTe)\n",
    "series_pre = pd.Series(predict_val, name='Predicted')\n",
    "compare_result = pd.concat([series_pre, yTe.reset_index()], axis=1)\n",
    "compare_result"
   ],
   "id": "75000857933fee73",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Predicted  index  Result\n",
       "0            2  36347       1\n",
       "1            0  36348       1\n",
       "2            1  36349       2\n",
       "3            1  36350       1\n",
       "4            0  36351       0\n",
       "..         ...    ...     ...\n",
       "994          1  37341       0\n",
       "995          2  37342       2\n",
       "996          1  37343       1\n",
       "997          1  37344       2\n",
       "998          0  37345       1\n",
       "\n",
       "[999 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>index</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>36347</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>36348</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>36349</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>36350</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>36351</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>1</td>\n",
       "      <td>37341</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2</td>\n",
       "      <td>37342</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1</td>\n",
       "      <td>37343</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1</td>\n",
       "      <td>37344</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>37345</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T05:39:54.098748Z",
     "start_time": "2025-02-04T05:39:53.564997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "result_subset = compare_result.tail(200)\n",
    "plt.figure(figsize=(10,10))\n",
    "result_subset.plot(x='index', y=['Predicted', 'Result'], kind='line')\n",
    "plt.title(\"Prediction vs Real\")\n",
    "plt.show()"
   ],
   "id": "bf60fe066e7da7b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 147
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# xTr = xTr.drop(columns=['pad1', 'pad2', 'pad3'])\n",
    "# xTe = xTe.drop(columns=['pad1', 'pad2', 'pad3'])"
   ],
   "id": "75c183890ef24f2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 保存模型",
   "id": "de0d8edbd57b1b47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T05:39:54.200991Z",
     "start_time": "2025-02-04T05:39:54.100352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "local_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
    "dump(mlp, f'./sklearn_mlp_{local_time}.joblib')"
   ],
   "id": "3112f4279bcb97e0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./sklearn_mlp_2025_02_04_13_39_54.joblib']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 148
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ### 加载模型\n",
    "# # model_name = 'sklearn_mlp_' + local_time + '.joblib'\n",
    "# model_name = './' + 'sklearn_mlp_2025_01_21_22_41_15.joblib'\n",
    "# mlp = load(model_name)"
   ],
   "id": "b01de6170bcd5bc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T05:39:54.644930Z",
     "start_time": "2025-02-04T05:39:54.202999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = './' + 'sklearn_mlp_2025_01_31_09_00_15.joblib'\n",
    "mlp = load(model_name)\n",
    "predict_val = mlp.predict(xTe)\n",
    "series_pre = pd.Series(predict_val, name='Predicted')\n",
    "compare_result = pd.concat([series_pre, yTe.reset_index()], axis=1)\n",
    "compare_result"
   ],
   "id": "2c112ea4e3a3be4c",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Month\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[149], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msklearn_mlp_2025_01_31_09_00_15.joblib\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      2\u001B[0m mlp \u001B[38;5;241m=\u001B[39m load(model_name)\n\u001B[1;32m----> 3\u001B[0m predict_val \u001B[38;5;241m=\u001B[39m mlp\u001B[38;5;241m.\u001B[39mpredict(xTe)\n\u001B[0;32m      4\u001B[0m series_pre \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mSeries(predict_val, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPredicted\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      5\u001B[0m compare_result \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([series_pre, yTe\u001B[38;5;241m.\u001B[39mreset_index()], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32mD:\\Anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1156\u001B[0m, in \u001B[0;36mMLPClassifier.predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m   1143\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Predict using the multi-layer perceptron classifier.\u001B[39;00m\n\u001B[0;32m   1144\u001B[0m \n\u001B[0;32m   1145\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1153\u001B[0m \u001B[38;5;124;03m    The predicted classes.\u001B[39;00m\n\u001B[0;32m   1154\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1155\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m-> 1156\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_predict(X)\n",
      "File \u001B[1;32mD:\\Anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1160\u001B[0m, in \u001B[0;36mMLPClassifier._predict\u001B[1;34m(self, X, check_input)\u001B[0m\n\u001B[0;32m   1158\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_predict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m   1159\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Private predict method with optional input validation\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1160\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pass_fast(X, check_input\u001B[38;5;241m=\u001B[39mcheck_input)\n\u001B[0;32m   1162\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1163\u001B[0m         y_pred \u001B[38;5;241m=\u001B[39m y_pred\u001B[38;5;241m.\u001B[39mravel()\n",
      "File \u001B[1;32mD:\\Anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:202\u001B[0m, in \u001B[0;36mBaseMultilayerPerceptron._forward_pass_fast\u001B[1;34m(self, X, check_input)\u001B[0m\n\u001B[0;32m    183\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Predict using the trained model\u001B[39;00m\n\u001B[0;32m    184\u001B[0m \n\u001B[0;32m    185\u001B[0m \u001B[38;5;124;03mThis is the same as _forward_pass but does not record the activations\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;124;03m    The decision function of the samples for each class in the model.\u001B[39;00m\n\u001B[0;32m    200\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    201\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m check_input:\n\u001B[1;32m--> 202\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_data(X, accept_sparse\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsr\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsc\u001B[39m\u001B[38;5;124m\"\u001B[39m], reset\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    204\u001B[0m \u001B[38;5;66;03m# Initialize first layer\u001B[39;00m\n\u001B[0;32m    205\u001B[0m activation \u001B[38;5;241m=\u001B[39m X\n",
      "File \u001B[1;32mD:\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:548\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[0;32m    483\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_validate_data\u001B[39m(\n\u001B[0;32m    484\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    485\u001B[0m     X\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mno_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    489\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params,\n\u001B[0;32m    490\u001B[0m ):\n\u001B[0;32m    491\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001B[39;00m\n\u001B[0;32m    492\u001B[0m \n\u001B[0;32m    493\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    546\u001B[0m \u001B[38;5;124;03m        validated.\u001B[39;00m\n\u001B[0;32m    547\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 548\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_feature_names(X, reset\u001B[38;5;241m=\u001B[39mreset)\n\u001B[0;32m    550\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_tags()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequires_y\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m    551\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    552\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m estimator \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    553\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequires y to be passed, but the target y is None.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    554\u001B[0m         )\n",
      "File \u001B[1;32mD:\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:481\u001B[0m, in \u001B[0;36mBaseEstimator._check_feature_names\u001B[1;34m(self, X, reset)\u001B[0m\n\u001B[0;32m    476\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m missing_names \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m unexpected_names:\n\u001B[0;32m    477\u001B[0m     message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    478\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFeature names must be in the same order as they were in fit.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    479\u001B[0m     )\n\u001B[1;32m--> 481\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(message)\n",
      "\u001B[1;31mValueError\u001B[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Month\n"
     ]
    }
   ],
   "execution_count": 149
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T05:39:54.645934Z",
     "start_time": "2025-02-04T05:39:54.645934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# testing score\n",
    "accuracy_score(yTe.Result.values, mlp.predict(xTe))"
   ],
   "id": "83583340c07151b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "result_subset = compare_result.tail(200)\n",
    "plt.figure(figsize=(10,10))\n",
    "result_subset.plot(x='index', y=['Predicted', 'Result'], kind='line')\n",
    "plt.title(\"Prediction vs Real\")\n",
    "plt.show()"
   ],
   "id": "fcaccde8fc213e40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 自定义的pytorch模型结构",
   "id": "bfb8d2bc00d3ecc9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 自定义的 top-k loss\n",
    "def top_k_loss(y_true, y_pred, top_k=2, top_1_weight=1.0, top_2_weight=0.5, misclassification_weight=3):\n",
    "    \"\"\"\n",
    "    y_true: 真实标签，形状 [batch_size]\n",
    "    y_pred: 模型预测的 logits，形状 [batch_size, num_classes]\n",
    "    top_k: top-k 的 k 值\n",
    "    top_1_weight: top-1 命中的权重（越高越小）\n",
    "    top_2_weight: top-2 命中的权重（越高越小）\n",
    "    misclassification_weight: 未命中的样本的损失权重（越高越大）\n",
    "    \"\"\"\n",
    "    # # 获取 top-k 的类别索引和对应的概率值\n",
    "    # top_k_values, top_k_indices = torch.topk(y_pred, top_k, dim=1, largest=True, sorted=True)\n",
    "    # \n",
    "    # # 计算命中情况：top-1 和 top-2\n",
    "    # top_1_correct = (top_k_indices[:, 0] == y_true).float()  # top-1 命中\n",
    "    # top_2_correct = ((top_k_indices[:, 0] == y_true) | (top_k_indices[:, 1] == y_true)).float()  # top-2 命中\n",
    "    # \n",
    "    # # 计算损失\n",
    "    # # top-1 命中损失为 0，top-2 命中损失较小，未命中损失较大\n",
    "    # loss = torch.where(top_1_correct == 1.0, torch.tensor(top_1_weight), torch.tensor(top_2_weight))\n",
    "    # loss = torch.where(top_2_correct == 1.0, loss, torch.tensor(misclassification_weight))\n",
    "    # \n",
    "    # return loss.mean()\n",
    "\n",
    "    # # 获取 top-k 预测的类别索引\n",
    "    # top_k_values, top_k_indices = torch.topk(y_pred, top_k, dim=1, largest=True, sorted=True)\n",
    "    # # print('y_pred: ', y_pred)\n",
    "    # # print('y_true', y_true)\n",
    "    # # print('top_k_values: ', top_k_values)\n",
    "    # # print('top_k_indices: ', top_k_indices)\n",
    "    # \n",
    "    # # 计算 top-1 和 top-2 命中情况\n",
    "    # top_1_correct = (top_k_indices[:, 0] == y_true).float()  # top-1 命中\n",
    "    # top_2_correct = ((top_k_indices[:, 0] == y_true) | (top_k_indices[:, 1] == y_true)).float()  # top-2 命中\n",
    "    # \n",
    "    # # **关键修改：让损失函数与 y_pred 相关，并使用 -log 计算损失**\n",
    "    # # y_pred_prob = torch.softmax(y_pred, dim=1)  # 转换为概率分布\n",
    "    # y_pred_confidence = torch.gather(y_pred, 1, y_true.unsqueeze(1)).squeeze(1)  # 获取真实类别的置信度\n",
    "    # \n",
    "    # # **修改损失计算方式**\n",
    "    # loss = -torch.log(y_pred_confidence + 1e-9) * (\n",
    "    #     top_1_correct * top_1_weight +\n",
    "    #     (1 - top_1_correct) * top_2_correct * top_2_weight +\n",
    "    #     (1 - top_2_correct) * misclassification_weight\n",
    "    # )\n",
    "    # \n",
    "    # return loss.mean()\n",
    "    \n",
    "    \"\"\"\n",
    "    结合 CrossEntropyLoss 和自定义权重的 Top-K 损失\n",
    "    \"\"\"\n",
    "    # 计算 CrossEntropyLoss\n",
    "    ce_loss = F.cross_entropy(y_pred, y_true, reduction='none')  # [batch_size]\n",
    "    # print('y_true', y_true) \n",
    "    # print(f'y_pred {torch.round(y_pred * 10) / 10}')\n",
    "    # # 获取 top-k 预测的类别索引\n",
    "    # top_k_values, top_k_indices = torch.topk(y_pred, top_k, dim=1, largest=True, sorted=True)\n",
    "    # \n",
    "    # # 计算 top-1 和 top-2 命中情况\n",
    "    # top_1_correct = (top_k_indices[:, 0] == y_true).float()\n",
    "    # top_2_correct = ((top_k_indices[:, 0] == y_true) | (top_k_indices[:, 1] == y_true)).float()\n",
    "    # \n",
    "    # # 应用不同的权重\n",
    "    # weight = (\n",
    "    #     top_1_correct * top_1_weight +\n",
    "    #     (1 - top_1_correct) * top_2_correct * top_2_weight +\n",
    "    #     (1 - top_2_correct) * misclassification_weight\n",
    "    # )\n",
    "\n",
    "    # return (ce_loss * weight).mean()\n",
    "    return ce_loss.mean()\n",
    "\n",
    "# 定义 MLP 模型（与之前类似）\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers=[256, 128, 32], output_dim=3, dropout_prob=0.3):\n",
    "        super(MLPModel, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            # layers.append(nn.BatchNorm1d(hidden_dim))  # 添加 BatchNorm 层\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_prob))  # 添加 Dropout 层\n",
    "            in_dim = hidden_dim\n",
    "        layers.append(nn.Linear(in_dim, output_dim))  # 输出层\n",
    "        layers.append(nn.Softmax(dim=1))  # 归一化为概率分布\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x) * 2  # 将 softmax 归一化到 (0,2) 范围\n",
    "\n",
    "# 定义训练过程\n",
    "def train_model(model, x_train, y_train, x_val, y_val, epochs=100, batch_size=512, \n",
    "                learning_rate=1e-4, early_stopping=False, patience=10):\n",
    "    # 创建优化器\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = top_k_loss  # 使用自定义的 top-k loss\n",
    "\n",
    "    # 批次处理数据\n",
    "    dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 初始化早期停止相关变量\n",
    "    best_train_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    best_model_weights = None\n",
    "\n",
    "    # 用于记录训练损失和验证损失\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(labels, outputs)  # 计算损失\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            # 反向传播和优化\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 计算训练集上的平均损失\n",
    "        train_loss = running_train_loss / len(dataloader)\n",
    "\n",
    "        # 计算验证集上的损失\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(x_val)\n",
    "            val_loss = criterion(y_val, val_outputs)\n",
    "\n",
    "        # 保存训练损失和验证损失\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        # 打印当前的训练损失和验证损失\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # 早期停止：检查验证损失\n",
    "        if early_stopping:\n",
    "            if (train_loss - best_train_loss) < best_train_loss * 1e-2:\n",
    "                best_train_loss = train_loss\n",
    "                best_model_weights = model.state_dict()\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "    # 恢复最好的模型权重\n",
    "    if early_stopping and best_model_weights:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "\n",
    "    # 绘制训练损失和验证损失的曲线\n",
    "    plot_loss_curve(train_losses, val_losses)\n",
    "\n",
    "    return model\n",
    "\n",
    "# 计算 top-k 准确率\n",
    "def compute_top_k_accuracy(model, x_test, y_test, k=2):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x_test)\n",
    "        _, top_k_indices = torch.topk(outputs, k, dim=1, largest=True, sorted=True)\n",
    "        correct_top_1 = torch.sum(top_k_indices[:, 0] == y_test).item()\n",
    "        correct_top_2 = torch.sum((top_k_indices[:, 0] == y_test) | (top_k_indices[:, 1] == y_test)).item()\n",
    "        top_1_accuracy = correct_top_1 / len(y_test)\n",
    "        top_2_accuracy = correct_top_2 / len(y_test)\n",
    "    return top_1_accuracy, top_2_accuracy\n",
    "\n",
    "# 绘制训练和验证损失的曲线\n",
    "def plot_loss_curve(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', color='red', linestyle='--', marker='x')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 从 pandas 数据加载训练和验证数据\n",
    "def load_data_from_pandas(x_train_pd, y_train_pd, x_val_pd, y_val_pd, num_classes=3):\n",
    "    \"\"\"\n",
    "    将 pandas 数据加载为 torch 张量\n",
    "    x_train_pd: pandas DataFrame, 训练数据\n",
    "    y_train_pd: pandas Series, 训练标签\n",
    "    x_val_pd: pandas DataFrame, 验证数据\n",
    "    y_val_pd: pandas Series, 验证标签\n",
    "    \"\"\"\n",
    "    # 将 pandas DataFrame 转换为 torch Tensor\n",
    "    x_train = torch.tensor(x_train_pd.values.astype('float32'), dtype=torch.float32)\n",
    "    x_val = torch.tensor(x_val_pd.values.astype('float32'), dtype=torch.float32)\n",
    "\n",
    "    # 将 pandas Series 转换为 torch Tensor，并进行 one-hot 编码\n",
    "    y_train = torch.tensor(y_train_pd.values.astype('int64'), dtype=torch.int64).reshape(-1)\n",
    "    y_val = torch.tensor(y_val_pd.values.astype('int64'), dtype=torch.int64).reshape(-1)\n",
    "\n",
    "    # 进行 one-hot 编码\n",
    "    # y_train_one_hot = torch.nn.functional.one_hot(y_train, num_classes=num_classes).float()\n",
    "    # y_val_one_hot = torch.nn.functional.one_hot(y_val, num_classes=num_classes).float()\n",
    "    y_train_one_hot = y_train\n",
    "    y_val_one_hot = y_val\n",
    "\n",
    "    return x_train, y_train_one_hot, x_val, y_val_one_hot\n",
    "\n",
    "# 假设我们有 pandas 数据\n",
    "x_train_pd = xTr\n",
    "y_train_pd = yTr\n",
    "x_val_pd = xTe\n",
    "y_val_pd = yTe\n",
    "\n",
    "# 从 pandas 数据加载到 PyTorch 张量\n",
    "x_train, y_train, x_val, y_val = load_data_from_pandas(x_train_pd, y_train_pd, x_val_pd, y_val_pd)\n",
    "\n",
    "# 模型初始化\n",
    "input_dim = xTr.shape[1]  # 特征维度\n",
    "model = MLPModel(input_dim=input_dim, hidden_layers=[64, 32, 16, 8], output_dim=3, dropout_prob=0.2)\n",
    "\n",
    "# 训练模型\n",
    "trained_model = train_model(model, x_train, y_train, x_val, y_val, epochs=1500, batch_size=512, learning_rate=1e-3, early_stopping=True, patience=100)\n",
    "\n",
    "# 评估模型的 top-1 和 top-2 准确率\n",
    "top_1_accuracy, top_2_accuracy = compute_top_k_accuracy(trained_model, x_val, y_val, k=2)\n",
    "print(f\"Top-1 Accuracy: {top_1_accuracy:.4f}\")\n",
    "print(f\"Top-2 Accuracy: {top_2_accuracy:.4f}\")\n",
    "\n"
   ],
   "id": "c4ea8b9708c0baa4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 优化后的MLP",
   "id": "96e71009b96663b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers=[128, 64, 32], output_dim=3, dropout_prob=0.4):\n",
    "        super(MLPModel, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Dropout(dropout_prob))  # 输入 Dropout\n",
    "        \n",
    "        in_dim = input_dim\n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))  # 添加 BatchNorm\n",
    "            layers.append(nn.GELU())  # 更好的激活函数\n",
    "            layers.append(nn.Dropout(dropout_prob))  # Dropout 层\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        layers.append(nn.Linear(in_dim, output_dim))\n",
    "        layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x) * 2  # 归一化\n",
    "        return self.network(x)  # 归一化\n",
    "\n",
    "# 训练时添加学习率调度器\n",
    "def train_model(model, x_train, y_train, x_val, y_val, epochs=100, batch_size=512, \n",
    "                learning_rate=1e-4, early_stopping=True, patience=10):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1, reduction='mean')\n",
    "\n",
    "    # 批次处理数据\n",
    "    dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_train_loss = 100.0\n",
    "    best_model_weights = None\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        running_train_loss = 0.0\n",
    "        model.train()\n",
    "        for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            if epoch % 50 == 0:\n",
    "                print('labels:', labels)\n",
    "                print('outputs:', outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # 计算训练集上的平均损失\n",
    "        train_loss = running_train_loss / len(dataloader)\n",
    "        \n",
    "        scheduler.step()  # 更新学习率\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(x_val)\n",
    "            val_loss = criterion(val_outputs, y_val).item()\n",
    "            \n",
    "            # 保存训练损失和验证损失\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # 打印当前的训练损失和验证损失\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        if early_stopping == True:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_weights = model.state_dict()\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "        else:\n",
    "            if (train_loss - best_train_loss) < -best_train_loss * 1e-2:\n",
    "                best_train_loss = train_loss\n",
    "                best_model_weights = model.state_dict()\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(\"normal stopping.\")\n",
    "                    break\n",
    "\n",
    "    if best_model_weights:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "        \n",
    "    # 绘制训练损失和验证损失的曲线\n",
    "    plot_loss_curve(train_losses, val_losses)\n",
    "\n",
    "    return model\n",
    "\n",
    "# 计算 top-k 准确率\n",
    "def compute_top_k_accuracy(model, x_test, y_test, k=2):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x_test)\n",
    "        _, top_k_indices = torch.topk(outputs, k, dim=1, largest=True, sorted=True)\n",
    "        correct_top_1 = torch.sum(top_k_indices[:, 0] == y_test).item()\n",
    "        correct_top_2 = torch.sum((top_k_indices[:, 0] == y_test) | (top_k_indices[:, 1] == y_test)).item()\n",
    "        top_1_accuracy = correct_top_1 / len(y_test)\n",
    "        top_2_accuracy = correct_top_2 / len(y_test)\n",
    "    return top_1_accuracy, top_2_accuracy\n",
    "\n",
    "# 绘制训练和验证损失的曲线\n",
    "def plot_loss_curve(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', color='red', linestyle='--', marker='x')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 从 pandas 数据加载训练和验证数据\n",
    "def load_data_from_pandas(x_train_pd, y_train_pd, x_val_pd, y_val_pd, num_classes=3):\n",
    "    \"\"\"\n",
    "    将 pandas 数据加载为 torch 张量\n",
    "    x_train_pd: pandas DataFrame, 训练数据\n",
    "    y_train_pd: pandas Series, 训练标签\n",
    "    x_val_pd: pandas DataFrame, 验证数据\n",
    "    y_val_pd: pandas Series, 验证标签\n",
    "    \"\"\"\n",
    "    # 将 pandas DataFrame 转换为 torch Tensor\n",
    "    x_train = torch.tensor(x_train_pd.values.astype('float32'), dtype=torch.float32)\n",
    "    x_val = torch.tensor(x_val_pd.values.astype('float32'), dtype=torch.float32)\n",
    "\n",
    "    # 将 pandas Series 转换为 torch Tensor，并进行 one-hot 编码\n",
    "    y_train = torch.tensor(y_train_pd.values.astype('int64'), dtype=torch.int64).reshape(-1)\n",
    "    y_val = torch.tensor(y_val_pd.values.astype('int64'), dtype=torch.int64).reshape(-1)\n",
    "\n",
    "    # 进行 one-hot 编码\n",
    "    # y_train_one_hot = torch.nn.functional.one_hot(y_train, num_classes=num_classes).float()\n",
    "    # y_val_one_hot = torch.nn.functional.one_hot(y_val, num_classes=num_classes).float()\n",
    "    y_train_one_hot = y_train\n",
    "    y_val_one_hot = y_val\n",
    "\n",
    "    return x_train, y_train_one_hot, x_val, y_val_one_hot\n",
    "\n",
    "# 假设我们有 pandas 数据\n",
    "x_train_pd = xTr\n",
    "y_train_pd = yTr\n",
    "x_val_pd = xTe\n",
    "y_val_pd = yTe\n",
    "\n",
    "# 从 pandas 数据加载到 PyTorch 张量\n",
    "x_train, y_train, x_val, y_val = load_data_from_pandas(x_train_pd, y_train_pd, x_val_pd, y_val_pd)\n",
    "\n",
    "# 模型初始化\n",
    "input_dim = xTr.shape[1]  # 特征维度\n",
    "model = MLPModel(input_dim=input_dim, hidden_layers=[128, 64, 32, 16], output_dim=3, dropout_prob=0.0)\n",
    "\n",
    "# 训练模型\n",
    "trained_model = train_model(model, x_train, y_train, x_val, y_val, epochs=1500, batch_size=512, learning_rate=1e-4, early_stopping=False, patience=100)\n",
    "\n",
    "# 评估模型的 top-1 和 top-2 准确率\n",
    "top_1_accuracy, top_2_accuracy = compute_top_k_accuracy(trained_model, x_val, y_val, k=2)\n",
    "print(f\"Top-1 Accuracy: {top_1_accuracy:.4f}\")\n",
    "print(f\"Top-2 Accuracy: {top_2_accuracy:.4f}\")"
   ],
   "id": "9f7ada5fd3b380a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [128/1500], Train Loss: 0.5718, Validation Loss: 1.1372\n",
      "Epoch [129/1500], Train Loss: 0.5707, Validation Loss: 1.1403\n",
      "Epoch [130/1500], Train Loss: 0.5679, Validation Loss: 1.1370\n",
      "Epoch [131/1500], Train Loss: 0.5646, Validation Loss: 1.1443\n",
      "Epoch [132/1500], Train Loss: 0.5625, Validation Loss: 1.1419\n",
      "Epoch [133/1500], Train Loss: 0.5596, Validation Loss: 1.1424\n",
      "Epoch [134/1500], Train Loss: 0.5575, Validation Loss: 1.1437\n",
      "Epoch [135/1500], Train Loss: 0.5564, Validation Loss: 1.1435\n",
      "Epoch [136/1500], Train Loss: 0.5545, Validation Loss: 1.1436\n",
      "Epoch [137/1500], Train Loss: 0.5542, Validation Loss: 1.1437\n",
      "Epoch [138/1500], Train Loss: 0.5532, Validation Loss: 1.1460\n",
      "Epoch [139/1500], Train Loss: 0.5526, Validation Loss: 1.1467\n",
      "Epoch [140/1500], Train Loss: 0.5506, Validation Loss: 1.1466\n",
      "Epoch [141/1500], Train Loss: 0.5505, Validation Loss: 1.1466\n",
      "Epoch [142/1500], Train Loss: 0.5527, Validation Loss: 1.1473\n",
      "Epoch [143/1500], Train Loss: 0.5480, Validation Loss: 1.1491\n",
      "Epoch [144/1500], Train Loss: 0.5501, Validation Loss: 1.1477\n",
      "Epoch [145/1500], Train Loss: 0.5476, Validation Loss: 1.1459\n",
      "Epoch [146/1500], Train Loss: 0.5481, Validation Loss: 1.1478\n",
      "Epoch [147/1500], Train Loss: 0.5487, Validation Loss: 1.1462\n",
      "Epoch [148/1500], Train Loss: 0.5481, Validation Loss: 1.1470\n",
      "Epoch [149/1500], Train Loss: 0.5487, Validation Loss: 1.1465\n",
      "Epoch [150/1500], Train Loss: 0.5492, Validation Loss: 1.1466\n",
      "labels: tensor([0, 2, 1, 2, 2, 2, 2, 1, 0, 2, 0, 1, 2, 1, 1, 1, 2, 2, 0, 2, 1, 0, 1, 0,\n",
      "        0, 2, 1, 1, 2, 0, 0, 0, 1, 0, 0, 2, 0, 2, 2, 1, 2, 2, 2, 1, 0, 0, 2, 2,\n",
      "        2, 0, 1, 2, 1, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 0, 1, 1, 2, 2, 0, 2, 0, 0,\n",
      "        2, 0, 2, 2, 2, 2, 0, 0, 1, 0, 2, 2, 1, 2, 0, 2, 0, 1, 2, 2, 1, 1, 1, 2,\n",
      "        0, 2, 2, 2, 0, 2, 1, 0, 0, 2, 1, 2, 0, 2, 1, 0, 1, 2, 2, 2, 0, 2, 0, 0,\n",
      "        2, 2, 0, 0, 2, 0, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 1,\n",
      "        0, 1, 0, 0, 2, 2, 1, 2, 1, 1, 2, 0, 2, 2, 2, 0, 0, 1, 1, 1, 2, 0, 2, 2,\n",
      "        1, 0, 0, 2, 1, 2, 2, 0, 2, 0, 0, 2, 2, 2, 1, 2, 2, 0, 0, 0, 1, 0, 2, 0,\n",
      "        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 2, 2, 0, 2, 1, 2, 2, 2, 0, 0, 1, 2, 2, 2,\n",
      "        0, 0, 1, 0, 2, 2, 2, 1, 1, 2, 0, 0, 2, 2, 1, 0, 1, 2, 0, 2, 0, 0, 0, 1,\n",
      "        1, 0, 2, 1, 1, 2, 1, 0, 2, 0, 2, 0, 2, 1, 2, 1, 2, 0, 0, 0, 2, 1, 1, 2,\n",
      "        2, 1, 0, 2, 2, 2, 0, 1, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 1, 0, 2, 0, 1,\n",
      "        0, 1, 2, 1, 0, 0, 2, 2, 1, 2, 2, 1, 2, 0, 2, 1, 2, 2, 0, 0, 2, 2, 0, 1,\n",
      "        1, 1, 0, 1, 2, 1, 0, 0, 2, 2, 2, 1, 0, 2, 0, 0, 2, 0, 0, 1, 2, 0, 1, 1,\n",
      "        2, 2, 0, 2, 0, 1, 1, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 2, 0, 1, 2, 0, 0, 2, 1, 0, 2, 0, 2, 0, 2, 0, 2, 2, 2, 2, 0, 2,\n",
      "        2, 2, 2, 0, 2, 2, 2, 0, 2, 0, 2, 2, 0, 0, 0, 2, 2, 0, 0, 0, 2, 1, 0, 0,\n",
      "        2, 2, 1, 2, 2, 0, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 1, 2, 0, 2, 2, 2, 2, 0,\n",
      "        2, 2, 2, 2, 1, 0, 1, 2, 0, 2, 2, 0, 0, 1, 2, 0, 2, 2, 1, 1, 1, 2, 1, 0,\n",
      "        2, 2, 2, 0, 2, 2, 0, 0, 2, 0, 0, 1, 2, 0, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2,\n",
      "        2, 0, 0, 0, 1, 2, 1, 1, 0, 0, 0, 2, 0, 0, 1, 2, 0, 2, 0, 2, 2, 0, 2, 1,\n",
      "        2, 2, 2, 2, 2, 0, 2, 0])\n",
      "outputs: tensor([[1.4585, 0.3989, 0.1426],\n",
      "        [0.2507, 0.6752, 1.0740],\n",
      "        [0.4501, 0.8818, 0.6681],\n",
      "        ...,\n",
      "        [1.8330, 0.1183, 0.0487],\n",
      "        [0.1137, 0.2706, 1.6157],\n",
      "        [1.8326, 0.0855, 0.0819]], grad_fn=<MulBackward0>)\n",
      "labels: tensor([2, 1, 0, 0, 0, 0, 2, 2, 0, 1, 1, 0, 0, 2, 1, 2, 0, 2, 1, 2, 2, 1, 0, 0,\n",
      "        1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 0,\n",
      "        1, 2, 0, 2, 1, 0, 1, 0, 2, 2, 2, 0, 1, 1, 1, 0, 2, 1, 2, 1, 2, 2, 0, 2,\n",
      "        2, 0, 2, 2, 2, 0, 2, 0, 2, 0, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 0, 2, 2,\n",
      "        1, 2, 0, 0, 0, 1, 0, 1, 2, 2, 1, 1, 0, 0, 0, 2, 0, 2, 2, 2, 0, 2, 2, 1,\n",
      "        1, 0, 0, 1, 2, 0, 2, 2, 0, 2, 0, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2,\n",
      "        0, 0, 0, 2, 0, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 1, 1, 2,\n",
      "        2, 0, 2, 0, 1, 2, 2, 0, 1, 2, 0, 2, 0, 1, 0, 2, 0, 1, 2, 1, 0, 0, 2, 1,\n",
      "        2, 2, 2, 0, 0, 2, 2, 2, 1, 0, 1, 1, 1, 2, 2, 0, 0, 0, 2, 2, 0, 1, 2, 0,\n",
      "        2, 0, 2, 2, 2, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 0, 1,\n",
      "        1, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 1, 1, 2, 0, 2, 0, 0, 2, 2, 1,\n",
      "        1, 2, 0, 2, 2, 1, 2, 1, 1, 1, 2, 2, 0, 0, 2, 1, 0, 2, 2, 1, 0, 2, 0, 2,\n",
      "        0, 2, 2, 1, 0, 2, 0, 0, 2, 2, 2, 1, 2, 2, 2, 0, 0, 2, 0, 2, 2, 1, 1, 2,\n",
      "        0, 2, 1, 2, 0, 2, 1, 2, 2, 2, 2, 1, 0, 2, 2, 2, 1, 0, 2, 0, 0, 0, 2, 2,\n",
      "        1, 1, 2, 0, 2, 2, 2, 0, 2, 1, 2, 2, 0, 0, 0, 2, 2, 0, 0, 2, 0, 2, 2, 2,\n",
      "        2, 0, 1, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 0, 2, 1, 2,\n",
      "        2, 2, 2, 0, 1, 0, 2, 2, 0, 1, 2, 1, 2, 2, 0, 0, 1, 1, 2, 0, 2, 2, 1, 1,\n",
      "        2, 0, 1, 2, 0, 1, 2, 2, 1, 2, 2, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 2, 2, 1,\n",
      "        0, 2, 0, 0, 2, 2, 0, 2, 2, 0, 0, 1, 0, 2, 1, 2, 0, 0, 2, 2, 2, 2, 2, 2,\n",
      "        0, 2, 1, 1, 2, 2, 1, 2, 0, 2, 2, 2, 2, 0, 0, 2, 1, 1, 0, 2, 0, 0, 0, 0,\n",
      "        2, 1, 2, 2, 2, 0, 1, 1, 2, 2, 0, 1, 1, 0, 0, 1, 0, 1, 2, 2, 2, 2, 0, 2,\n",
      "        2, 2, 1, 0, 2, 1, 2, 1])\n",
      "outputs: tensor([[0.1407, 0.0601, 1.7993],\n",
      "        [1.0033, 0.2526, 0.7440],\n",
      "        [1.8800, 0.0399, 0.0800],\n",
      "        ...,\n",
      "        [0.3265, 0.3413, 1.3322],\n",
      "        [0.0343, 0.5200, 1.4456],\n",
      "        [0.4480, 0.8844, 0.6676]], grad_fn=<MulBackward0>)\n",
      "labels: tensor([1, 2, 0, 1, 1, 0, 0, 2, 1, 0, 2, 0, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 0, 1,\n",
      "        0, 2, 0, 1, 0, 0, 2, 2, 2, 0, 2, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 2, 2, 0,\n",
      "        2, 1, 0, 2, 0, 2, 1, 0, 2, 0, 0, 2, 2, 0, 2, 1, 1, 2, 1, 2, 2, 2, 1, 0,\n",
      "        2, 0, 2, 1, 1, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 0,\n",
      "        0, 0, 0, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 2, 2, 2, 1, 0, 1, 2, 0, 1, 1,\n",
      "        2, 2, 0, 0, 0, 2, 1, 2, 0, 0, 2, 0, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 0, 1,\n",
      "        2, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 0, 2, 2, 1, 1, 2, 0, 2, 2, 1, 1, 2, 2,\n",
      "        1, 1, 1, 0, 2, 1, 2, 1, 0, 0, 2, 2, 0, 0, 0, 0, 2, 2, 2, 0, 2, 2, 2, 2,\n",
      "        0, 0, 0, 1, 0, 2, 2, 0, 0, 2, 2, 2, 0, 2, 2, 0, 2, 1, 2, 1, 2, 2, 2, 0,\n",
      "        2, 0, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 0, 2, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
      "        2, 0, 2, 0, 0, 0, 2, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 2, 2, 2, 1, 2, 0, 2,\n",
      "        0, 2, 0, 2, 2, 0, 2, 2, 2, 1, 2, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "        0, 1, 0, 0, 1, 0, 2, 1, 1, 1, 2, 1, 2, 1, 0, 2, 0, 0, 0, 0, 1, 1, 2, 0,\n",
      "        1, 1, 0, 2, 0, 2, 0, 1, 0, 0, 2, 2, 1, 1, 2, 1, 0, 0, 2, 0, 0, 2, 1, 0,\n",
      "        2, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1,\n",
      "        0, 0, 1, 2, 2, 1, 1, 2, 0, 1, 2, 0, 2, 1, 0, 1, 1, 0, 0, 0, 0, 2, 2, 2,\n",
      "        1, 2, 2, 0, 0, 2, 1, 2, 1, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 0, 2, 1, 2,\n",
      "        2, 2, 0, 0, 1, 0, 0, 2, 0, 0, 2, 1, 1, 0, 2, 0, 1, 1, 2, 2, 2, 2, 0, 2,\n",
      "        0, 1, 2, 2, 2, 1, 0, 0, 2, 2, 2, 2, 1, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2,\n",
      "        0, 2, 1, 2, 2, 0, 0, 0, 2, 1, 2, 2, 1, 1, 0, 0, 0, 2, 2, 2, 0, 2, 1, 1,\n",
      "        1, 0, 2, 2, 2, 2, 1, 0, 0, 2, 0, 0, 1, 1, 1, 0, 2, 2, 1, 2, 0, 0, 0, 2,\n",
      "        2, 0, 2, 0, 2, 1, 2, 2])\n",
      "outputs: tensor([[0.0212, 1.7618, 0.2170],\n",
      "        [0.5856, 0.8792, 0.5353],\n",
      "        [1.7333, 0.2317, 0.0349],\n",
      "        ...,\n",
      "        [0.1276, 1.5136, 0.3588],\n",
      "        [0.1836, 0.1855, 1.6309],\n",
      "        [0.0298, 0.0667, 1.9036]], grad_fn=<MulBackward0>)\n",
      "labels: tensor([2, 2, 0, 2, 1, 0, 0, 1, 0, 0, 2, 0, 0, 2, 0, 2, 2, 2, 1, 0, 2, 0, 2, 2,\n",
      "        2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 0, 0, 2, 0, 0, 0, 1, 2,\n",
      "        2, 1, 2, 2, 2, 2, 2, 2, 0, 2, 1, 1, 1, 1, 2, 0, 2, 1, 2, 1, 0, 2, 2, 0,\n",
      "        2, 0, 2, 0, 2, 1, 0, 0, 2, 0, 2, 0, 2, 2, 0, 2, 2, 1, 0, 1, 1, 0, 1, 0,\n",
      "        1, 2, 2, 2, 2, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 2, 2, 1, 2, 1, 2, 0,\n",
      "        1, 2, 2, 1, 2, 0, 2, 1, 2, 0, 2, 2, 0, 2, 2, 1, 2, 0, 1, 1, 0, 0, 0, 0,\n",
      "        2, 2, 0, 1, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 1, 2, 0, 2, 2, 1, 2, 1, 1, 1,\n",
      "        0, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 0, 1, 0, 2, 2, 1, 2, 2, 2, 0, 1, 1,\n",
      "        2, 2, 1, 0, 2, 0, 2, 1, 2, 0, 1, 1, 2, 2, 2, 1, 2, 2, 2, 0, 2, 2, 0, 2,\n",
      "        2, 0, 2, 2, 2, 0, 0, 2, 1, 2, 2, 0, 1, 1, 1, 1, 1, 2, 0, 0, 0, 2, 0, 2,\n",
      "        1, 0, 2, 2, 0, 0, 2, 2, 2, 0, 0, 0, 2, 0, 2, 0, 2, 1, 0, 1, 1, 2, 0, 2,\n",
      "        1, 2, 2, 1, 2, 0, 0, 1, 2, 1, 0, 1, 2, 0, 0, 1, 2, 2, 0, 0, 2, 1, 0, 1,\n",
      "        0, 2, 2, 2, 2, 1, 2, 1, 2, 2, 0, 1, 2, 1, 0, 1, 2, 0, 0, 0, 0, 1, 2, 0,\n",
      "        1, 2, 2, 2, 1, 2, 2, 2, 2, 0, 0, 2, 2, 2, 0, 1, 1, 2, 0, 1, 2, 0, 0, 2,\n",
      "        2, 1, 1, 2, 1, 1, 1, 2, 2, 0, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 0, 0, 0, 0,\n",
      "        1, 2, 1, 2, 1, 2, 0, 2, 0, 2, 2, 0, 0, 2, 0, 0, 2, 2, 2, 0, 1, 1, 2, 2,\n",
      "        1, 2, 0, 2, 2, 2, 2, 2, 1, 2, 1, 2, 0, 2, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 1, 0, 1, 2, 2, 2, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2,\n",
      "        0, 2, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 1, 2, 0, 0, 0, 2, 2, 2, 2, 1, 1, 2,\n",
      "        2, 2, 1, 2, 2, 2, 2, 2, 0, 2, 1, 2, 0, 1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2,\n",
      "        2, 0, 0, 1, 2, 0, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 0, 2, 2, 0, 0, 1, 0, 1,\n",
      "        0, 2, 0, 2, 2, 1, 2, 1])\n",
      "outputs: tensor([[1.4081e-01, 2.6727e-01, 1.5919e+00],\n",
      "        [3.7216e-01, 6.0173e-01, 1.0261e+00],\n",
      "        [1.8168e+00, 8.7259e-02, 9.5892e-02],\n",
      "        ...,\n",
      "        [4.8718e-02, 1.9091e+00, 4.2157e-02],\n",
      "        [1.9870e+00, 1.1468e-02, 1.5472e-03],\n",
      "        [2.5034e-01, 1.5379e+00, 2.1176e-01]], grad_fn=<MulBackward0>)\n",
      "labels: tensor([2, 1, 0, 2, 2, 0, 1, 0, 0, 0, 1, 2, 0, 0, 2, 1, 1, 1, 2, 2, 0, 1, 1, 1,\n",
      "        0, 0, 0, 2, 2, 0, 1, 0, 2, 2, 2, 2, 1, 2, 0, 0, 0, 0, 0, 2, 0, 2, 1, 2,\n",
      "        1, 2, 1, 0, 0, 1, 2, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 1, 1, 2, 0, 2, 2, 0,\n",
      "        0, 2, 0, 0, 2, 0, 2, 0, 0, 0, 2, 1, 2, 0, 1, 1, 2, 2, 0, 0, 0, 1, 0, 1,\n",
      "        0, 2, 0, 0, 0, 2, 1, 2, 0, 2, 0, 2, 2, 2, 2, 1, 2, 0, 1, 0, 2, 2, 0, 1,\n",
      "        2, 2, 2, 2, 1, 2, 0, 2, 0, 1, 1, 0, 0, 2, 2, 0, 2, 0, 2, 2, 2, 1, 2, 2,\n",
      "        1, 2, 1, 1, 0, 1, 2, 0, 2, 0, 1, 2, 2, 1, 0, 1, 2, 2, 0, 0, 1, 0, 0, 0,\n",
      "        1, 2, 2, 0, 0, 1, 1, 1, 2, 2, 0, 2, 2, 0, 0, 2, 1, 2, 1, 0, 2, 1, 0, 0,\n",
      "        1, 0, 2, 0, 2, 0, 1, 0, 1, 2, 0, 1, 0, 2, 1, 0, 2, 2, 2, 1, 0, 2, 2, 2,\n",
      "        0, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 0, 0, 1, 2, 2, 2, 2, 1, 1, 0, 0, 0, 1,\n",
      "        1, 0, 1, 1, 1, 2, 1, 0, 1, 0, 0, 1, 0, 2, 2, 1, 1, 1, 2, 1, 2, 0, 0, 2,\n",
      "        0, 2, 2, 2, 0, 1, 2, 0, 2, 2, 0, 2, 2, 1, 1, 1, 2, 1, 0, 2, 2, 1, 1, 1,\n",
      "        1, 0, 2, 2, 0, 2, 2, 0, 0, 1, 1, 2, 0, 0, 1, 2, 0, 2, 1, 0, 2, 2, 2, 1,\n",
      "        2, 0, 0, 2, 0, 2, 1, 2, 0, 1, 0, 0, 1, 2, 1, 1, 1, 2, 2, 0, 0, 2, 1, 1,\n",
      "        1, 0, 0, 1, 2, 0, 1, 1, 0, 2, 2, 2, 2, 0, 2, 2, 1, 2, 2, 1, 2, 2, 0, 2,\n",
      "        2, 2, 2, 1, 1, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 0,\n",
      "        2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 1, 2, 1, 1, 1, 0, 1, 0, 0, 2, 0,\n",
      "        2, 2, 0, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 1, 0, 0, 1, 0, 0, 2, 2, 2, 2, 1,\n",
      "        2, 1, 0, 0, 1, 2, 1, 0, 0, 0, 2, 1, 2, 0, 1, 2, 2, 0, 2, 2, 2, 2, 2, 1,\n",
      "        0, 0, 1, 2, 2, 2, 1, 2, 2, 0, 1, 1, 2, 2, 1, 1, 1, 1, 0, 2, 0, 0, 2, 0,\n",
      "        2, 2, 2, 1, 1, 2, 0, 1, 0, 0, 2, 1, 0, 2, 2, 1, 0, 1, 0, 0, 2, 0, 2, 0,\n",
      "        2, 1, 1, 1, 2, 0, 0, 2])\n",
      "outputs: tensor([[0.0278, 0.1034, 1.8688],\n",
      "        [0.1775, 0.5272, 1.2952],\n",
      "        [1.8790, 0.0913, 0.0297],\n",
      "        ...,\n",
      "        [1.8338, 0.1094, 0.0569],\n",
      "        [1.6753, 0.2028, 0.1219],\n",
      "        [0.0365, 0.0325, 1.9310]], grad_fn=<MulBackward0>)\n",
      "labels: tensor([2, 0, 0, 1, 0, 2, 1, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 2, 1, 0, 1, 1,\n",
      "        2, 0, 2, 0, 2, 1, 2, 0, 1, 2, 2, 1, 0, 2, 0, 2, 2, 0, 2, 0, 1, 2, 2, 2,\n",
      "        1, 2, 0, 2, 1, 0, 0, 2, 2, 2, 0, 2, 1, 2, 2, 1, 0, 2, 2, 0, 1, 0, 2, 2,\n",
      "        1, 2, 2, 1, 2, 0, 0, 2, 2, 1, 2, 0, 2, 0, 2, 0, 2, 2, 2, 1, 1, 0, 0, 1,\n",
      "        2, 2, 0, 0, 1, 0, 1, 0, 1, 0, 2, 1, 2, 0, 0, 1, 0, 0, 0, 0, 2, 1, 2, 1,\n",
      "        2, 2, 2, 1, 0, 2, 2, 2, 1, 0, 2, 2, 2, 2, 2, 0, 0, 0, 1, 2, 1, 2, 2, 2,\n",
      "        2, 2, 1, 0, 2, 2, 1, 2, 0, 0, 2, 2, 2, 0, 2, 1, 1, 2, 0, 1, 2, 1, 2, 2,\n",
      "        0, 0, 2, 1, 2, 2, 0, 2, 0, 1, 0, 2, 2, 2, 1, 2, 1, 2, 2, 0, 1, 2, 2, 2,\n",
      "        0, 0, 2, 1, 1, 2, 0, 0, 2, 1, 0, 2, 2, 0, 2, 2, 0, 0, 2, 0, 2, 0, 2, 2,\n",
      "        2, 2, 1, 2, 2, 0, 1, 0, 0, 2, 2, 1, 0, 0, 1, 0, 1, 2, 2, 0, 0, 1, 0, 0,\n",
      "        2, 0, 1, 0, 2, 1, 0, 2, 1, 1, 2, 1, 0, 0, 2, 2, 1, 1, 0, 1, 2, 2, 2, 2,\n",
      "        0, 2, 1, 0, 2, 1, 2, 1, 2, 2, 2, 1, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 1,\n",
      "        2, 2, 2, 1, 1, 2, 0, 2, 2, 2, 0, 0, 2, 0, 1, 2, 1, 1, 2, 1, 0, 2, 1, 0,\n",
      "        0, 2, 0, 0, 2, 1, 0, 0, 1, 0, 1, 2, 2, 0, 2, 2, 2, 2, 2, 1, 0, 1, 2, 1,\n",
      "        2, 2, 2, 2, 0, 2, 0, 2, 2, 0, 1, 2, 2, 2, 0, 1, 0, 2, 2, 0, 1, 1, 2, 2,\n",
      "        0, 2, 0, 2, 1, 1, 2, 0, 2, 2, 2, 2, 2, 0, 0, 1, 2, 2, 1, 0, 2, 2, 2, 1,\n",
      "        0, 1, 2, 1, 2, 2, 2, 0, 2, 1, 0, 2, 1, 0, 1, 0, 2, 1, 0, 2, 1, 0, 1, 1,\n",
      "        1, 2, 2, 0, 2, 2, 2, 2, 2, 1, 0, 2, 2, 1, 1, 2, 1, 1, 0, 2, 0, 2, 2, 2,\n",
      "        1, 1, 2, 2, 2, 2, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 1, 0, 1, 1, 2, 2, 2, 0,\n",
      "        0, 0, 1, 2, 1, 2, 0, 0, 2, 2, 0, 0, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 0,\n",
      "        1, 1, 0, 2, 0, 2, 2, 0, 0, 0, 2, 2, 2, 1, 1, 1, 2, 1, 0, 2, 2, 0, 0, 2,\n",
      "        0, 0, 2, 2, 2, 0, 0, 0])\n",
      "outputs: tensor([[0.0482, 0.0583, 1.8935],\n",
      "        [1.5593, 0.1612, 0.2795],\n",
      "        [1.9651, 0.0181, 0.0168],\n",
      "        ...,\n",
      "        [1.7070, 0.1835, 0.1095],\n",
      "        [1.6212, 0.1804, 0.1983],\n",
      "        [1.9329, 0.0501, 0.0170]], grad_fn=<MulBackward0>)\n",
      "labels: tensor([0, 2, 0, 2, 0, 0, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 1,\n",
      "        1, 0, 1, 2, 0, 2, 2, 1, 0, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 0, 2,\n",
      "        0, 1, 1, 2, 1, 2, 1, 2, 1, 2, 0, 2, 2, 0, 2, 2, 0, 1, 1, 2, 2, 0, 1, 1,\n",
      "        1, 0, 1, 2, 1, 2, 1, 0, 1, 1, 2, 0, 2, 2, 0, 2, 2, 1, 1, 0, 2, 1, 0, 0,\n",
      "        0, 0, 0, 2, 2, 0, 0, 0, 2, 1, 1, 2, 0, 2, 2, 1, 2, 1, 2, 2, 2, 0, 2, 0,\n",
      "        1, 0, 0, 2, 1, 1, 2, 0, 2, 0, 1, 2, 1, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 2,\n",
      "        2, 0, 2, 2, 1, 2, 0, 0, 1, 2, 1, 2, 2, 1, 2, 2, 1, 0, 0, 0, 2, 0, 2, 1,\n",
      "        2, 1, 2, 0, 2, 0, 1, 1, 2, 1, 0, 2, 2, 2, 0, 0, 2, 2, 0, 2, 1, 1, 2, 0,\n",
      "        0, 2, 0, 1, 0, 0, 1, 1, 2, 1, 2, 0, 2, 0, 2, 1, 0, 2, 2, 1, 2, 1, 0, 2,\n",
      "        2, 2, 0, 2, 2, 1, 1, 1, 0, 2, 0, 0, 2, 2, 2, 0, 1, 1, 0, 1, 1, 2, 2, 1,\n",
      "        0, 1, 1, 2, 0, 0, 2, 2, 1, 2, 2, 1, 2, 1, 2, 0, 0, 2, 1, 1, 1, 2, 0, 2,\n",
      "        2, 2, 2, 2, 2, 1, 0, 2, 2, 0, 2, 2, 1, 2, 0, 0, 2, 0, 2, 1, 2, 0, 0, 2,\n",
      "        1, 1, 1, 2, 1, 2, 1, 2, 0, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 0, 2, 2,\n",
      "        0, 1, 2, 1, 2, 2, 1, 0, 0, 0, 1, 0, 0, 2, 1, 2, 0, 0, 2, 0, 1, 2, 2, 2,\n",
      "        2, 0, 0, 2, 0, 2, 1, 2, 1, 0, 1, 2, 0, 2, 0, 2, 2, 1, 1, 2, 0, 0, 2, 1,\n",
      "        0, 2, 0, 0, 1, 1, 1, 0, 2, 0, 2, 2, 0, 1, 1, 1, 0, 0, 2, 1, 2, 0, 2, 0,\n",
      "        0, 0, 1, 2, 2, 2, 1, 0, 2, 1, 2, 1, 2, 1, 1, 2, 0, 1, 1, 2, 2, 1, 1, 0,\n",
      "        0, 2, 0, 2, 2, 0, 0, 2, 1, 2, 2, 2, 0, 2, 1, 2, 2, 2, 1, 1, 2, 1, 0, 1,\n",
      "        2, 2, 0, 2, 2, 2, 0, 0, 0, 1, 0, 2, 2, 2, 1, 2, 1, 2, 0, 2, 0, 1, 2, 2,\n",
      "        0, 0, 1, 2, 0, 2, 1, 0, 0, 0, 2, 1, 0, 2, 2, 2, 0, 0, 1, 2, 0, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 1, 2, 1, 2, 2, 0, 0, 0, 2, 0, 0, 2, 1, 0,\n",
      "        1, 2, 1, 0, 2, 0, 1, 1])\n",
      "outputs: tensor([[0.5233, 0.9778, 0.4988],\n",
      "        [0.0523, 0.0611, 1.8866],\n",
      "        [1.7560, 0.1886, 0.0554],\n",
      "        ...,\n",
      "        [1.7241, 0.1485, 0.1273],\n",
      "        [0.1797, 1.7987, 0.0216],\n",
      "        [0.0540, 1.3920, 0.5540]], grad_fn=<MulBackward0>)\n",
      "labels: tensor([2, 0, 2, 0, 2, 2, 0, 2, 0, 0, 2, 1, 0, 1, 0, 2, 1, 0, 2, 1, 2, 2, 2, 0,\n",
      "        1, 2, 1, 2, 1, 0, 0, 0, 0, 2, 1, 2, 0, 2, 1, 1, 0, 1, 0, 1, 2, 2, 0, 2,\n",
      "        1, 2, 2, 2, 1, 2, 1, 2, 0, 1, 0, 2, 0, 2, 2, 2, 1, 1, 2, 2, 0, 0, 0, 0,\n",
      "        2, 2, 0, 0, 1, 2, 2, 1, 2, 2, 1, 0, 0, 1, 0, 0, 2, 0, 2, 0, 2, 2, 2, 1,\n",
      "        2, 2, 1, 0, 2, 1, 2, 1, 2, 0, 2, 0, 2, 2, 0, 0, 0, 2, 2, 2, 2, 0, 2, 1,\n",
      "        1, 1, 1, 0, 0, 1, 2, 2, 2, 2, 1, 0, 2, 0, 2, 1, 0, 2, 0, 1, 2, 2, 2, 0,\n",
      "        0, 1, 0, 1, 2, 2, 2, 2, 2, 2, 0, 1, 0, 2, 2, 2, 0, 0, 2, 1, 0, 1, 1, 0,\n",
      "        2, 1, 2, 0, 2, 0, 2, 2, 0, 1, 2, 0, 0, 2, 2, 1, 2, 0, 0, 1, 2, 2, 2, 0,\n",
      "        0, 2, 1, 1, 2, 0, 1, 0, 2, 1, 0, 2, 2, 2, 0, 0, 1, 0, 2, 0, 0, 2, 2, 2,\n",
      "        0, 0, 2, 2, 0, 0, 0, 1, 0, 0, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 0,\n",
      "        1, 2, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 0, 1, 2, 2, 2, 2, 0, 0, 0,\n",
      "        2, 0, 0, 2, 1, 1, 2, 2, 2, 1, 1, 2, 0, 0, 0, 2, 0, 2, 2, 1, 2, 2, 2, 1,\n",
      "        0, 2, 2, 1, 0, 2, 1, 1, 2, 1, 0, 1, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 0,\n",
      "        2, 2, 0, 2, 1, 0, 1, 2, 1, 2, 0, 1, 0, 1, 1, 0, 2, 1, 2, 1, 1, 2, 0, 2,\n",
      "        0, 0, 0, 2, 1, 2, 2, 2, 0, 0, 0, 1, 0, 2, 1, 2, 2, 0, 1, 1, 1, 2, 2, 0,\n",
      "        1, 2, 0, 2, 1, 0, 0, 2, 0, 1, 2, 2, 1, 1, 2, 1, 1, 0, 0, 1, 2, 0, 1, 2,\n",
      "        1, 0, 2, 1, 1, 2, 2, 2, 0, 0, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2,\n",
      "        0, 2, 0, 2, 0, 2, 1, 2, 2, 1, 2, 2, 0, 2, 2, 0, 0, 2, 1, 0, 0, 0, 2, 0,\n",
      "        0, 1, 1, 2, 1, 0, 1, 2, 0, 0, 1, 2, 0, 0, 0, 2, 0, 0, 2, 0, 2, 2, 2, 1,\n",
      "        1, 2, 0, 1, 1, 2, 1, 0, 2, 1, 0, 1, 2, 2, 1, 2, 2, 0, 2, 0, 2, 1, 1, 0,\n",
      "        2, 0, 2, 1, 0, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 0,\n",
      "        0, 2, 1, 2, 1, 2, 2, 1])\n",
      "outputs: tensor([[0.0739, 0.1433, 1.7828],\n",
      "        [1.9352, 0.0426, 0.0222],\n",
      "        [0.8813, 0.4874, 0.6312],\n",
      "        ...,\n",
      "        [0.0231, 0.0676, 1.9093],\n",
      "        [1.2307, 0.5945, 0.1748],\n",
      "        [0.2821, 1.6980, 0.0199]], grad_fn=<MulBackward0>)\n",
      "labels: tensor([2, 1, 1, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 1, 2, 0, 1, 1, 2,\n",
      "        1, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 2, 2, 0, 1, 2, 0,\n",
      "        0, 2, 2, 2, 0, 0, 1, 1, 2, 2, 1, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 2,\n",
      "        1, 2, 2, 2, 2, 0, 1, 0, 0, 0, 0, 1, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 2, 2,\n",
      "        0, 2, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 0, 1,\n",
      "        0, 2, 0, 0, 2, 0, 1, 2, 0, 2, 2, 0, 0, 2, 1, 0, 2, 2, 1, 0, 1, 1, 2, 0,\n",
      "        2, 1, 2, 0, 1, 0, 2, 1, 2, 2, 0, 2, 2, 2, 2, 0, 0, 2, 1, 1, 2, 0, 2, 2,\n",
      "        1, 1, 2, 2, 0, 0, 2, 1, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 0, 2,\n",
      "        1, 2, 0, 2, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1, 0, 1, 1, 2, 2, 1, 2, 1, 0, 0,\n",
      "        2, 0, 2, 2, 2, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0, 2, 1, 1, 2, 2, 2,\n",
      "        1, 0, 1, 1, 2, 0, 0, 2, 2, 1, 2, 0, 2, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 2,\n",
      "        2, 0, 2, 1, 0, 2, 0, 1, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 0, 0, 0, 0, 2, 1,\n",
      "        0, 2, 2, 0, 0, 1, 2, 2, 2, 2, 0, 1, 0, 0, 1, 2, 2, 0, 2, 2, 2, 1, 0, 0,\n",
      "        1, 1, 2, 0, 2, 0, 0, 0, 2, 2, 1, 2, 1, 1, 0, 1, 2, 2, 2, 0, 0, 2, 2, 1,\n",
      "        2, 2, 1, 1, 2, 0, 1, 2, 1, 1, 1, 0, 2, 1, 0, 0, 0, 2, 2, 2, 0, 2, 2, 0,\n",
      "        2, 0, 2, 1, 2, 1, 0, 1, 2, 2, 0, 2, 2, 0, 1, 2, 2, 1, 2, 0, 2, 2, 1, 0,\n",
      "        0, 2, 2, 1, 1, 2, 0, 2, 0, 1, 2, 1, 2, 2, 0, 2, 1, 0, 1, 2, 2, 2, 0, 2,\n",
      "        2, 2, 1, 1, 0, 1, 1, 0, 2, 2, 1, 2, 2, 1, 2, 0, 1, 1, 2, 2, 2, 1, 0, 1,\n",
      "        2, 2, 1, 0, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 2, 2, 2, 1, 2, 0, 1, 2, 1, 2,\n",
      "        1, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 2, 2, 1, 0, 0, 2, 0, 1, 0, 1, 1, 0, 2,\n",
      "        1, 0, 1, 2, 2, 2, 0, 1, 1, 2, 0, 1, 2, 1, 2, 0, 2, 0, 0, 1, 0, 2, 2, 2,\n",
      "        0, 0, 2, 2, 0, 1, 0, 1])\n",
      "outputs: tensor([[0.0485, 0.0628, 1.8887],\n",
      "        [0.0085, 1.9796, 0.0119],\n",
      "        [0.2474, 1.1405, 0.6121],\n",
      "        ...,\n",
      "        [0.0620, 1.9073, 0.0307],\n",
      "        [1.9656, 0.0160, 0.0183],\n",
      "        [0.3416, 1.5019, 0.1565]], grad_fn=<MulBackward0>)\n",
      "labels: tensor([1, 1, 2, 2, 2, 1, 1, 0, 2, 1, 1, 0, 0, 0, 1, 1, 0, 2, 1, 2, 2, 0, 2, 2,\n",
      "        1, 2, 2, 0, 2, 0, 2, 2, 2, 1, 1, 0, 1, 0, 2, 1, 0, 2, 2, 0, 1, 0, 2, 2,\n",
      "        0, 2, 1, 2, 1, 1, 1, 0, 0, 1, 1, 2, 0, 0, 0, 2, 2, 1, 1, 1, 2, 2, 1, 2,\n",
      "        2, 1, 2, 0, 0, 2, 2, 0, 2, 0, 1, 2, 0, 2, 0, 2, 1, 1, 2, 2, 0, 2, 1, 1,\n",
      "        1, 1, 0, 1, 1, 2, 2, 0, 2, 2, 1, 1, 0, 0, 0, 1, 2, 0, 2, 0, 2, 2, 2, 1,\n",
      "        0, 0, 0, 2, 2, 0, 0, 2, 0, 2, 1, 2, 2, 2, 0, 2, 1, 2, 2, 0, 0, 0, 0, 1,\n",
      "        1, 1, 2, 0, 0, 0, 2, 1, 1, 0, 1, 0, 2, 2, 1, 0, 0, 2, 2, 1, 2, 0, 2, 2,\n",
      "        0, 2, 1, 0, 0, 2, 0, 0, 1, 2, 2, 1, 1, 2, 2, 0, 1, 2, 0, 0, 2, 1, 0, 2,\n",
      "        0, 2, 0, 2, 1, 1, 0, 0, 2, 2, 0, 1, 0, 1, 1, 1, 0, 2, 0, 2, 2, 2, 1, 1,\n",
      "        1, 0, 2, 0, 2, 1, 2, 2, 0, 2, 0, 1, 2, 1, 0, 2, 0, 1, 1, 2, 0, 1, 2, 0,\n",
      "        0, 2, 0, 0, 2, 2, 1, 0, 0, 2, 0, 2, 1, 1, 1, 0, 0, 2, 0, 0, 2, 2, 2, 1,\n",
      "        2, 0, 2, 2, 2, 0, 2, 2, 0, 2, 0, 1, 2, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 2,\n",
      "        1, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 1, 0, 0, 1, 1, 0,\n",
      "        2, 1, 0, 2, 0, 0, 2, 1, 0, 2, 0, 2, 2, 2, 0, 1, 2, 2, 1, 2, 0, 2, 1, 1,\n",
      "        2, 1, 1, 0, 2, 0, 0, 1, 1, 1, 1, 1, 2, 0, 2, 2, 0, 2, 2, 2, 2, 1, 2, 0,\n",
      "        0, 0, 1, 2, 0, 2, 2, 2, 0, 2, 1, 2, 2, 0, 1, 2, 1, 1, 0, 2, 2, 2, 2, 2,\n",
      "        0, 2, 1, 0, 2, 2, 0, 2, 2, 0, 1, 1, 1, 0, 2, 2, 0, 1, 0, 0, 0, 1, 2, 0,\n",
      "        0, 1, 0, 1, 1, 0, 2, 1, 1, 1, 2, 0, 2, 0, 2, 1, 0, 0, 2, 0, 2, 2, 0, 0,\n",
      "        0, 0, 1, 0, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 2, 2, 1, 2, 0,\n",
      "        0, 2, 1, 2, 0, 1, 1, 0, 1, 1, 2, 0, 1, 1, 0, 2, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        0, 2, 2, 2, 2, 1, 2, 2, 2, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 2, 2, 0, 2, 1,\n",
      "        1, 2, 0, 0, 2, 1, 1, 1])\n",
      "outputs: tensor([[0.2576, 1.0594, 0.6830],\n",
      "        [0.1604, 1.3374, 0.5023],\n",
      "        [0.0220, 0.0132, 1.9648],\n",
      "        ...,\n",
      "        [0.5280, 1.2777, 0.1943],\n",
      "        [0.1970, 1.6850, 0.1180],\n",
      "        [0.2545, 1.4835, 0.2620]], grad_fn=<MulBackward0>)\n",
      "labels: tensor([2, 2, 2, 0, 0, 2, 0, 1, 0, 2, 0, 0, 2, 1, 1, 0, 2, 2, 2, 2, 0, 2, 2, 0,\n",
      "        2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 0, 2, 2, 1, 0, 2, 1, 1, 0,\n",
      "        0, 1, 1, 2, 2, 2, 2, 2, 1, 0, 1, 2, 2, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2,\n",
      "        1, 0, 2, 2, 1, 2, 1, 1, 0, 2, 0, 0, 0, 2, 0, 1, 1, 0, 0, 2, 0, 2, 0, 2,\n",
      "        0, 2, 0, 1, 1, 0, 0, 2, 0, 2, 1, 0, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 1, 2,\n",
      "        0, 0, 1, 2, 1, 0, 2, 1, 0, 2, 0, 2, 2, 2, 2, 1, 2, 1, 0, 0, 1, 0, 2, 0,\n",
      "        0, 2, 0, 1, 2, 0, 1, 1, 1, 2, 0, 0, 2, 2, 0, 2, 2, 0, 2, 0, 2, 2, 0, 2,\n",
      "        1, 1, 1, 0, 0, 2, 2, 2, 2, 1, 0, 1, 0, 2, 0, 2, 0, 1, 0, 1, 2, 2, 1, 2,\n",
      "        0, 1, 2, 2, 1, 2, 1, 2, 2, 2, 0, 0, 0, 2, 2, 1, 0, 2, 1, 2, 2, 1, 2, 2,\n",
      "        1, 2, 0, 2, 1, 2, 1, 0, 2, 2, 0, 1, 0, 2, 2, 1, 0, 2, 1, 0, 1, 2, 1, 0,\n",
      "        2, 0, 0, 1, 1, 0, 1, 1, 2, 0, 0, 1, 2, 0, 1, 0, 2, 0, 0, 2, 1, 0, 0, 0,\n",
      "        2, 1, 2, 0, 0, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 0, 0, 2, 1, 2, 1, 0,\n",
      "        0, 2, 1, 1, 1, 0, 0, 2, 0, 0, 2, 2, 0, 0, 1, 0, 0, 2, 0, 2, 1, 1, 1, 2,\n",
      "        2, 0, 0, 1, 2, 0, 0, 2, 0, 0, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 0, 2, 2, 0,\n",
      "        2, 1, 0, 0, 2, 0, 2, 1, 0, 2, 2, 1, 0, 1, 0, 2, 1, 0, 1, 2, 0, 2, 0, 1,\n",
      "        2, 0, 0, 1, 0, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 0, 1, 1, 0, 0, 2, 0,\n",
      "        1, 1, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0, 0, 2, 1, 1, 1, 2, 0, 2, 1, 0, 1, 0,\n",
      "        0, 2, 0, 0, 1, 0, 1, 2, 0, 0, 0, 2, 0, 2, 0, 0, 1, 2, 1, 2, 2, 2, 2, 2,\n",
      "        0, 0, 2, 2, 0, 2, 0, 0, 0, 1, 2, 1, 0, 2, 2, 0, 2, 2, 0, 2, 1, 1, 0, 2,\n",
      "        2, 0, 1, 2, 0, 0, 2, 2, 1, 2, 1, 1, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 2, 0, 0, 2, 1, 0, 0, 1, 0, 2, 0, 2, 0, 0, 2, 0, 1, 2, 0, 0, 2, 1,\n",
      "        2, 2, 0, 2, 2, 2, 2, 1])\n",
      "outputs: tensor([[0.1397, 0.2736, 1.5866],\n",
      "        [0.0596, 0.0505, 1.8900],\n",
      "        [0.1372, 0.0575, 1.8052],\n",
      "        ...,\n",
      "        [0.1734, 0.4755, 1.3511],\n",
      "        [0.0821, 0.0491, 1.8688],\n",
      "        [0.1803, 1.4894, 0.3303]], grad_fn=<MulBackward0>)\n",
      "labels: tensor([2, 0, 0, 2, 2, 1, 2, 2, 0, 0, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 0, 0, 0,\n",
      "        1, 0, 2, 1, 0, 0, 2, 2, 2, 2, 2, 0, 1, 2, 2, 0, 1, 2, 2, 0, 2, 1, 0, 1,\n",
      "        2, 2, 0, 0, 1, 0, 1, 0, 2, 0, 1, 2, 1, 2, 2, 0, 0, 1, 0, 0, 1, 2, 1, 2,\n",
      "        2, 0, 2, 0, 1, 0, 0, 0, 2, 1, 2, 1, 0, 1, 0, 2, 0, 0, 2, 2, 0, 2, 2, 0,\n",
      "        1, 2, 2, 1, 2, 0, 2, 1, 0, 1, 1, 2, 0, 2, 0, 1, 2, 1, 2, 2, 0, 2, 1, 1,\n",
      "        1, 2, 2, 0, 2, 1, 1, 2, 1, 0, 0, 0, 2, 2, 2, 2, 2, 0, 2, 1, 2, 0, 1, 0,\n",
      "        1, 1, 2, 0, 1, 1, 2, 0, 0, 2, 2, 0, 1, 1, 1, 0, 2, 2, 1, 0, 2, 1, 2, 0,\n",
      "        0, 0, 2, 1, 2, 1, 1, 0, 1, 1, 0, 2, 1, 2, 1, 2, 2, 0, 0, 2, 0, 0, 2, 2,\n",
      "        2, 2, 0, 1, 0, 0, 1, 1, 2, 2, 2, 1, 2, 2, 0, 1, 1, 0, 1, 2, 1, 1, 1, 1,\n",
      "        2, 1, 1, 1, 2, 1, 2, 0, 0, 2, 2, 0, 1, 2, 2, 0, 0, 0, 2, 2, 1, 0, 1, 1,\n",
      "        1, 0, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 2, 0, 1, 1, 1, 1, 0, 2, 1, 1, 2,\n",
      "        0, 0, 2, 2, 0, 0, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 1, 2, 2, 2, 1, 0, 2, 1,\n",
      "        2, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2, 1, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 1, 0,\n",
      "        2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 2, 2, 0, 2, 1, 1, 0, 2, 2, 1, 0, 2, 2, 0,\n",
      "        2, 0, 2, 0, 2, 2, 0, 1, 1, 2, 0, 0, 2, 1, 0, 1, 0, 1, 0, 2, 0, 2, 2, 1,\n",
      "        2, 2, 2, 2, 2, 0, 2, 0, 1, 1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 2,\n",
      "        2, 0, 2, 1, 2, 1, 0, 0, 0, 0, 2, 0, 2, 2, 1, 0, 1, 2, 2, 0, 0, 2, 0, 0,\n",
      "        2, 0, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2,\n",
      "        0, 0, 1, 0, 2, 2, 0, 2, 0, 0, 2, 2, 1, 2, 2, 2, 0, 1, 1, 0, 2, 2, 2, 2,\n",
      "        0, 1, 2, 0, 2, 0, 0, 1, 1, 0, 2, 2, 2, 2, 0, 1, 2, 0, 0, 2, 0, 2, 1, 0,\n",
      "        2, 1, 2, 2, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 1,\n",
      "        2, 1, 0, 2, 1, 2, 1, 2])\n",
      "outputs: tensor([[0.4956, 0.3616, 1.1428],\n",
      "        [1.7870, 0.1694, 0.0436],\n",
      "        [1.4895, 0.3867, 0.1238],\n",
      "        ...,\n",
      "        [0.1171, 0.0930, 1.7899],\n",
      "        [0.3267, 0.8478, 0.8254],\n",
      "        [0.0344, 0.1485, 1.8170]], grad_fn=<MulBackward0>)\n",
      "labels: tensor([0, 0, 1, 1, 2, 0, 1, 2, 0, 1, 0, 2, 2, 0, 2, 2, 1, 0, 2, 1, 2, 0, 1, 2,\n",
      "        2, 1, 2, 1, 0, 2, 0, 0, 2, 1, 2, 1, 2, 1, 0, 0, 0, 2, 2, 1, 2, 2, 0, 1,\n",
      "        2, 0, 2, 1, 2, 2, 0, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 0, 2, 0, 1, 2, 2,\n",
      "        2, 1, 1, 0, 2, 2, 2, 1, 2, 2, 1, 1, 0, 0, 1, 2, 2, 2, 1, 0, 2, 0, 2, 0,\n",
      "        2, 2, 2, 2, 0, 0, 2, 1, 0, 1, 1, 0, 2, 0, 0, 0, 0, 2, 2, 0, 2, 2, 1, 1,\n",
      "        0, 2, 1, 2, 1, 2, 1, 2, 2, 0, 2, 1, 2, 2, 2, 0, 0, 1, 0, 1, 2, 1, 1, 2,\n",
      "        1, 2, 0, 0, 2, 1, 0, 0, 2, 2, 1, 0, 2, 0, 0, 0, 2, 2, 2, 2, 1, 2, 1, 0,\n",
      "        2, 2, 0, 2, 1, 2, 2, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 1, 2,\n",
      "        1, 2, 0, 2, 2, 0, 2, 2, 0, 0, 0, 1, 2, 1, 1, 2, 2, 0, 1, 2, 1, 2, 0, 0,\n",
      "        2, 1, 1, 2, 0, 1, 2, 2, 0, 1, 2, 2, 0, 2, 2, 2, 1, 0, 2, 2, 2, 1, 0, 2,\n",
      "        0, 1, 0, 2, 0, 0, 0, 2, 1, 1, 2, 2, 2, 1, 0, 2, 0, 0, 2, 2, 1, 1, 2, 0,\n",
      "        0, 1, 0, 0, 0, 1, 1, 2, 0, 2, 0, 2, 2, 0, 2, 2, 1, 1, 0, 0, 0, 1, 2, 2,\n",
      "        1, 0, 1, 2, 0, 1, 2, 0, 2, 1, 0, 1, 1, 2, 2, 1, 2, 0, 0, 2, 2, 2, 2, 0,\n",
      "        1, 0, 2, 0, 2, 0, 0, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2,\n",
      "        0, 1, 1, 1, 0, 1, 2, 1, 2, 0, 1, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 2, 1, 2,\n",
      "        0, 2, 0, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 0, 1, 2, 2, 2, 0, 0, 2,\n",
      "        2, 0, 0, 1, 0, 2, 0, 2, 1, 2, 0, 1, 0, 2, 2, 1, 1, 2, 2, 2, 1, 0, 2, 1,\n",
      "        0, 0, 2, 0, 1, 0, 2, 0, 0, 2, 0, 2, 2, 0, 1, 2, 2, 2, 1, 1, 0, 2, 1, 0,\n",
      "        1, 0, 0, 1, 1, 2, 1, 1, 0, 0, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 0, 2, 2, 1,\n",
      "        2, 0, 1, 0, 2, 1, 0, 2, 0, 0, 0, 0, 2, 0, 2, 1, 2, 0, 1, 1, 2, 0, 1, 1,\n",
      "        2, 2, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 0, 1, 2, 0, 0, 2, 2, 2, 1, 0, 2, 0,\n",
      "        1, 1, 0, 1, 2, 2, 2, 2])\n",
      "outputs: tensor([[1.6646e+00, 2.5071e-01, 8.4700e-02],\n",
      "        [1.9954e+00, 3.9196e-03, 6.3306e-04],\n",
      "        [3.4868e-02, 1.7865e+00, 1.7859e-01],\n",
      "        ...,\n",
      "        [2.7095e-02, 1.6669e-01, 1.8062e+00],\n",
      "        [1.5037e-02, 1.6158e-01, 1.8234e+00],\n",
      "        [3.2906e-02, 5.0846e-02, 1.9162e+00]], grad_fn=<MulBackward0>)\n",
      "labels: tensor([2, 2, 0, 1, 1, 2, 1, 2, 0, 0, 2, 0, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 2, 0,\n",
      "        2, 0, 2, 2, 0, 0, 1, 2, 0, 0, 2, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 1,\n",
      "        0, 2, 2, 0, 2, 0, 2, 0, 0, 1, 2, 1, 0, 2, 2, 0, 1, 2, 2, 2, 2, 0, 2, 0,\n",
      "        2, 2, 0, 2, 0, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 0, 0, 0, 2, 2, 0, 1, 0, 2,\n",
      "        2, 1, 1, 2, 0, 1, 1, 1, 2, 1, 0, 1, 0, 1, 2, 0, 0, 2, 2, 2, 0, 1, 2, 1,\n",
      "        0, 0, 2, 2, 1, 1, 1, 0, 1, 2, 0, 0, 2, 2, 2, 2, 0, 0, 2, 1, 1, 1, 1, 1,\n",
      "        0, 2, 2, 2, 0, 1, 2, 1, 0, 2, 0, 2, 2, 2, 0, 2, 1, 1, 0, 1, 0, 1, 2, 1,\n",
      "        2, 2, 1, 2, 1, 1, 2, 0, 2, 0, 2, 1, 2, 0, 0, 2, 2, 2, 0, 0, 1, 2, 1, 1,\n",
      "        0, 2, 2, 2, 2, 2, 1, 2, 2, 0, 2, 2, 2, 1, 0, 2, 0, 2, 2, 2, 2, 0, 0, 2,\n",
      "        0, 0, 2, 2, 1, 0, 2, 2, 0, 2, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 0, 0,\n",
      "        0, 0, 2, 0, 2, 0, 1, 1, 2, 0, 0, 0, 2, 2, 1, 2, 0, 2, 0, 2, 0, 0, 2, 2,\n",
      "        1, 0, 0, 1, 1, 2, 1, 0, 2, 2, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 2, 1, 2, 2,\n",
      "        2, 1, 2, 0, 1, 0, 2, 2, 2, 1, 2, 2, 1, 1, 2, 1, 0, 1, 1, 2, 1, 2, 2, 0,\n",
      "        0, 0, 2, 1, 1, 0, 0, 2, 1, 2, 0, 0, 2, 0, 2, 2, 1, 2, 1, 0, 0, 2, 2, 0,\n",
      "        2, 1, 0, 0, 2, 1, 2, 2, 2, 1, 1, 0, 2, 0, 0, 2, 0, 2, 2, 0, 2, 2, 1, 0,\n",
      "        2, 2, 2, 2, 1, 2, 2, 0, 2, 2, 0, 2, 2, 0, 1, 1, 0, 1, 1, 2, 2, 2, 1, 2,\n",
      "        2, 2, 0, 2, 0, 0, 0, 0, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 1, 1, 1, 0,\n",
      "        0, 0, 1, 0, 1, 0, 2, 2, 2, 2, 1, 1, 2, 0, 0, 2, 1, 1, 2, 1, 1, 1, 0, 2,\n",
      "        2, 2, 2, 2, 1, 0, 0, 0, 1, 2, 2, 1, 1, 1, 2, 1, 2, 0, 2, 2, 1, 1, 2, 1,\n",
      "        0, 1, 2, 0, 0, 1, 2, 0, 0, 1, 2, 2, 2, 0, 2, 2, 0, 1, 0, 2, 2, 1, 2, 1,\n",
      "        2, 2, 0, 0, 0, 0, 1, 0, 2, 2, 0, 1, 0, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2,\n",
      "        0, 1, 0, 0, 2, 0, 2, 1])\n",
      "outputs: tensor([[0.0459, 0.2423, 1.7119],\n",
      "        [0.2210, 0.1930, 1.5860],\n",
      "        [1.8572, 0.0788, 0.0640],\n",
      "        ...,\n",
      "        [1.8868, 0.0854, 0.0277],\n",
      "        [0.2163, 0.3971, 1.3866],\n",
      "        [0.2634, 1.3536, 0.3831]], grad_fn=<MulBackward0>)\n",
      "labels: tensor([2, 1, 0, 0, 0, 2, 2, 1, 1, 2, 2, 2, 2, 2, 0, 2, 0, 2, 0, 0, 1, 2, 2, 1,\n",
      "        1, 2, 1, 2, 2, 1, 1, 2, 0, 2, 1, 2, 0, 1, 0, 2, 2, 0, 0, 0, 2, 2, 1, 1,\n",
      "        0, 1, 2, 2, 2, 1, 2, 2, 1, 0, 2, 1, 2, 2, 2, 2, 1, 1, 2, 0, 0, 2, 2, 1,\n",
      "        1, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 1, 1, 2, 0, 2, 1, 2, 0, 0, 1, 0, 0, 2,\n",
      "        2, 2, 1, 2, 1, 2, 0, 2, 2, 0, 1, 1, 2, 0, 0, 1, 2, 2, 0, 2, 1, 2, 1, 0,\n",
      "        2, 2, 2, 0, 2, 0, 0, 0, 2, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 2, 1, 1, 0,\n",
      "        1, 2, 1, 0, 0, 2, 1, 0, 0, 2, 2, 2, 0, 2, 2, 1, 2, 1, 2, 2, 0, 1, 2, 0,\n",
      "        2, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 2, 1, 2, 1, 1, 2, 0, 1, 0, 2, 0, 0, 2,\n",
      "        1, 0, 2, 0, 0, 0, 1, 0, 1, 1, 1, 2, 0, 2, 2, 0, 0, 0, 1, 2, 2, 0, 2, 2,\n",
      "        2, 2, 2, 2, 0, 0, 2, 0, 0, 1, 0, 2, 2, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0,\n",
      "        2, 2, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 0, 0, 0, 1, 2, 0, 2, 2, 1, 1,\n",
      "        1, 2, 0, 0, 0, 0, 1, 0, 1, 2, 0, 0, 2, 0, 0, 2, 0, 2, 0, 2, 2, 2, 0, 2,\n",
      "        1, 2, 1, 0, 1, 2, 2, 1, 0, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 0, 0, 0, 0,\n",
      "        1, 2, 0, 2, 0, 2, 2, 0, 1, 2, 2, 0, 2, 0, 0, 2, 0, 2, 2, 2, 2, 2, 2, 0,\n",
      "        1, 1, 2, 2, 0, 2, 1, 1, 0, 2, 1, 0, 1, 2, 1, 2, 0, 2, 1, 2, 0, 1, 2, 0,\n",
      "        2, 0, 0, 1, 0, 0, 0, 1, 0, 2, 1, 2, 0, 1, 0, 2, 1, 1, 1, 1, 1, 0, 2, 0,\n",
      "        0, 2, 1, 2, 1, 2, 2, 2, 1, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2,\n",
      "        1, 2, 2, 0, 0, 0, 2, 0, 1, 0, 2, 0, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 0,\n",
      "        1, 2, 1, 0, 0, 2, 0, 2, 2, 0, 0, 2, 0, 2, 2, 2, 0, 1, 0, 1, 1, 0, 1, 0,\n",
      "        1, 1, 2, 0, 2, 1, 2, 0, 2, 2, 2, 1, 0, 1, 2, 0, 2, 1, 2, 2, 0, 2, 0, 2,\n",
      "        2, 2, 2, 0, 0, 2, 0, 1, 1, 2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 1, 2, 2, 2, 2,\n",
      "        2, 2, 0, 0, 2, 2, 2, 2])\n",
      "outputs: tensor([[0.0852, 0.3751, 1.5396],\n",
      "        [0.1666, 0.9231, 0.9103],\n",
      "        [0.2325, 0.6558, 1.1117],\n",
      "        ...,\n",
      "        [0.1354, 0.1314, 1.7332],\n",
      "        [0.0751, 0.1640, 1.7610],\n",
      "        [0.0666, 0.1859, 1.7475]], grad_fn=<MulBackward0>)\n",
      "labels: tensor([0, 0, 2, 2, 1, 0, 2, 2, 1, 2, 0, 1, 0, 2, 0, 1, 2, 2, 0, 1, 2, 1, 2, 0,\n",
      "        1, 1, 2, 2, 0, 2, 0, 1, 2, 0, 1, 2, 2, 0, 0, 2, 2, 2, 2, 0, 1, 2, 2, 1,\n",
      "        2, 2, 1, 2, 1, 1, 2, 0, 1, 2, 0, 2, 1, 2, 0, 2, 1, 0, 0, 1, 1, 2, 0, 2,\n",
      "        2, 0, 0, 1, 2, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 2, 2, 2, 2, 0, 2, 2, 1, 2,\n",
      "        1, 0, 0, 2, 0, 1, 0, 0, 2, 2, 1, 2, 2, 0, 2, 0, 0, 1, 2, 2, 0, 1, 2, 2,\n",
      "        1, 2, 0, 0, 1, 1, 2, 0, 2, 1, 0, 2, 2, 1, 0, 0, 1, 2, 1, 0, 1, 2, 2, 1,\n",
      "        0, 2, 0, 2, 0, 2, 0, 2, 2, 2, 2, 0, 0, 0, 1, 2, 2, 1, 0, 1, 0, 0, 2, 1,\n",
      "        0, 2, 0, 2, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2, 1, 1, 0, 2, 1, 1, 0, 2,\n",
      "        2, 2, 0, 1, 2, 0, 2, 2, 2, 1, 2, 0, 0, 2, 1, 2, 2, 1, 0, 1, 0, 0, 2, 0,\n",
      "        2, 2, 0, 2, 2, 2, 1, 0, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 1, 0, 0, 2, 2,\n",
      "        0, 1, 2, 1, 2, 2, 1, 0, 2, 0, 2, 0, 0, 2, 0, 2, 2, 1, 1, 2, 2, 2, 0, 1,\n",
      "        2, 2, 2, 2, 0, 1, 1, 1, 1, 0, 2, 0, 2, 0, 2, 1, 2, 0, 2, 1, 0, 0, 2, 0,\n",
      "        2, 2, 0, 2, 1, 0, 2, 2, 2, 0, 2, 1, 1, 2, 2, 0, 2, 1, 2, 0, 1, 0, 2, 2,\n",
      "        0, 2, 0, 1, 1, 0, 2, 0, 0, 1, 0, 2, 0, 2, 1, 1, 1, 1, 0, 1, 2, 2, 2, 2,\n",
      "        2, 2, 2, 0, 1, 2, 2, 2, 2, 0, 1, 2, 2, 0, 1, 1, 0, 0, 2, 2, 0, 0, 2, 0,\n",
      "        1, 0, 1, 2, 0, 1, 0, 1, 2, 0, 0, 1, 1, 1, 1, 2, 1, 2, 2, 0, 0, 1, 0, 0,\n",
      "        1, 1, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 1, 0, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2,\n",
      "        2, 1, 2, 2, 0, 2, 1, 2, 2, 2, 0, 0, 2, 2, 0, 0, 2, 2, 1, 2, 0, 2, 0, 2,\n",
      "        0, 2, 2, 1, 2, 0, 1, 2, 2, 1, 1, 2, 2, 0, 2, 2, 0, 2, 2, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2,\n",
      "        2, 2, 2, 2, 2, 1, 0, 0, 1, 0, 2, 0, 2, 2, 0, 2, 1, 2, 2, 1, 2, 0, 1, 0,\n",
      "        2, 1, 2, 2, 2, 2, 1, 1])\n",
      "outputs: tensor([[1.6338, 0.2542, 0.1120],\n",
      "        [1.8030, 0.1480, 0.0490],\n",
      "        [0.2626, 0.4436, 1.2938],\n",
      "        ...,\n",
      "        [0.1127, 0.1229, 1.7644],\n",
      "        [0.1335, 1.5627, 0.3037],\n",
      "        [0.0970, 1.7728, 0.1302]], grad_fn=<MulBackward0>)\n",
      "labels: tensor([0, 2, 2, 2, 2, 1, 1, 2, 2, 1, 0, 1, 2, 0, 0, 2, 1, 1, 0, 1, 2, 2, 1, 2,\n",
      "        2, 1, 2, 1, 1, 0, 0, 2, 0, 1, 0, 2, 0, 2, 2, 2, 2, 1, 2, 1, 0, 1, 2, 1,\n",
      "        0, 1, 2, 0, 0, 1, 2, 2, 0, 0, 1, 0, 1, 0, 2, 2, 1, 2, 2, 2, 1, 2, 2, 0,\n",
      "        2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 0, 0, 1, 2, 0, 2, 0, 2, 1, 2, 0, 0, 2, 1,\n",
      "        1, 0, 0, 1, 0, 1, 0, 2, 1, 2, 0, 0, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2,\n",
      "        0, 1, 2, 2, 0, 0, 1, 1, 1, 0, 2, 2, 1, 0, 0, 0, 2, 2, 2, 2, 1, 2, 0, 1,\n",
      "        1, 2, 0, 2, 0, 0, 0, 0, 0, 2, 1, 0, 0, 2, 0, 0, 1, 2, 0, 2, 1, 1, 1, 2,\n",
      "        2, 2, 1, 0, 2, 0, 2, 2, 2, 0, 0, 2, 2, 2, 0, 1, 2, 2, 1, 0, 0, 2, 1, 0,\n",
      "        0, 2, 1, 1, 0, 2, 2, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 0, 1, 0, 1,\n",
      "        2, 1, 2, 0, 2, 2, 0, 1, 2, 2, 1, 2, 2, 0, 0, 0, 2, 1, 0, 1, 2, 0, 2, 2,\n",
      "        2, 0, 2, 2, 0, 2, 2, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 2, 2, 1, 2, 0,\n",
      "        0, 0, 2, 2, 0, 0, 0, 0, 0, 2, 1, 2, 2, 1, 2, 2, 2, 0, 0, 2, 1, 0, 0, 2,\n",
      "        1, 2, 0, 0, 0, 2, 2, 0, 1, 2, 2, 1, 2, 0, 0, 1, 1, 1, 2, 1, 2, 1, 1, 2,\n",
      "        0, 1, 0, 2, 2, 2, 2, 0, 1, 1, 0, 2, 2, 0, 2, 1, 2, 2, 0, 2, 2, 2, 0, 0,\n",
      "        1, 2, 0, 0, 1, 2, 1, 2, 2, 0, 0, 2, 2, 1, 0, 1, 1, 2, 2, 1, 2, 0, 1, 1,\n",
      "        2, 0, 2, 0, 2, 2, 0, 1, 0, 1, 2, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 2, 0,\n",
      "        0, 2, 0, 0, 0, 2, 0, 1, 2, 2, 1, 1, 0, 1, 0, 2, 0, 1, 1, 1, 2, 2, 2, 1,\n",
      "        2, 2, 0, 0, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 0, 0, 0, 1, 0, 0, 0, 2, 0, 2,\n",
      "        0, 2, 0, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 2, 2,\n",
      "        1, 1, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 2, 1, 1, 2, 0, 2, 0, 1, 2,\n",
      "        0, 2, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 0, 2, 0, 1, 2, 0, 1, 2, 1, 2, 1, 0,\n",
      "        2, 0, 2, 0, 2, 0, 1, 2])\n",
      "outputs: tensor([[1.8163, 0.0935, 0.0902],\n",
      "        [0.1487, 0.1746, 1.6767],\n",
      "        [0.1425, 0.1252, 1.7322],\n",
      "        ...,\n",
      "        [1.5334, 0.2495, 0.2172],\n",
      "        [0.5624, 0.3479, 1.0896],\n",
      "        [0.2215, 0.1753, 1.6033]], grad_fn=<MulBackward0>)\n",
      "labels: tensor([0, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 1, 0, 2, 1, 2, 0, 2, 0, 2, 2, 2, 2,\n",
      "        2, 2, 1, 1, 0, 0, 0, 1, 1, 2, 1, 2, 0, 2, 0, 1, 2, 2, 2, 1, 2, 2, 2, 2,\n",
      "        0, 1, 2, 1, 0, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 2, 0, 2, 2, 1, 2, 1, 2,\n",
      "        2, 2, 2, 2, 2, 1, 0, 2, 2, 0, 0, 1, 2, 2, 2, 2, 0, 0, 1, 1, 0, 1, 2, 1,\n",
      "        1, 1, 0, 0, 2, 0, 1, 0, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 1,\n",
      "        2, 1, 2, 2, 2, 1, 0, 2, 1, 0, 2, 0, 0, 2, 2, 1, 2, 0, 0, 2, 0, 1, 2, 0,\n",
      "        0, 1, 2, 2, 2, 2, 0, 2, 1, 2, 0, 0, 1, 0, 0, 2, 2, 1, 1, 0, 0, 2, 2, 0,\n",
      "        0, 0, 2, 1, 2, 0, 0, 0, 0, 2, 2, 0, 2, 2, 2, 1, 1, 2, 0, 2, 2, 0, 2, 2,\n",
      "        0, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 0, 1, 0, 2, 2, 2, 2, 1, 0, 1, 2, 0, 2,\n",
      "        2, 2, 0, 0, 0, 0, 2, 2, 0, 1, 2, 1, 1, 2, 2, 0])\n",
      "outputs: tensor([[1.8140e+00, 9.4885e-02, 9.1162e-02],\n",
      "        [1.0886e-01, 1.8411e+00, 5.0072e-02],\n",
      "        [3.3869e-01, 5.1409e-01, 1.1472e+00],\n",
      "        [1.7706e+00, 2.2749e-01, 1.9497e-03],\n",
      "        [2.3624e-02, 2.6089e-02, 1.9503e+00],\n",
      "        [1.2244e-01, 3.8742e-01, 1.4901e+00],\n",
      "        [5.3921e-02, 3.6838e-02, 1.9092e+00],\n",
      "        [2.9088e-01, 4.3882e-01, 1.2703e+00],\n",
      "        [8.6996e-02, 4.5466e-02, 1.8675e+00],\n",
      "        [1.2691e-01, 2.3580e-01, 1.6373e+00],\n",
      "        [1.1414e-01, 1.9587e-01, 1.6900e+00],\n",
      "        [1.6493e+00, 1.2813e-01, 2.2259e-01],\n",
      "        [9.5179e-02, 1.8366e+00, 6.8246e-02],\n",
      "        [1.8661e+00, 1.1480e-01, 1.9137e-02],\n",
      "        [5.5582e-02, 8.1888e-02, 1.8625e+00],\n",
      "        [3.3790e-02, 1.8937e+00, 7.2510e-02],\n",
      "        [7.9505e-02, 2.1476e-01, 1.7057e+00],\n",
      "        [1.2112e+00, 5.7217e-01, 2.1662e-01],\n",
      "        [5.5775e-02, 5.2752e-02, 1.8915e+00],\n",
      "        [1.7420e+00, 1.8044e-01, 7.7583e-02],\n",
      "        [5.4057e-02, 7.5148e-02, 1.8708e+00],\n",
      "        [2.3054e-01, 4.6472e-01, 1.3047e+00],\n",
      "        [1.0792e-01, 1.8135e-01, 1.7107e+00],\n",
      "        [6.0418e-02, 1.4696e-01, 1.7926e+00],\n",
      "        [7.4105e-02, 1.8392e-01, 1.7420e+00],\n",
      "        [5.2051e-02, 2.4599e-01, 1.7020e+00],\n",
      "        [4.0095e-02, 1.8909e+00, 6.9009e-02],\n",
      "        [3.1259e-01, 1.1495e+00, 5.3794e-01],\n",
      "        [1.7941e+00, 1.9059e-01, 1.5302e-02],\n",
      "        [1.7398e+00, 2.0251e-01, 5.7710e-02],\n",
      "        [1.7614e+00, 1.2166e-01, 1.1690e-01],\n",
      "        [2.3280e-02, 1.9444e+00, 3.2351e-02],\n",
      "        [1.5018e-02, 1.8396e+00, 1.4533e-01],\n",
      "        [2.9739e-02, 5.4494e-02, 1.9158e+00],\n",
      "        [9.0501e-02, 7.9470e-01, 1.1148e+00],\n",
      "        [1.2219e-01, 2.9991e-01, 1.5779e+00],\n",
      "        [1.8113e+00, 1.1035e-01, 7.8309e-02],\n",
      "        [2.8651e-01, 3.3010e-01, 1.3834e+00],\n",
      "        [1.9305e+00, 4.0838e-02, 2.8630e-02],\n",
      "        [1.4908e+00, 2.3573e-01, 2.7347e-01],\n",
      "        [1.5286e-01, 2.0755e-01, 1.6396e+00],\n",
      "        [2.1716e-01, 1.8982e-01, 1.5930e+00],\n",
      "        [5.3057e-02, 9.7175e-02, 1.8498e+00],\n",
      "        [2.1381e-01, 1.4758e+00, 3.1039e-01],\n",
      "        [3.1822e-01, 3.9840e-01, 1.2834e+00],\n",
      "        [3.4465e-01, 2.0618e-01, 1.4492e+00],\n",
      "        [3.8009e-01, 7.2937e-01, 8.9054e-01],\n",
      "        [2.7486e-01, 2.1448e-01, 1.5107e+00],\n",
      "        [1.7345e+00, 9.8901e-02, 1.6663e-01],\n",
      "        [1.1626e-01, 5.8477e-01, 1.2990e+00],\n",
      "        [3.7062e-02, 1.5923e-02, 1.9470e+00],\n",
      "        [2.7119e-02, 1.8999e+00, 7.2999e-02],\n",
      "        [1.9008e+00, 4.5028e-02, 5.4149e-02],\n",
      "        [1.4324e-01, 3.5836e-01, 1.4984e+00],\n",
      "        [1.9983e-01, 1.5815e-01, 1.6420e+00],\n",
      "        [1.3432e-01, 1.3883e+00, 4.7736e-01],\n",
      "        [9.8833e-02, 1.4637e-01, 1.7548e+00],\n",
      "        [4.0918e-01, 1.2698e+00, 3.2106e-01],\n",
      "        [2.1663e-01, 2.5443e-01, 1.5289e+00],\n",
      "        [4.4916e-02, 1.9114e+00, 4.3711e-02],\n",
      "        [6.0999e-02, 1.9303e+00, 8.7211e-03],\n",
      "        [3.9537e-01, 2.8910e-01, 1.3155e+00],\n",
      "        [2.1425e-01, 1.7300e+00, 5.5757e-02],\n",
      "        [2.6183e-01, 2.2920e-01, 1.5090e+00],\n",
      "        [3.4861e-01, 1.6839e-01, 1.4830e+00],\n",
      "        [1.7756e+00, 1.3981e-01, 8.4558e-02],\n",
      "        [1.3232e-01, 9.2673e-01, 9.4096e-01],\n",
      "        [7.1536e-02, 4.0349e-02, 1.8881e+00],\n",
      "        [4.3294e-02, 1.4973e+00, 4.5943e-01],\n",
      "        [2.9916e-01, 2.2607e-01, 1.4748e+00],\n",
      "        [1.1829e-01, 1.8520e+00, 2.9705e-02],\n",
      "        [7.6425e-02, 5.2734e-02, 1.8708e+00],\n",
      "        [7.1730e-02, 1.0188e-01, 1.8264e+00],\n",
      "        [3.8350e-01, 4.3745e-01, 1.1790e+00],\n",
      "        [5.5394e-02, 2.3696e-01, 1.7076e+00],\n",
      "        [5.6096e-02, 4.9448e-02, 1.8945e+00],\n",
      "        [2.2273e-01, 2.5496e-01, 1.5223e+00],\n",
      "        [7.4989e-02, 1.8644e+00, 6.0595e-02],\n",
      "        [1.9864e+00, 6.1409e-03, 7.4126e-03],\n",
      "        [1.8751e-01, 1.3563e-01, 1.6769e+00],\n",
      "        [1.5754e-01, 2.4710e-01, 1.5954e+00],\n",
      "        [1.6370e+00, 2.4277e-01, 1.2018e-01],\n",
      "        [1.7842e+00, 1.4989e-01, 6.5899e-02],\n",
      "        [1.8055e-01, 1.4564e+00, 3.6303e-01],\n",
      "        [3.0003e-01, 1.0232e+00, 6.7676e-01],\n",
      "        [8.9493e-02, 6.7806e-01, 1.2324e+00],\n",
      "        [5.0500e-02, 2.2191e-02, 1.9273e+00],\n",
      "        [2.4340e-01, 2.0448e-01, 1.5521e+00],\n",
      "        [1.2573e+00, 2.8878e-01, 4.5394e-01],\n",
      "        [8.0700e-01, 2.6219e-01, 9.3080e-01],\n",
      "        [2.0763e-02, 1.9672e+00, 1.2025e-02],\n",
      "        [4.0314e-01, 5.2921e-01, 1.0677e+00],\n",
      "        [4.2366e-01, 1.0254e+00, 5.5096e-01],\n",
      "        [6.5958e-02, 1.4914e+00, 4.4269e-01],\n",
      "        [1.0945e-01, 3.8444e-01, 1.5061e+00],\n",
      "        [1.1069e-01, 1.1530e+00, 7.3628e-01],\n",
      "        [4.4424e-01, 1.4409e+00, 1.1485e-01],\n",
      "        [1.0458e+00, 4.1199e-01, 5.4222e-01],\n",
      "        [1.6051e+00, 2.0863e-01, 1.8627e-01],\n",
      "        [1.9491e+00, 3.5092e-02, 1.5832e-02],\n",
      "        [4.1399e-02, 5.9447e-01, 1.3641e+00],\n",
      "        [6.7393e-01, 6.0846e-01, 7.1762e-01],\n",
      "        [2.2174e-02, 1.9481e+00, 2.9741e-02],\n",
      "        [1.7868e+00, 1.0369e-01, 1.0955e-01],\n",
      "        [2.3527e-01, 1.8435e-01, 1.5804e+00],\n",
      "        [1.5723e-01, 1.4652e-01, 1.6963e+00],\n",
      "        [1.9741e+00, 2.1358e-02, 4.5770e-03],\n",
      "        [1.9713e-01, 7.0331e-01, 1.0996e+00],\n",
      "        [1.9035e+00, 6.0135e-02, 3.6376e-02],\n",
      "        [8.5011e-02, 1.0412e-01, 1.8109e+00],\n",
      "        [1.5238e-01, 9.7670e-02, 1.7500e+00],\n",
      "        [1.6672e-01, 1.3642e-01, 1.6969e+00],\n",
      "        [7.5366e-02, 1.6058e-01, 1.7641e+00],\n",
      "        [4.7175e-01, 3.5033e-01, 1.1779e+00],\n",
      "        [1.5855e+00, 3.8562e-01, 2.8933e-02],\n",
      "        [1.2079e-01, 1.7259e-01, 1.7066e+00],\n",
      "        [1.5710e+00, 2.3864e-01, 1.9040e-01],\n",
      "        [1.9319e+00, 5.6445e-02, 1.1645e-02],\n",
      "        [1.8847e-01, 4.9835e-01, 1.3132e+00],\n",
      "        [1.2227e+00, 5.1264e-01, 2.6463e-01],\n",
      "        [1.2419e-01, 1.6313e-01, 1.7127e+00],\n",
      "        [2.7728e-01, 1.6083e+00, 1.1440e-01],\n",
      "        [6.7419e-02, 4.2260e-02, 1.8903e+00],\n",
      "        [5.2603e-02, 1.4550e-01, 1.8019e+00],\n",
      "        [2.6987e-01, 4.9708e-01, 1.2330e+00],\n",
      "        [5.9584e-02, 1.8690e+00, 7.1431e-02],\n",
      "        [1.9282e+00, 4.0976e-02, 3.0859e-02],\n",
      "        [1.0254e-01, 4.6192e-01, 1.4355e+00],\n",
      "        [2.2338e-01, 1.3334e+00, 4.4319e-01],\n",
      "        [1.6200e+00, 2.3770e-01, 1.4227e-01],\n",
      "        [2.6570e-01, 2.8380e-01, 1.4505e+00],\n",
      "        [1.8133e+00, 1.4694e-01, 3.9756e-02],\n",
      "        [1.8446e+00, 5.9562e-02, 9.5835e-02],\n",
      "        [7.2298e-02, 1.5246e-01, 1.7752e+00],\n",
      "        [1.4132e-01, 4.5047e-01, 1.4082e+00],\n",
      "        [7.4063e-01, 8.7087e-01, 3.8850e-01],\n",
      "        [3.5160e-02, 2.9512e-01, 1.6697e+00],\n",
      "        [1.9561e+00, 2.9546e-02, 1.4349e-02],\n",
      "        [1.9203e+00, 5.1412e-02, 2.8292e-02],\n",
      "        [1.3675e-01, 2.7555e-01, 1.5877e+00],\n",
      "        [1.7738e+00, 1.3756e-01, 8.8630e-02],\n",
      "        [2.1882e-01, 1.3117e+00, 4.6950e-01],\n",
      "        [1.2026e-01, 4.8629e-01, 1.3935e+00],\n",
      "        [1.7997e+00, 1.7933e-01, 2.0954e-02],\n",
      "        [7.4548e-01, 8.1257e-01, 4.4195e-01],\n",
      "        [3.4859e-02, 1.8898e+00, 7.5305e-02],\n",
      "        [9.0640e-02, 1.2611e-01, 1.7832e+00],\n",
      "        [1.2663e-01, 1.5723e-01, 1.7161e+00],\n",
      "        [2.0096e-01, 1.8090e-01, 1.6181e+00],\n",
      "        [1.5732e-01, 1.9630e-01, 1.6464e+00],\n",
      "        [1.9531e+00, 3.3710e-02, 1.3213e-02],\n",
      "        [2.7807e-01, 4.8927e-01, 1.2327e+00],\n",
      "        [4.1376e-01, 6.3369e-01, 9.5255e-01],\n",
      "        [9.8246e-02, 2.4636e-01, 1.6554e+00],\n",
      "        [1.7652e+00, 1.8149e-01, 5.3302e-02],\n",
      "        [1.7187e+00, 2.0384e-01, 7.7503e-02],\n",
      "        [1.3023e-01, 1.6899e+00, 1.7984e-01],\n",
      "        [9.9690e-01, 9.2946e-01, 7.3641e-02],\n",
      "        [1.4717e+00, 3.8335e-01, 1.4491e-01],\n",
      "        [4.5103e-02, 4.7104e-02, 1.9078e+00],\n",
      "        [4.8380e-01, 9.1793e-01, 5.9827e-01],\n",
      "        [2.7728e-01, 1.3825e+00, 3.4019e-01],\n",
      "        [9.5650e-02, 1.5744e+00, 3.2991e-01],\n",
      "        [1.8337e+00, 6.9040e-02, 9.7307e-02],\n",
      "        [1.9308e+00, 4.3986e-02, 2.5249e-02],\n",
      "        [4.2712e-01, 4.3317e-01, 1.1397e+00],\n",
      "        [3.0455e-01, 8.5591e-02, 1.6099e+00],\n",
      "        [1.6841e+00, 2.4387e-01, 7.1987e-02],\n",
      "        [1.7756e+00, 1.4148e-01, 8.2957e-02],\n",
      "        [1.9515e+00, 4.2020e-02, 6.5120e-03],\n",
      "        [3.6998e-01, 6.0272e-01, 1.0273e+00],\n",
      "        [5.3409e-02, 1.8983e+00, 4.8341e-02],\n",
      "        [3.4606e-02, 5.6820e-02, 1.9086e+00],\n",
      "        [1.8155e+00, 5.8800e-02, 1.2572e-01],\n",
      "        [1.6008e+00, 1.9562e-01, 2.0354e-01],\n",
      "        [1.9342e+00, 3.2299e-02, 3.3499e-02],\n",
      "        [1.6847e+00, 1.4584e-01, 1.6948e-01],\n",
      "        [2.2858e-01, 3.9029e-01, 1.3811e+00],\n",
      "        [5.1203e-02, 2.4054e-02, 1.9247e+00],\n",
      "        [2.4289e-01, 2.0031e-01, 1.5568e+00],\n",
      "        [1.9921e-01, 1.5212e-01, 1.6487e+00],\n",
      "        [1.6535e-01, 7.4587e-02, 1.7601e+00],\n",
      "        [8.7640e-02, 1.9257e-01, 1.7198e+00],\n",
      "        [4.5541e-02, 1.4546e+00, 4.9988e-01],\n",
      "        [3.9768e-02, 1.6985e+00, 2.6169e-01],\n",
      "        [2.0297e-01, 2.2081e-01, 1.5762e+00],\n",
      "        [7.7370e-02, 1.0538e+00, 8.6887e-01],\n",
      "        [2.1663e-01, 6.1738e-01, 1.1660e+00],\n",
      "        [1.9798e-01, 2.6216e-01, 1.5399e+00],\n",
      "        [1.8287e+00, 1.2765e-01, 4.3630e-02],\n",
      "        [4.6004e-01, 7.2717e-01, 8.1279e-01],\n",
      "        [3.1515e-02, 1.0624e-01, 1.8622e+00],\n",
      "        [1.9685e+00, 1.8661e-02, 1.2789e-02],\n",
      "        [2.2771e-01, 1.5645e-01, 1.6158e+00],\n",
      "        [1.7877e+00, 1.2225e-01, 9.0044e-02],\n",
      "        [5.5044e-02, 5.5636e-01, 1.3886e+00],\n",
      "        [1.0656e+00, 7.8637e-01, 1.4803e-01],\n",
      "        [1.0277e-01, 1.3502e-01, 1.7622e+00],\n",
      "        [3.4196e-02, 4.5952e-02, 1.9199e+00],\n",
      "        [3.3300e-01, 6.6115e-01, 1.0058e+00],\n",
      "        [1.9447e+00, 4.1711e-02, 1.3553e-02],\n",
      "        [1.5464e-01, 1.0284e-01, 1.7425e+00],\n",
      "        [1.7841e-01, 4.2776e-01, 1.3938e+00],\n",
      "        [1.6544e+00, 1.7353e-01, 1.7210e-01],\n",
      "        [4.1798e-02, 1.8515e+00, 1.0674e-01],\n",
      "        [1.9605e+00, 2.8677e-02, 1.0800e-02],\n",
      "        [3.9020e-02, 1.2388e-01, 1.8371e+00],\n",
      "        [1.5909e-01, 1.5672e-01, 1.6842e+00],\n",
      "        [7.4241e-02, 1.7325e-01, 1.7525e+00],\n",
      "        [7.1564e-02, 2.1432e-01, 1.7141e+00],\n",
      "        [2.8743e-01, 1.3905e+00, 3.2203e-01],\n",
      "        [1.9556e+00, 3.7831e-02, 6.5552e-03],\n",
      "        [2.9106e-01, 1.1515e+00, 5.5741e-01],\n",
      "        [8.1006e-02, 1.1833e-01, 1.8007e+00],\n",
      "        [1.7127e+00, 2.1331e-01, 7.3952e-02],\n",
      "        [5.3958e-02, 1.1530e-01, 1.8307e+00],\n",
      "        [3.9695e-02, 1.9081e-01, 1.7695e+00],\n",
      "        [1.4577e-01, 4.8373e-01, 1.3705e+00],\n",
      "        [1.6412e+00, 2.8226e-01, 7.6569e-02],\n",
      "        [2.6809e-01, 9.0517e-01, 8.2674e-01],\n",
      "        [1.9845e+00, 1.0936e-02, 4.5196e-03],\n",
      "        [7.3397e-01, 2.4128e-01, 1.0247e+00],\n",
      "        [4.3432e-02, 1.0562e-01, 1.8509e+00],\n",
      "        [5.7282e-02, 1.7323e-01, 1.7695e+00],\n",
      "        [1.7842e+00, 1.3549e-01, 8.0295e-02],\n",
      "        [2.0138e-02, 1.9218e+00, 5.8073e-02],\n",
      "        [9.4067e-02, 2.8228e-01, 1.6237e+00],\n",
      "        [3.3070e-01, 1.6512e+00, 1.8106e-02],\n",
      "        [5.2253e-02, 1.9160e+00, 3.1740e-02],\n",
      "        [1.1747e-01, 1.6610e-01, 1.7164e+00],\n",
      "        [1.6700e-02, 1.7580e-01, 1.8075e+00],\n",
      "        [1.9719e+00, 2.3286e-02, 4.8141e-03]], grad_fn=<MulBackward0>)\n",
      "Epoch [151/1500], Train Loss: 0.5480, Validation Loss: 1.1474\n",
      "Epoch [152/1500], Train Loss: 0.5486, Validation Loss: 1.1467\n",
      "Epoch [153/1500], Train Loss: 0.5483, Validation Loss: 1.1475\n",
      "Epoch [154/1500], Train Loss: 0.5488, Validation Loss: 1.1481\n",
      "Epoch [155/1500], Train Loss: 0.5490, Validation Loss: 1.1487\n",
      "Epoch [156/1500], Train Loss: 0.5488, Validation Loss: 1.1470\n",
      "Epoch [157/1500], Train Loss: 0.5477, Validation Loss: 1.1478\n",
      "Epoch [158/1500], Train Loss: 0.5481, Validation Loss: 1.1482\n",
      "Epoch [159/1500], Train Loss: 0.5477, Validation Loss: 1.1479\n",
      "Epoch [160/1500], Train Loss: 0.5473, Validation Loss: 1.1489\n",
      "Epoch [161/1500], Train Loss: 0.5479, Validation Loss: 1.1484\n",
      "Epoch [162/1500], Train Loss: 0.5471, Validation Loss: 1.1493\n",
      "Epoch [163/1500], Train Loss: 0.5457, Validation Loss: 1.1484\n",
      "Epoch [164/1500], Train Loss: 0.5464, Validation Loss: 1.1520\n",
      "Epoch [165/1500], Train Loss: 0.5451, Validation Loss: 1.1505\n",
      "Epoch [166/1500], Train Loss: 0.5447, Validation Loss: 1.1514\n",
      "Epoch [167/1500], Train Loss: 0.5442, Validation Loss: 1.1497\n",
      "Epoch [168/1500], Train Loss: 0.5423, Validation Loss: 1.1491\n",
      "Epoch [169/1500], Train Loss: 0.5416, Validation Loss: 1.1514\n",
      "Epoch [170/1500], Train Loss: 0.5409, Validation Loss: 1.1550\n",
      "Epoch [171/1500], Train Loss: 0.5397, Validation Loss: 1.1533\n",
      "Epoch [172/1500], Train Loss: 0.5373, Validation Loss: 1.1538\n",
      "Epoch [173/1500], Train Loss: 0.5388, Validation Loss: 1.1570\n",
      "Epoch [174/1500], Train Loss: 0.5359, Validation Loss: 1.1572\n",
      "Epoch [175/1500], Train Loss: 0.5320, Validation Loss: 1.1577\n",
      "Epoch [176/1500], Train Loss: 0.5295, Validation Loss: 1.1593\n",
      "Epoch [177/1500], Train Loss: 0.5287, Validation Loss: 1.1596\n",
      "Epoch [178/1500], Train Loss: 0.5248, Validation Loss: 1.1588\n",
      "Epoch [179/1500], Train Loss: 0.5230, Validation Loss: 1.1618\n",
      "Epoch [180/1500], Train Loss: 0.5219, Validation Loss: 1.1648\n",
      "Epoch [181/1500], Train Loss: 0.5190, Validation Loss: 1.1646\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 自定义的模型架构2",
   "id": "b87a6cbefda0703d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 自定义的 top-k loss\n",
    "def top_k_loss(y_true, y_pred, top_k=2, top_1_weight=1.0, top_2_weight=0.5, misclassification_weight=1.5):\n",
    "    \"\"\"\n",
    "    y_true: 真实标签，形状 [batch_size]\n",
    "    y_pred: 模型预测的 logits，形状 [batch_size, num_classes]\n",
    "    top_k: top-k 的 k 值\n",
    "    top_1_weight: top-1 命中的权重（越高越小）\n",
    "    top_2_weight: top-2 命中的权重（越高越小）\n",
    "    misclassification_weight: 未命中的样本的损失权重（越高越大）\n",
    "    \"\"\"\n",
    "    # 获取 top-k 的类别索引和对应的概率值\n",
    "    top_k_values, top_k_indices = torch.topk(y_pred, top_k, dim=1, largest=True, sorted=True)\n",
    "    \n",
    "    # 计算命中情况：top-1 和 top-2\n",
    "    top_1_correct = (top_k_indices[:, 0] == y_true).float()  # top-1 命中\n",
    "    top_2_correct = ((top_k_indices[:, 0] == y_true) | (top_k_indices[:, 1] == y_true)).float()  # top-2 命中\n",
    "\n",
    "    # 根据命中情况计算权重\n",
    "    loss = torch.where(top_1_correct == 1.0, torch.tensor(top_1_weight, device=y_true.device), torch.tensor(top_2_weight, device=y_true.device))\n",
    "    loss = torch.where(top_2_correct == 1.0, loss, torch.tensor(misclassification_weight, device=y_true.device))\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "# 定义 MLP 模型（与之前类似）\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers=[256, 128, 32], output_dim=3, dropout_prob=0.3):\n",
    "        super(MLPModel, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_prob))  # 添加 Dropout 层\n",
    "            in_dim = hidden_dim\n",
    "        layers.append(nn.Linear(in_dim, output_dim))  # 输出层\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# 定义训练过程\n",
    "def train_model(model, x_train, y_train, x_val, y_val, epochs=100, batch_size=512, \n",
    "                learning_rate=1e-4, early_stopping=False, patience=10):\n",
    "    # 创建优化器\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = top_k_loss  # 使用自定义的 top-k loss\n",
    "\n",
    "    # 批次处理数据\n",
    "    dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 初始化早期停止相关变量\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    best_model_weights = None\n",
    "\n",
    "    # 用于记录训练损失和验证损失\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(labels, outputs)  # 计算损失\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            # 反向传播和优化\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 计算训练集上的平均损失\n",
    "        train_loss = running_train_loss / len(dataloader)\n",
    "\n",
    "        # 计算验证集上的损失\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(x_val)\n",
    "            val_loss = criterion(y_val, val_outputs)\n",
    "\n",
    "        # 保存训练损失和验证损失\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        # 打印当前的训练损失和验证损失\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # 早期停止：检查验证损失\n",
    "        if early_stopping:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_weights = model.state_dict()\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "    # 恢复最好的模型权重\n",
    "    if early_stopping and best_model_weights:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "\n",
    "    # 绘制训练损失和验证损失的曲线\n",
    "    plot_loss_curve(train_losses, val_losses)\n",
    "\n",
    "    return model\n",
    "\n",
    "# 计算 top-k 准确率\n",
    "def compute_top_k_accuracy(model, x_test, y_test, k=2):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x_test)\n",
    "        _, top_k_indices = torch.topk(outputs, k, dim=1, largest=True, sorted=True)\n",
    "        correct_top_1 = torch.sum(top_k_indices[:, 0] == y_test).item()\n",
    "        correct_top_2 = torch.sum((top_k_indices[:, 0] == y_test) | (top_k_indices[:, 1] == y_test)).item()\n",
    "        top_1_accuracy = correct_top_1 / len(y_test)\n",
    "        top_2_accuracy = correct_top_2 / len(y_test)\n",
    "    return top_1_accuracy, top_2_accuracy\n",
    "\n",
    "# 绘制训练和验证损失的曲线\n",
    "def plot_loss_curve(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', color='red', linestyle='--', marker='x')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# 从 pandas 数据加载训练和验证数据\n",
    "def load_data_from_pandas(x_train_pd, y_train_pd, x_val_pd, y_val_pd):\n",
    "    \"\"\"\n",
    "    将 pandas 数据加载为 torch 张量\n",
    "    x_train_pd: pandas DataFrame, 训练数据\n",
    "    y_train_pd: pandas Series, 训练标签\n",
    "    x_val_pd: pandas DataFrame, 验证数据\n",
    "    y_val_pd: pandas Series, 验证标签\n",
    "    \"\"\"\n",
    "    # 将 pandas DataFrame 转换为 torch Tensor\n",
    "    x_train = torch.tensor(x_train_pd.values, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train_pd.values, dtype=torch.long)\n",
    "\n",
    "    x_val = torch.tensor(x_val_pd.values, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val_pd.values, dtype=torch.long)\n",
    "\n",
    "    return x_train, y_train, x_val, y_val\n",
    "\n",
    "\n",
    "# 假设我们有 pandas 数据\n",
    "x_train_pd = xTr\n",
    "y_train_pd = yTr\n",
    "x_val_pd = xTe\n",
    "y_val_pd = yTe\n",
    "\n",
    "# 从 pandas 数据加载到 PyTorch 张量\n",
    "x_train, y_train, x_val, y_val = load_data_from_pandas(x_train_pd, y_train_pd, x_val_pd, y_val_pd)\n",
    "\n",
    "# 模型初始化\n",
    "input_dim = 10  # 特征维度\n",
    "model = MLPModel(input_dim=input_dim, hidden_layers=[256, 128, 32], output_dim=3, dropout_prob=0.3)\n",
    "\n",
    "# 训练模型\n",
    "trained_model = train_model(model, x_train, y_train, x_val, y_val, epochs=100, batch_size=512, \n",
    "                            learning_rate=1e-4, early_stopping=True, patience=10)\n",
    "\n",
    "# 评估模型的 top-1 和 top-2 准确率\n",
    "top_1_accuracy, top_2_accuracy = compute_top_k_accuracy(trained_model, x_val, y_val, k=2)\n",
    "print(f\"Top-1 Accuracy: {top_1_accuracy:.4f}\")\n",
    "print(f\"Top-2 Accuracy: {top_2_accuracy:.4f}\")\n"
   ],
   "id": "f751cf04429d7d3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 自定义模型架构3",
   "id": "956326f3790e725f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 自定义的 top-k loss\n",
    "def top_k_loss(y_true, y_pred, top_k=2, top_1_weight=1.0, top_2_weight=0.5, misclassification_weight=1.5):\n",
    "    \"\"\"\n",
    "    y_true: 真实标签，形状 [batch_size]\n",
    "    y_pred: 模型预测的 logits，形状 [batch_size, num_classes]\n",
    "    top_k: top-k 的 k 值\n",
    "    top_1_weight: top-1 命中的权重（越高越小）\n",
    "    top_2_weight: top-2 命中的权重（越高越小）\n",
    "    misclassification_weight: 未命中的样本的损失权重（越高越大）\n",
    "    \"\"\"\n",
    "    # 获取 top-k 的类别索引和对应的概率值\n",
    "    top_k_values, top_k_indices = torch.topk(y_pred, top_k, dim=1, largest=True, sorted=True)\n",
    "    \n",
    "    # 计算命中情况：top-1 和 top-2\n",
    "    top_1_correct = (top_k_indices[:, 0] == y_true).float()  # top-1 命中\n",
    "    top_2_correct = ((top_k_indices[:, 0] == y_true) | (top_k_indices[:, 1] == y_true)).float()  # top-2 命中\n",
    "\n",
    "    # 根据命中情况计算权重\n",
    "    loss = torch.where(top_1_correct == 1.0, torch.tensor(top_1_weight, device=y_true.device), torch.tensor(top_2_weight, device=y_true.device))\n",
    "    loss = torch.where(top_2_correct == 1.0, loss, torch.tensor(misclassification_weight, device=y_true.device))\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "# 定义 MLP 模型（添加了 BatchNorm 层）\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers=[256, 128, 32], output_dim=3, dropout_prob=0.3):\n",
    "        super(MLPModel, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))  # 添加 BatchNorm 层\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_prob))  # 添加 Dropout 层\n",
    "            in_dim = hidden_dim\n",
    "        layers.append(nn.Linear(in_dim, output_dim))  # 输出层\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# 定义训练过程\n",
    "def train_model(model, x_train, y_train, x_val, y_val, x_test, y_test, epochs=100, batch_size=512, \n",
    "                learning_rate=1e-4, early_stopping=False, patience=10):\n",
    "    # 创建优化器\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = top_k_loss  # 使用自定义的 top-k loss\n",
    "\n",
    "    # 批次处理数据\n",
    "    dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 初始化早期停止相关变量\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    best_model_weights = None\n",
    "\n",
    "    # 用于记录训练损失和验证损失\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(labels, outputs)  # 计算损失\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            # 反向传播和优化\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 计算训练集上的平均损失\n",
    "        train_loss = running_train_loss / len(dataloader)\n",
    "\n",
    "        # 计算验证集上的损失\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(x_val)\n",
    "            val_loss = criterion(y_val, val_outputs)\n",
    "\n",
    "        # 保存训练损失和验证损失\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        # 打印当前的训练损失和验证损失\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # 早期停止：检查验证损失\n",
    "        if early_stopping:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_weights = model.state_dict()\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "    # 恢复最好的模型权重\n",
    "    if early_stopping and best_model_weights:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "\n",
    "    # 绘制训练损失和验证损失的曲线\n",
    "    plot_loss_curve(train_losses, val_losses)\n",
    "\n",
    "    # 在测试集上评估模型\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test)\n",
    "        test_loss = criterion(y_test, test_outputs)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# 计算 top-k 准确率\n",
    "def compute_top_k_accuracy(model, x_data, y_data, k=2):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x_data)\n",
    "        _, top_k_indices = torch.topk(outputs, k, dim=1, largest=True, sorted=True)\n",
    "        correct_top_1 = torch.sum(top_k_indices[:, 0] == y_data).item()\n",
    "        correct_top_2 = torch.sum((top_k_indices[:, 0] == y_data) | (top_k_indices[:, 1] == y_data)).item()\n",
    "        top_1_accuracy = correct_top_1 / len(y_data)\n",
    "        top_2_accuracy = correct_top_2 / len(y_data)\n",
    "    return top_1_accuracy, top_2_accuracy\n",
    "\n",
    "# 绘制训练和验证损失的曲线\n",
    "def plot_loss_curve(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', color='red', linestyle='--', marker='x')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# 从 pandas 数据加载训练、验证和测试数据\n",
    "def load_data_from_pandas(x_train_pd, y_train_pd, x_test_pd, y_test_pd, val_size=0.1):\n",
    "    \"\"\"\n",
    "    将 pandas 数据加载为 torch 张量，并划分训练集和验证集\n",
    "    x_train_pd: pandas DataFrame, 训练数据\n",
    "    y_train_pd: pandas Series, 训练标签\n",
    "    x_test_pd: pandas DataFrame, 测试数据\n",
    "    y_test_pd: pandas Series, 测试标签\n",
    "    val_size: 验证集的比例\n",
    "    \"\"\"\n",
    "    # 划分训练集和验证集\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train_pd, y_train_pd, test_size=val_size, random_state=13)\n",
    "\n",
    "    # 将 pandas DataFrame 转换为 torch Tensor\n",
    "    x_train = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
    "\n",
    "    x_val = torch.tensor(x_val.values, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val.values, dtype=torch.long)\n",
    "\n",
    "    x_test = torch.tensor(x_test_pd.values, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test_pd.values, dtype=torch.long)\n",
    "\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "\n",
    "# 加载数据并划分验证集\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = load_data_from_pandas(xTr, yTr, xTe, yTe, val_size=0.15)\n",
    "\n",
    "\n",
    "model = MLPModel(input_dim=x_train.shape[1], hidden_layers=[256, 128, 32], output_dim=3, dropout_prob=0.3)\n",
    "trained_model = train_model(model, x_train, y_train, x_val, y_val, x_test, y_test, epochs=100, batch_size=512, learning_rate=1e-4, early_stopping=True)\n"
   ],
   "id": "d242935a5dabd7dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bc4656a5",
   "metadata": {},
   "source": [
    "### 3.7 Stacked Classifier ###"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import SVC"
   ],
   "id": "b630a432dc50b319"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "stacked_clf = StackingClassifier(estimators=[('svm', SVC(max_iter=100000)), ('logistic', LogisticRegression(C=0.01, max_iter=10000))],\n",
    "                                final_estimator=LogisticRegression(max_iter=10000),\n",
    "                                n_jobs=-1).fit(xTr, yTr.values.ravel())"
   ],
   "id": "d1ff3a7ac5466e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, stacked_clf.predict(xTr))"
   ],
   "id": "89cbb239a3759038"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# testing score\n",
    "accuracy_score(yTe.Result.values, stacked_clf.predict(xTe))"
   ],
   "id": "a4ddde81e2f0c7be"
  },
  {
   "cell_type": "markdown",
   "id": "e90e8cae",
   "metadata": {},
   "source": [
    "## 4. Result Analysis ##"
   ]
  },
  {
   "cell_type": "code",
   "id": "0d4a4507",
   "metadata": {},
   "source": [
    "## TODO: breakdown results across divisions and/or teams; i.e., see how model performs individually at subgroups"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a10a069c",
   "metadata": {},
   "source": [
    "## 5. Scrap Code ##"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "barcelona_df = learning_df[(learning_df['HomeTeam 17'] == 1) | (learning_df['AwayTeam 17'] == 1)]\n",
    "barcelona_df"
   ],
   "id": "6062bbe60dc13f71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "bxTr = xTr[(xTr['HomeTeam 17'] == 1) | (xTr['AwayTeam 17'] == 1)]\n",
    "bxTe = xTe[(xTe['HomeTeam 17'] == 1) | (xTe['AwayTeam 17'] == 1)]"
   ],
   "id": "ebf2f4232e36abc6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "byTr, byTe = yTr.loc[bxTr.index,:], yTe.loc[bxTe.index,:]",
   "id": "605452e5abb605ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# training score\n",
    "accuracy_score(byTr, l1_lr.predict(bxTr))"
   ],
   "id": "98151321f88aef9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# testing score\n",
    "accuracy_score(byTe, l1_lr.predict(bxTe))"
   ],
   "id": "7da0be8a78c64799"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# training score\n",
    "accuracy_score(byTr, l2_lr.predict(bxTr))"
   ],
   "id": "c8b3e93d0eb50d5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# testing score\n",
    "accuracy_score(byTe, l2_lr.predict(bxTe))"
   ],
   "id": "2f0a4b0253463085"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Pytorch MLP ##",
   "id": "1a9d44ed88b506d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "type(xTr)",
   "id": "e0ea07161a4dada9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "xTr.shape",
   "id": "eab34be724a70a74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ],
   "id": "b2cd03727fc3f4a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_weights = nn.Parameter(torch.randn(feature_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 应用注意力权重\n",
    "        weights = F.softmax(self.attention_weights, dim=0)\n",
    "        # 加权求和\n",
    "        x = x * weights\n",
    "        return x\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=xTr.shape[1], out_features=512)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=512)\n",
    "        self.attention = Attention(512)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=128)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=128)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=32)\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=32)\n",
    "        self.dropout3 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc4 = nn.Linear(in_features=32, out_features=3)  # 输出层改为3，对应三个类别\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(torch.relu(self.bn1(self.fc1(x))))\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout2(torch.relu(self.bn2(self.fc2(x))))\n",
    "        x = self.dropout3(torch.relu(self.bn3(self.fc3(x))))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# 数据预处理\n",
    "scaler = StandardScaler()\n",
    "xTr_scaled = scaler.fit_transform(xTr)\n",
    "xTr_tensor = torch.tensor(xTr_scaled, dtype=torch.float32).to(device)\n",
    "yTr_tensor = torch.tensor(yTr.values.ravel(), dtype=torch.long).to(device)\n",
    "\n",
    "# 创建数据加载器\n",
    "dataset = TensorDataset(xTr_tensor, yTr_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "# 创建模型实例\n",
    "model = MLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "train_start = time.time()\n",
    "# 训练模型\n",
    "model.train()\n",
    "for epoch in range(500):  # 假设训练200个epoch\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # 清除之前的梯度\n",
    "\n",
    "        outputs = model(inputs)  # 前向传播\n",
    "        loss = criterion(outputs, labels)  # 计算损失\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新参数\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)  # 累计损失\n",
    "        _, predicted = torch.max(outputs.data, 1)  # 获取预测结果\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total * 100  # 计算准确率\n",
    "\n",
    "    # 每个epoch结束后输出\n",
    "    print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "print(f'训练时长： {time.time() - train_start}s')"
   ],
   "id": "6e7e048c04345f63"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 假设 xTe 和 yTe 是 pandas DataFrame 或 Series\n",
    "# 数据预处理\n",
    "xTe_scaled = scaler.fit_transform(xTe)  # 使用与训练数据相同的标准化参数\n",
    "xTe_tensor = torch.tensor(xTe_scaled, dtype=torch.float32).to(device)\n",
    "yTe_tensor = torch.tensor(yTe.values.ravel(), dtype=torch.long).to(device)\n",
    "\n",
    "# 创建数据加载器\n",
    "test_dataset = TensorDataset(xTe_tensor, yTe_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 初始化用于计算准确率的变量\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 不计算梯度，因为在评估模式下不需要进行反向传播\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy on test set: {accuracy * 100:.2f}%')"
   ],
   "id": "5d7583df837e05c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Pytorch Transformer ##",
   "id": "cf72af33f38ca812"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_heads, num_layers, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.model_dim = input_dim  # 通常情况下，模型维度与输入维度相同\n",
    "\n",
    "        # Transformer Encoder Layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.model_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=512,  # 前馈网络的维度\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Transformer Encoder\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # 输出层\n",
    "        self.output_layer = nn.Linear(self.model_dim, self.num_classes)\n",
    "\n",
    "        # Batch Normalization\n",
    "        self.bn = nn.BatchNorm1d(self.model_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 增加一个假的序列维度\n",
    "        x = x.unsqueeze(1)\n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Batch Normalization\n",
    "        x = self.bn(x[:, 0, :])  # 取序列的第一个元素进行批量归一化\n",
    "\n",
    "        # 输出层\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# 补充维度\n",
    "n_samples_xTr = xTr.shape[0]\n",
    "n_samples_xTe = xTe.shape[0]\n",
    "for i in range(1, 4):  # 从 1 到 3，因为需要添加三列\n",
    "    xTr[f'pad{i}'] = 0  # 添加填充列，初始化为 0\n",
    "    xTe[f'pad{i}'] = 0  # 添加填充列，初始化为 0\n",
    "\n",
    "# 参数设置\n",
    "input_dim = xTr.shape[1]  # 输入特征的维度\n",
    "num_classes = 3  # 类别数\n",
    "num_heads = 10  # 注意力头的数量\n",
    "num_layers = 3  # Transformer层的数量\n",
    "dropout = 0.8  # Dropout比率\n",
    "\n",
    "# 创建模型\n",
    "model = TransformerModel(input_dim, num_classes, num_heads, num_layers, dropout).to(device)\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 数据加载\n",
    "# 假设 xTr 和 xTe 已经是适当的 torch.Tensor 对象\n",
    "xTr_values = xTr.values.astype(float)\n",
    "xTe_values = xTe.values.astype(float)\n",
    "\n",
    "xTr_tensor = torch.tensor(xTr_values, dtype=torch.float32).to(device)\n",
    "xTe_tensor = torch.tensor(xTe_values, dtype=torch.float32).to(device)\n",
    "yTr_tensor = torch.tensor(yTr.values, dtype=torch.long).to(device).squeeze(1)\n",
    "yTe_tensor = torch.tensor(yTe.values, dtype=torch.long).to(device).squeeze(1)\n",
    "# 转换为 one-hot 编码\n",
    "yTr_tensor = F.one_hot(yTr_tensor, num_classes=num_classes).float()\n",
    "yTe_tensor = F.one_hot(yTe_tensor, num_classes=num_classes).float()\n",
    "\n",
    "# 数据加载器\n",
    "train_dataset = TensorDataset(xTr_tensor, yTr_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(xTe_tensor, yTe_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "train_start = time.time()\n",
    "# 训练模型\n",
    "model.train()\n",
    "for epoch in range(500):  # 运行更多的 epoch 以获得更好的结果\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)  # 累计损失\n",
    "        _, predicted = torch.max(outputs.data, 1)  # 获取预测结果\n",
    "        _, truth = torch.max(labels.data, 1)\n",
    "        total += truth.size(0)\n",
    "        correct += (predicted == truth).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total * 100  # 计算准确率\n",
    "\n",
    "    # 每个epoch结束后输出\n",
    "    print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "print(f'训练时长： {time.time() - train_start}s')\n",
    "\n",
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 初始化用于计算准确率的变量\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 不计算梯度，因为在评估模式下不需要进行反向传播\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, truth = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == truth).sum().item()\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy on test set: {accuracy * 100:.2f}%')"
   ],
   "id": "7c086a748d381401"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
