{
 "cells": [
  {
   "cell_type": "code",
   "id": "6e504281",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:35.045807Z",
     "start_time": "2025-02-02T10:51:35.012147Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "import joblib"
   ],
   "execution_count": 193,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a341b52b",
   "metadata": {},
   "source": [
    "## 0. DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "id": "e9469f20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:36.334857Z",
     "start_time": "2025-02-02T10:51:36.192317Z"
    }
   },
   "source": [
    "football_df = pd.read_csv('data/all_data_with_elo.csv', low_memory = False)\n",
    "football_df"
   ],
   "execution_count": 194,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:36.387895Z",
     "start_time": "2025-02-02T10:51:36.335861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 提前6个月，用来划分赛季\n",
    "# 将字符串转换为日期类型，指定日期格式\n",
    "football_df['Date'] = pd.to_datetime(football_df['Date'], format='%Y/%m/%d')\n",
    "football_df['Date'] = football_df['Date'] - pd.DateOffset(months=6)\n",
    "football_df"
   ],
   "id": "91c6f576d106bfec",
   "execution_count": 195,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fcbbc3ce",
   "metadata": {},
   "source": [
    "## 1. Descriptive Statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b828a5",
   "metadata": {},
   "source": [
    "**1.1 DataFrame Shape**"
   ]
  },
  {
   "cell_type": "code",
   "id": "15427373",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:36.417703Z",
     "start_time": "2025-02-02T10:51:36.404066Z"
    }
   },
   "source": [
    "# no. rows and no. cols\n",
    "football_df.shape"
   ],
   "execution_count": 196,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7b649b14",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:36.465773Z",
     "start_time": "2025-02-02T10:51:36.450747Z"
    }
   },
   "source": [
    "# feature names\n",
    "print(football_df.columns.tolist())"
   ],
   "execution_count": 197,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fdb560ee",
   "metadata": {},
   "source": [
    "**1.2 NaN Values**"
   ]
  },
  {
   "cell_type": "code",
   "id": "d3377302",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:36.625681Z",
     "start_time": "2025-02-02T10:51:36.594389Z"
    }
   },
   "source": [
    "football_df.isnull().sum()"
   ],
   "execution_count": 198,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7eae5438",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:36.857505Z",
     "start_time": "2025-02-02T10:51:36.838148Z"
    }
   },
   "source": [
    "# total elements in \n",
    "football_df.size"
   ],
   "execution_count": 199,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2afbc469",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:37.210244Z",
     "start_time": "2025-02-02T10:51:37.181135Z"
    }
   },
   "source": [
    "# total number of NaN\n",
    "football_df.size - football_df.count().sum()"
   ],
   "execution_count": 200,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a9b3446d",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:37.377675Z",
     "start_time": "2025-02-02T10:51:37.340978Z"
    }
   },
   "source": [
    "# total number of NaN rows\n",
    "football_df.isnull().any(axis = 1).sum()"
   ],
   "execution_count": 201,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "12fcf2ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:37.426600Z",
     "start_time": "2025-02-02T10:51:37.405218Z"
    }
   },
   "source": [
    "# total number of NaN columns\n",
    "football_df.isnull().any(axis = 0).sum()"
   ],
   "execution_count": 202,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "da2853e2",
   "metadata": {},
   "source": [
    "## 2. Data Wrangling and Feature Transformation/Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cde42d8",
   "metadata": {},
   "source": [
    "### 2.1 NaN Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a34fc35",
   "metadata": {},
   "source": [
    "`TODO`: drop NaN values along columns: {Date, Home Team, Away Team, FTR} <br>\n",
    "`TODO`: identify betting odds w/ most available data"
   ]
  },
  {
   "cell_type": "code",
   "id": "7f75f7d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:37.551122Z",
     "start_time": "2025-02-02T10:51:37.531647Z"
    }
   },
   "source": [
    "# 当前方法仅提取这几个字段 分区 日期 主队 客队 full-time-result 三家机构的胜平负 主队ELO评分 客队ELO评分\n",
    "# nan_mask = ['Div', 'Date', 'HomeTeam', 'AwayTeam', 'FTR', 'B365H', 'B365D', 'B365A', \n",
    "#             'IWH', 'IWD', 'IWA', 'WHH', 'WHD', 'WHA', 'AHh', 'B365AHH', 'B365AHA', 'HomeTeamELO', 'AwayTeamELO']\n",
    "nan_mask = ['Div', 'Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'B365H', 'B365D', 'B365A', 'WHH', 'WHD', 'WHA', 'AHh', 'B365AHH', 'B365AHA', 'AHCh', 'B365CAHH', 'B365CAHA','HomeTeamELO', 'AwayTeamELO']"
   ],
   "execution_count": 203,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:37.676371Z",
     "start_time": "2025-02-02T10:51:37.621495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nan_football_df = football_df.dropna(subset = nan_mask)\n",
    "nan_football_df"
   ],
   "id": "23e46d7ed335b538",
   "execution_count": 204,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:37.737955Z",
     "start_time": "2025-02-02T10:51:37.688523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conditions = [\n",
    "    nan_football_df['AHCh'] >= 2.5,         # AHh >= 2.75\n",
    "    (nan_football_df['AHCh'] >= 1.5) & (nan_football_df['AHCh'] <= 2.25),  # 1.75 < AHh <= 2.75\n",
    "    (nan_football_df['AHCh'] >= 0.25) & (nan_football_df['AHCh'] <= 1.25),  # 0.25 < AHh <= 1.75\n",
    "    nan_football_df['AHCh'] == 0,            # AHh == 0\n",
    "    (nan_football_df['AHCh'] >= -1.25) & (nan_football_df['AHCh'] <= -0.25),  # -1.75 < AHh <= -0.25\n",
    "    (nan_football_df['AHCh'] >= -2.25) & (nan_football_df['AHCh'] <= -1.5),  # -2.75 < AHh <= -1.75\n",
    "    nan_football_df['AHCh'] <= -2.5\n",
    "]\n",
    "# easy_conditions = [\n",
    "#     nan_football_df['AHh'] <= -0.25,\n",
    "#     nan_football_df['AHh'] == 0,\n",
    "#     nan_football_df['AHh'] >= 0.25,\n",
    "# ]\n",
    "labels = [3, 2, 1, 0, -1, -2, -3]\n",
    "# easy_labels = [-1, 0, 1]\n",
    "\n",
    "nan_football_df['balance_val'] = np.select(conditions, labels)\n",
    "nan_football_df"
   ],
   "id": "b9dbb5d1679a1e6b",
   "execution_count": 205,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:37.853433Z",
     "start_time": "2025-02-02T10:51:37.819608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nan_football_df['asia_final_result'] = nan_football_df['FTHG'] - nan_football_df['FTAG'] + nan_football_df['balance_val']\n",
    "nan_football_df"
   ],
   "id": "11c29f5924e5d7a1",
   "execution_count": 206,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:37.908233Z",
     "start_time": "2025-02-02T10:51:37.868768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nan_football_df_noNone = nan_football_df.dropna(subset = nan_mask)\n",
    "nan_football_df_noNone"
   ],
   "id": "d1bdaa50359f4dd6",
   "execution_count": 207,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:38.021093Z",
     "start_time": "2025-02-02T10:51:37.998201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nan_football_df_noNone.reset_index(inplace=True, drop=True)\n",
    "nan_football_df_noNone"
   ],
   "id": "fd25a49ce3655cb6",
   "execution_count": 208,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:38.228661Z",
     "start_time": "2025-02-02T10:51:38.200334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conditions = [\n",
    "    nan_football_df_noNone['asia_final_result'] < 0,\n",
    "    nan_football_df_noNone['asia_final_result'] == 0,\n",
    "    nan_football_df_noNone['asia_final_result'] > 0,\n",
    "]\n",
    "easy_labels = [0, 1, 2]\n",
    "\n",
    "nan_football_df_noNone['easy_label'] = np.select(conditions, easy_labels)\n",
    "nan_football_df_noNone"
   ],
   "id": "143b539d11ec3c73",
   "execution_count": 209,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ad42e459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:38.696170Z",
     "start_time": "2025-02-02T10:51:38.664634Z"
    }
   },
   "source": [
    "# resize shape\n",
    "football_df.shape[0] - nan_football_df_noNone.shape[0]"
   ],
   "execution_count": 210,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "42864535",
   "metadata": {},
   "source": [
    "### 2.2 Feature Encoding <br>\n",
    "* $\\phi(Date)$ $\\Rightarrow$ one column for *year*, second column for *month*, third column for *day of year*\n",
    "* One hot encode Division, Home and Away Teams\n",
    "* Label encode Full Time Result (Win/Draw/Loss)"
   ]
  },
  {
   "cell_type": "code",
   "id": "7b5bb61e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:41.190565Z",
     "start_time": "2025-02-02T10:51:41.175402Z"
    }
   },
   "source": [
    "feats = nan_mask\n",
    "feats.append('easy_label')\n",
    "feats.append('balance_val')"
   ],
   "execution_count": 211,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:41.477685Z",
     "start_time": "2025-02-02T10:51:41.439836Z"
    }
   },
   "cell_type": "code",
   "source": "nan_football_df_noNone",
   "id": "9919d74193a28304",
   "execution_count": 212,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:41.693212Z",
     "start_time": "2025-02-02T10:51:41.658728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_df_feat = nan_football_df_noNone.copy()[feats]\n",
    "learning_df_feat"
   ],
   "id": "5213f7b7293d7cc0",
   "execution_count": 213,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "fe16f1b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:42.056546Z",
     "start_time": "2025-02-02T10:51:41.814647Z"
    }
   },
   "source": [
    "learning_df_feat.reset_index(inplace=True, drop=True)\n",
    "# 保存文件作为历史文件\n",
    "learning_df_feat.to_csv('.\\prediction_data/history_data_balance.csv', index=False, encoding='utf-8-sig')\n",
    "learning_df_feat"
   ],
   "execution_count": 214,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "15884417",
   "metadata": {},
   "source": [
    "**2.2.1 Division and Home/Away Team Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "id": "913c0088",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:42.107711Z",
     "start_time": "2025-02-02T10:51:42.059550Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "div_encoder = OneHotEncoder()\n",
    "home_encoder = OneHotEncoder()\n",
    "away_encoder = OneHotEncoder()"
   ],
   "execution_count": 215,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "bc739d2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:42.239564Z",
     "start_time": "2025-02-02T10:51:42.203376Z"
    }
   },
   "source": [
    "onehot_div = div_encoder.fit_transform(learning_df_feat.Div.values.reshape(-1,1)).toarray().astype(int)\n",
    "onehot_div_df = pd.DataFrame(onehot_div, columns = [\"Div \"+str(int(i)) for i in range(onehot_div.shape[1])])\n",
    "\n",
    "onehot_home = home_encoder.fit_transform(learning_df_feat.HomeTeam.values.reshape(-1,1)).toarray().astype(int)\n",
    "onehot_home_df = pd.DataFrame(onehot_home, columns = ['HomeTeam ' + str(int(i)) for i in np.arange(onehot_home.shape[1])])\n",
    "\n",
    "onehot_away = away_encoder.fit_transform(learning_df_feat.AwayTeam.values.reshape(-1,1)).toarray().astype(int)\n",
    "onehot_away_df = pd.DataFrame(onehot_away, columns = ['AwayTeam ' + str(int(i)) for i in np.arange(onehot_away.shape[1])])"
   ],
   "execution_count": 216,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:42.571867Z",
     "start_time": "2025-02-02T10:51:42.531556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 保存编码器到本地\n",
    "joblib.dump(div_encoder, 'div_encoder.pkl')\n",
    "joblib.dump(home_encoder, 'home_encoder.pkl')\n",
    "joblib.dump(away_encoder, 'away_encoder.pkl')"
   ],
   "id": "afe5dd460928770d",
   "execution_count": 217,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f8444147",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:42.754150Z",
     "start_time": "2025-02-02T10:51:42.718278Z"
    }
   },
   "source": [
    "learning_df_div = pd.concat([learning_df_feat, onehot_div_df, onehot_home_df, onehot_away_df], axis = 1)\n",
    "learning_df_div.drop(columns = ['Div'], inplace = True)"
   ],
   "execution_count": 218,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "321f2f32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:42.915750Z",
     "start_time": "2025-02-02T10:51:42.876152Z"
    }
   },
   "source": "learning_df_div",
   "execution_count": 219,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1b0fefa3",
   "metadata": {},
   "source": [
    "**2.2.2 Full Time Result Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "id": "f9442a80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:43.311546Z",
     "start_time": "2025-02-02T10:51:43.284547Z"
    }
   },
   "source": [
    "target_encoder = LabelEncoder()\n",
    "learning_df_div['Result'] = target_encoder.fit_transform(learning_df_div.easy_label) \n",
    "learning_df_div['Result_FTR'] = target_encoder.fit_transform(learning_df_div.FTR)"
   ],
   "execution_count": 220,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "18c1cb41",
   "metadata": {},
   "source": [
    "**2.2.3 Date Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "id": "f5dfc529",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:43.432884Z",
     "start_time": "2025-02-02T10:51:43.394181Z"
    }
   },
   "source": [
    "learning_df_div['Year'] = pd.DatetimeIndex(learning_df_div.Date).year\n",
    "\n",
    "learning_df_div['Month'] = pd.DatetimeIndex(learning_df_div.Date).month\n",
    "learning_df_div['Sin_Month'] = np.sin(2*np.pi*learning_df_div.Month/12)\n",
    "learning_df_div['Cos_Month'] = np.cos(2*np.pi*learning_df_div.Month/12)\n",
    "\n",
    "learning_df_div['DayofYear'] = pd.DatetimeIndex(learning_df_div.Date).dayofyear\n",
    "learning_df_div['Sin_Day'] = np.sin(2*np.pi*learning_df_div.DayofYear/365)\n",
    "learning_df_div['Cos_Day'] = np.cos(2*np.pi*learning_df_div.DayofYear/365)\n",
    "\n",
    "# 注意 inplace是在原始frame修改，返回值是Nonetype\n",
    "# learning_df = learning_df_div.drop(columns = ['Date','Month'], inplace = True)\n",
    "# learning_df = learning_df_div.drop(columns = ['Date','Month'])\n",
    "learning_df = learning_df_div.drop(columns = ['Date'])\n",
    "# learning_df.drop(columns = ['Date'], inplace = True)"
   ],
   "execution_count": 221,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:43.582689Z",
     "start_time": "2025-02-02T10:51:43.552491Z"
    }
   },
   "cell_type": "code",
   "source": "learning_df",
   "id": "7e7bc241",
   "execution_count": 222,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:43.842801Z",
     "start_time": "2025-02-02T10:51:43.802717Z"
    }
   },
   "cell_type": "code",
   "source": "# For Test\n",
   "id": "866fbac3337d0235",
   "execution_count": 223,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b14da74e",
   "metadata": {},
   "source": [
    "### 2.3 Feature Engineering <br>\n",
    "* $\\phi(x)$ feature transformation $\\Rightarrow$ last match result, win/loss streak to date, wins to season date\n",
    "* $\\phi(x)$ feature engineering $\\Rightarrow$ average the home, away, and draw odds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843342cb",
   "metadata": {},
   "source": [
    "**2.3.1 Last Match Result** <br>\n",
    "Indicate the result from the last match played between both teams"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 定义一个函数来计算两队之间上一场比赛的结果\n",
    "def compute_last_matches(df):\n",
    "    \n",
    "    unique_matchups = list(set((list(zip(df.HomeTeam, df.AwayTeam)))))\n",
    "    df['Last Match Result'] = np.nan\n",
    "    for home, away in unique_matchups:\n",
    "        matchup_df = df[(df.HomeTeam == home) & (df.AwayTeam == away)]\n",
    "        # 使用 shift(1) 方法将 FTR（全场比赛结果）列中的数据向下移动一行，这样每行的 last_match_result 将对应于这两队之前的一场比赛的结果。fill_value='Na' 确保了数据移动后空出的位置填充为 'Na'。\n",
    "        # last_match_result = matchup_df.FTR.shift(1, fill_value='Na')\n",
    "        last_match_result = matchup_df.easy_label.shift(1, fill_value='Na')\n",
    "        df.loc[matchup_df.index, 'Last Match Result'] = last_match_result\n",
    "        \n",
    "    lmr_encoder = LabelEncoder()\n",
    "    df['Last Match Result'] = lmr_encoder.fit_transform(df['Last Match Result'])\n",
    "    df.drop(columns = ['easy_label'], inplace = True)\n",
    "    return df"
   ],
   "id": "4b25dbdd59e2d17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:44.004610Z",
     "start_time": "2025-02-02T10:51:43.976285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_last_n_matches(df, n=5):\n",
    "    unique_matchups = list(set(zip(df.HomeTeam, df.AwayTeam)))\n",
    "    df['Last 5 Match Results'] = np.nan  # 新增一列用于存储过去 5 场比赛的结果\n",
    "    \n",
    "    for home, away in unique_matchups:\n",
    "        matchup_df = df[(df.HomeTeam == home) & (df.AwayTeam == away)]\n",
    "        \n",
    "        # 获取过去 n 场比赛的结果\n",
    "        # last_n_results = [matchup_df.FTR.shift(i, fill_value='Na') for i in range(1, n+1)]\n",
    "        last_n_results = [matchup_df.easy_label.shift(i, fill_value='Na') for i in range(1, n+1)]\n",
    "        \n",
    "        # 将计算得到的过去 n 场比赛的结果合并为一个字符串或列表，取决于需求\n",
    "        # 这里使用字符串形式：'result1/result2/...'\n",
    "        matchup_df['Last 5 Match Results'] = pd.DataFrame(last_n_results).T.apply(lambda x: '/'.join(x), axis=1)\n",
    "        \n",
    "        # 将计算得到的结果更新回原始 df 中\n",
    "        df.loc[matchup_df.index, 'Last 5 Match Results'] = matchup_df['Last 5 Match Results']\n",
    "    \n",
    "    # 对 Last 5 Match Results 列进行标签编码\n",
    "    lmr_encoder = LabelEncoder()\n",
    "    df['Last 5 Match Results'] = lmr_encoder.fit_transform(df['Last 5 Match Results'])\n",
    "    \n",
    "    # 删除原始的 FTR 列\n",
    "    df.drop(columns=['easy_label'], inplace=True)\n",
    "    \n",
    "    return df\n"
   ],
   "id": "bee90a274731b07e",
   "execution_count": 224,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "751bdc97",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:48.713334Z",
     "start_time": "2025-02-02T10:51:44.164657Z"
    }
   },
   "source": [
    "# 定义一个函数来计算两队之间上一场比赛的结果\n",
    "def compute_last_matches(df):\n",
    "    \n",
    "    unique_matchups = list(set((list(zip(df.HomeTeam, df.AwayTeam)))))\n",
    "    df['Last Match Result'] = np.nan\n",
    "    for home, away in unique_matchups:\n",
    "        matchup_df = df[(df.HomeTeam == home) & (df.AwayTeam == away)]\n",
    "        # 使用 shift(1) 方法将 FTR（全场比赛结果）列中的数据向下移动一行，这样每行的 last_match_result 将对应于这两队之前的一场比赛的结果。fill_value='Na' 确保了数据移动后空出的位置填充为 'Na'。\n",
    "        # last_match_result = matchup_df.FTR.shift(1, fill_value='Na')\n",
    "        # 因为easy_label 不适合作为上次比较结果\n",
    "        last_match_result = matchup_df.Result.shift(1, fill_value=3)\n",
    "        # last_match_result = matchup_df.Result_FTR.shift(1, fill_value=3)\n",
    "        df.loc[matchup_df.index, 'Last Match Result'] = last_match_result\n",
    "        \n",
    "    lmr_encoder = LabelEncoder()\n",
    "    df['Last Match Result'] = lmr_encoder.fit_transform(df['Last Match Result'])\n",
    "    df.drop(columns = ['easy_label'], inplace = True)\n",
    "    df.drop(columns = ['FTR'], inplace = True)\n",
    "    return df\n",
    "learning_df = compute_last_matches(learning_df)\n",
    "# learning_df.drop(columns = ['FTR'], inplace = True)"
   ],
   "execution_count": 225,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:48.734684Z",
     "start_time": "2025-02-02T10:51:48.714342Z"
    }
   },
   "cell_type": "code",
   "source": "learning_df",
   "id": "39989f3e6bc102f2",
   "execution_count": 226,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8ff554cf",
   "metadata": {},
   "source": [
    "**2.3.2 Home and Away Win/Loss Streak** <br>\n",
    "Important note about this feature: the win/loss streak is the teams *home* and *away* win streak, *not* its ***consecutive*** win/loss streak."
   ]
  },
  {
   "cell_type": "code",
   "id": "41f1fcc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:48.743868Z",
     "start_time": "2025-02-02T10:51:48.734684Z"
    }
   },
   "source": [
    "# https://stackoverflow.com/questions/52976336/compute-winning-streak-with-pandas\n",
    "# https://joshdevlin.com/blog/calculate-streaks-in-pandas/"
   ],
   "execution_count": 227,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4658caa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:48.757930Z",
     "start_time": "2025-02-02T10:51:48.744385Z"
    }
   },
   "source": [
    "def compute_winstreak(df):\n",
    "    \n",
    "    years = df.Year.unique()\n",
    "    df_lst = []    \n",
    "    for year in years:\n",
    "        \n",
    "        year_df = df[df.Year == year]\n",
    "        year_df['HomeWin'] = year_df.Result.replace([0, 1, 2], [0, 0, 1])\n",
    "        year_df['AwayWin'] = year_df.Result.replace([0, 1, 2], [1, 0, 0])\n",
    "        year_df['HomeWinStreak'] = None\n",
    "        year_df['AwayWinStreak'] = None\n",
    "        \n",
    "        hometeams = year_df.HomeTeam.unique()\n",
    "        awayteams = year_df.AwayTeam.unique()\n",
    "        if year > 2024:\n",
    "            # 将 AwayWin = 3 当作 0 来处理，保持计算连胜记录\n",
    "            year_df['HomeWin'] = year_df['HomeWin'].replace(3, 0)\n",
    "            year_df['AwayWin'] = year_df['AwayWin'].replace(3, 0)\n",
    "        \n",
    "        for team in hometeams:\n",
    "            team_df = year_df[(year_df.HomeTeam == team)]\n",
    "            team_df = team_df.sort_values(['Year', 'DayofYear'], ascending = (True, True))\n",
    "            \n",
    "            home_win_streak = 0  # 初始化连胜场数\n",
    "            streaks = []  # 用来存储每场比赛的连续胜利次数\n",
    "            for idx, row in team_df.iterrows():\n",
    "                streaks.append(home_win_streak)  # 当前场次视为未进行，记录上一场的连胜次数\n",
    "                # 计算当前场次的连胜，忽略当前比赛的胜负\n",
    "                if row['HomeWin'] == 1:  # 如果上一场比赛主队赢\n",
    "                    home_win_streak += 1  # 连胜场数递增\n",
    "                else:  # 如果上一场比赛主队输了\n",
    "                    home_win_streak = 0  # 连胜场数重置为 0\n",
    "            # 将计算出的连胜场数赋值到 DataFrame 中\n",
    "            team_df['HomeWinStreak'] = streaks\n",
    "            # 将更新后的数据回写到原 DataFrame\n",
    "            year_df.loc[team_df.index, 'HomeWinStreak'] = team_df.HomeWinStreak\n",
    "            \n",
    "            # team_grouper = (team_df.HomeWin != team_df.HomeWin.shift()).cumsum()\n",
    "            # team_df['HomeWinStreak'] = team_df[['HomeWin']].groupby(team_grouper).cumsum()\n",
    "            # team_df.loc[team_df.HomeWinStreak >0, 'HomeWinStreak'] -= 1\n",
    "            # year_df.loc[team_df.index, 'HomeWinStreak'] = team_df.HomeWinStreak\n",
    "            \n",
    "        for team in awayteams:\n",
    "            team_df = year_df[(year_df.AwayTeam == team)]\n",
    "            team_df = team_df.sort_values(['Year', 'DayofYear'], ascending = (True, True))\n",
    "\n",
    "            away_win_streak = 0  # 初始化连胜场数\n",
    "            streaks = []  # 用来存储每场比赛的连续胜利次数\n",
    "            for idx, row in team_df.iterrows():\n",
    "                streaks.append(away_win_streak)  # 当前场次视为未进行，记录上一场的连胜次数\n",
    "                # 计算当前场次的连胜，忽略当前比赛的胜负\n",
    "                if row['AwayWin'] == 1:  # 如果上一场比赛主队赢\n",
    "                    away_win_streak += 1  # 连胜场数递增\n",
    "                else:  # 如果上一场比赛主队输了\n",
    "                    away_win_streak = 0  # 连胜场数重置为 0\n",
    "            # 将计算出的连胜场数赋值到 DataFrame 中\n",
    "            team_df['AwayWinStreak'] = streaks\n",
    "            # 将更新后的数据回写到原 DataFrame\n",
    "            year_df.loc[team_df.index, 'AwayWinStreak'] = team_df.AwayWinStreak\n",
    "            \n",
    "            # team_grouper = (team_df.AwayWin != team_df.AwayWin.shift()).cumsum()\n",
    "            # team_df['AwayWinStreak'] = team_df[['AwayWin']].groupby(team_grouper).cumsum()\n",
    "            # team_df.loc[team_df.AwayWinStreak >0, 'AwayWinStreak'] -= 1\n",
    "            # year_df.loc[team_df.index, 'AwayWinStreak'] = team_df.AwayWinStreak\n",
    "            \n",
    "        df_lst.append(year_df)\n",
    "        \n",
    "    return pd.concat(df_lst, axis = 0).drop(columns = ['HomeWin', 'AwayWin'])#,'DayofYear'])"
   ],
   "execution_count": 228,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "aca53647",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:50.840589Z",
     "start_time": "2025-02-02T10:51:48.759249Z"
    }
   },
   "source": [
    "learning_df = compute_winstreak(learning_df)\n",
    "learning_df"
   ],
   "execution_count": 229,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "94e1c235",
   "metadata": {},
   "source": [
    "**2.3.4 Season Home/Away Wins to Date** <br>\n",
    "Indicate the number of wins for a team as home and away to date within current season"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "toy = learning_df[(learning_df.Year == 2010) & (learning_df.HomeTeam == 'Barcelona')][['HomeTeam', 'AwayTeam', 'Result']]\n",
    "toy['HomeWin'] = toy.Result.replace([0, 1, 2], [0, 0, 1])\n",
    "toy['HomeWinsToDate'] = toy.HomeWin.cumsum()"
   ],
   "id": "2e5f10c06aee07e5"
  },
  {
   "cell_type": "code",
   "id": "885dc94a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:50.855144Z",
     "start_time": "2025-02-02T10:51:50.841094Z"
    }
   },
   "source": [
    "def compute_winstodate(df):\n",
    "    \n",
    "    years = df.Year.unique()\n",
    "    df_lst = []    \n",
    "    for year in years:\n",
    "        \n",
    "        year_df = df[df.Year == year]\n",
    "        year_df['HomeWin'] = year_df.Result.replace([0, 1, 2], [0, 0, 1])\n",
    "        year_df['AwayWin'] = year_df.Result.replace([0, 1, 2], [1, 0, 0])\n",
    "        year_df['HomeWinsToDate'] = None\n",
    "        year_df['AwayWinsToDate'] = None\n",
    "        \n",
    "        hometeams = year_df.HomeTeam.unique()\n",
    "        awayteams = year_df.AwayTeam.unique()\n",
    "        if year > 2024:\n",
    "            # 将 AwayWin = 3 当作 0 来处理，保持计算连胜记录\n",
    "            year_df['HomeWin'] = year_df['HomeWin'].replace(3, 0)\n",
    "            year_df['AwayWin'] = year_df['AwayWin'].replace(3, 0)\n",
    "            \n",
    "        for team in hometeams:\n",
    "            team_df = year_df[(year_df.HomeTeam == team)]\n",
    "            team_df = team_df.sort_values(['Year', 'DayofYear'], ascending = (True, True))\n",
    "\n",
    "            # 计算截至当前场次之前的累计胜利次数（不包含当前场次）\n",
    "            team_df['HomeWinsToDate'] = team_df.HomeWin.shift(1).cumsum()\n",
    "            # 填充 NaN 值为 0，因为第一场比赛没有上一场比赛的数据\n",
    "            team_df['HomeWinsToDate'].fillna(0, inplace=True)\n",
    "            # 将更新后的数据回写到原 DataFrame\n",
    "            year_df.loc[team_df.index, 'HomeWinsToDate'] = team_df.HomeWinsToDate\n",
    "    \n",
    "            # team_df['HomeWinsToDate'] = team_df.HomeWin.cumsum()\n",
    "            # year_df.loc[team_df.index, 'HomeWinsToDate'] = team_df.HomeWinsToDate\n",
    "            \n",
    "        for team in awayteams:\n",
    "            team_df = year_df[(year_df.AwayTeam == team)]\n",
    "            team_df = team_df.sort_values(['Year', 'DayofYear'], ascending = (True, True))\n",
    "            \n",
    "            # 计算截至当前场次之前的累计胜利次数（不包含当前场次）\n",
    "            team_df['AwayWinsToDate'] = team_df.AwayWin.shift(1).cumsum()\n",
    "            # 填充 NaN 值为 0，因为第一场比赛没有上一场比赛的数据\n",
    "            team_df['AwayWinsToDate'].fillna(0, inplace=True)\n",
    "            # 将更新后的数据回写到原 DataFrame\n",
    "            year_df.loc[team_df.index, 'AwayWinsToDate'] = team_df.AwayWinsToDate\n",
    "            \n",
    "            # team_df['AwayWinsToDate'] = team_df.AwayWin.cumsum()\n",
    "            # year_df.loc[team_df.index, 'AwayWinsToDate'] = team_df.AwayWinsToDate\n",
    "            \n",
    "        df_lst.append(year_df)\n",
    "        \n",
    "    return pd.concat(df_lst, axis = 0).drop(columns = ['HomeWin', 'AwayWin','DayofYear'])"
   ],
   "execution_count": 230,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "b60fbbf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:52.601486Z",
     "start_time": "2025-02-02T10:51:50.855144Z"
    }
   },
   "source": [
    "learning_df = compute_winstodate(learning_df)\n",
    "learning_df.drop(columns = ['HomeTeam', 'AwayTeam'], inplace = True)"
   ],
   "execution_count": 231,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:52.624981Z",
     "start_time": "2025-02-02T10:51:52.602495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# learning_df\n",
    "learning_df"
   ],
   "id": "b23f5c1f7a05744b",
   "execution_count": 232,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:52.648784Z",
     "start_time": "2025-02-02T10:51:52.625986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 保存为pkl文件\n",
    "learning_df.to_pickle('E:/Data/PKL/learning_df.pkl')"
   ],
   "id": "749b75afd7a2bc77",
   "execution_count": 233,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "36c2496f",
   "metadata": {},
   "source": [
    "**2.3.5 Website Odds** <br>\n",
    "The `betting odds` recorded by various betting websites offer insight into sentiment surrounding the outcome of a particular game. "
   ]
  },
  {
   "cell_type": "code",
   "id": "1efc8290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:52.659174Z",
     "start_time": "2025-02-02T10:51:52.649789Z"
    }
   },
   "source": [
    "# betting_feats = ['B365H', 'B365D', 'B365A', 'IWH', 'IWD', 'IWA', 'WHH', 'WHD', 'WHA', \"AHh\", \"B365AHH\", \"B365AHA\"]\n",
    "betting_feats = ['B365H', 'B365D', 'B365A']\n",
    "betting_feats_asia = ['AHh', 'B365AHH', 'B365AHA', 'AHCh', 'B365CAHH', 'B365CAHA']\n",
    "betting_feats"
   ],
   "execution_count": 234,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "031548ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:52.667993Z",
     "start_time": "2025-02-02T10:51:52.660179Z"
    }
   },
   "source": [
    "def compute_meanodds(df, betting_feats):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    home_odds = []\n",
    "    away_odds = []\n",
    "    draw_odds = []\n",
    "    for odd in betting_feats:\n",
    "        odd_type = odd[-1]\n",
    "        if odd_type == 'H':\n",
    "            home_odds.append(odd)\n",
    "        elif odd_type == 'A':\n",
    "            away_odds.append(odd)\n",
    "        else:\n",
    "            draw_odds.append(odd)\n",
    "    avg_home_odds = df[home_odds].mean(axis=1)\n",
    "    avg_away_odds = df[away_odds].mean(axis=1)\n",
    "    avg_draw_odds = df[draw_odds].mean(axis=1)\n",
    "    \n",
    "    ordered_cols = ['HomeOdds', 'AwayOdds', 'DrawOdds'] + df.columns.tolist()\n",
    "    \n",
    "    df['HomeOdds'] = avg_home_odds\n",
    "    df['AwayOdds'] = avg_away_odds\n",
    "    df['DrawOdds'] = avg_draw_odds\n",
    "    \n",
    "    return df[ordered_cols]"
   ],
   "execution_count": 235,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "08e0a28b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:52.688567Z",
     "start_time": "2025-02-02T10:51:52.668998Z"
    }
   },
   "source": [
    "learning_df = compute_meanodds(learning_df, betting_feats)"
   ],
   "execution_count": 236,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0e4768c8",
   "metadata": {},
   "source": [
    "### 2.4 Peek @ Learning DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "id": "b54348f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:52.716626Z",
     "start_time": "2025-02-02T10:51:52.689577Z"
    }
   },
   "source": [
    "learning_df"
   ],
   "execution_count": 237,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:52.736073Z",
     "start_time": "2025-02-02T10:51:52.718638Z"
    }
   },
   "cell_type": "code",
   "source": "learning_df.drop(columns = ['WHH', 'WHD', 'WHA', 'HomeOdds', 'AwayOdds', 'DrawOdds', 'FTHG', 'FTAG', 'Result_FTR'], inplace = True)",
   "id": "228a748b486ff190",
   "execution_count": 238,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7b46d936",
   "metadata": {},
   "source": [
    "# 3. Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e10788",
   "metadata": {},
   "source": [
    "* Establish a baseline Logistic Regression model fit over the entire learning dataframe without special regard to *division* and *team*. \n",
    "* Train model over 16 seasons, and predict for the remaining 3 seasons (approximate 80-20 split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ddd44e",
   "metadata": {},
   "source": [
    "### 3.1 Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "id": "a82f4a6b",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:52.744958Z",
     "start_time": "2025-02-02T10:51:52.737078Z"
    }
   },
   "source": [
    "split = 0.80\n",
    "no_seasons = 20\n",
    "\n",
    "print('No. seasons to train over: ' + str(round(split*no_seasons)))"
   ],
   "execution_count": 239,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f22ecde1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:52.764103Z",
     "start_time": "2025-02-02T10:51:52.745964Z"
    }
   },
   "source": "X, y = learning_df.loc[:, learning_df.columns != 'Result'], learning_df[['Result']]",
   "execution_count": 240,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "60373664",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:52.772853Z",
     "start_time": "2025-02-02T10:51:52.765612Z"
    }
   },
   "source": [
    "# full_feat = ['HomeWinStreak','AwayWinStreak','HomeWinsToDate', 'AwayWinsToDate', 'Last Match Result',\n",
    "#              'HomeTeamELO', 'AwayTeamELO', 'HomeOdds', 'AwayOdds', 'DrawOdds'] + betting_feats\n",
    "\n",
    "# exclude_feats = ['HomeWinsToDate', 'AwayWinsToDate', 'Last Match Result'] "
   ],
   "execution_count": 241,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "1db1f4fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:52.783882Z",
     "start_time": "2025-02-02T10:51:52.772853Z"
    }
   },
   "source": [
    "# X = X[X.columns[~X.columns.isin(exclude_feats)]]\n",
    "# X"
   ],
   "execution_count": 242,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4c621fff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:52.808093Z",
     "start_time": "2025-02-02T10:51:52.784396Z"
    }
   },
   "source": [
    "X"
   ],
   "execution_count": 243,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:52.819196Z",
     "start_time": "2025-02-02T10:51:52.809099Z"
    }
   },
   "cell_type": "code",
   "source": "y",
   "id": "232473f400896cfa",
   "execution_count": 244,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:52.828355Z",
     "start_time": "2025-02-02T10:51:52.820704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "split_year = 2024\n",
    "# start_year = split_year - 15\n",
    "start_year = 2019\n",
    "split_month = 12 - 7"
   ],
   "id": "44dc55d09c257a7d",
   "execution_count": 245,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0e361977",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:52.847719Z",
     "start_time": "2025-02-02T10:51:52.829359Z"
    }
   },
   "source": [
    "# 切分训练集和测试集\n",
    "# xTr, xTe = X[(X.Year < split_year) & (X.Year >= start_year)], X[X.Year >= split_year]\n",
    "# yTr, yTe = y.loc[xTr.index, :], y.loc[xTe.index, :]\n",
    "# 切分训练集和测试集\n",
    "xTr, xTe = X[((X.Year < split_year) & (X.Year >= start_year))\n",
    " | ((X.Year == split_year) & (X.Month < split_month)) ], X[(X.Year > split_year) | ((X.Year == split_year) & (X.Month >= split_month)) ]\n",
    "yTr, yTe = y.loc[xTr.index, :], y.loc[xTe.index, :]"
   ],
   "execution_count": 246,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2 Normalization <br>\n",
    "Following our various feature transformations and development, we arrived to a sparse dataframe with the exception of a few features(*Year, DayofYear*). It will be important to *normalize* these features as they are in gross magnitudes compared to the remaining features. During model training, having dominating features (in scale relative to others) can be dangerous as the weight updates may mistakengly favor these larger-scale features because it will have the largest influence on the target output. "
   ],
   "id": "50636cdc8bb898d6"
  },
  {
   "cell_type": "code",
   "id": "a72d269a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:52.867755Z",
     "start_time": "2025-02-02T10:51:52.848726Z"
    }
   },
   "source": [
    "# minmax_scaler.fit_transform()：这个方法首先拟合数据，即计算数据的最小值和最大值，这些值用于后续的缩放。然后，它将这些参数用于转换数据，将原始数据缩放到0和1之间。\n",
    "# minmax_scaler.transform()：这个方法使用在训练数据上计算得到的最小值和最大值来转换测试数据。这确保了训练数据和测试数据使用相同的缩放标准。\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "xTr.loc[:, ['Year']] = minmax_scaler.fit_transform(xTr.loc[:, ['Year']])\n",
    "xTe.loc[:, ['Year']] = minmax_scaler.transform(xTe.loc[:, ['Year']])\n",
    "# 保存到文件\n",
    "import time\n",
    "local_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
    "joblib.dump(minmax_scaler, f'minmax_scaler_{local_time}.pkl')  # 保存为 .pkl 文件"
   ],
   "execution_count": 247,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "11ee9c78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:52.901236Z",
     "start_time": "2025-02-02T10:51:52.869265Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "# to_scale = ['HomeWinStreak','AwayWinStreak','HomeWinsToDate', 'AwayWinsToDate', 'HomeTeamELO', 'AwayTeamELO', 'HomeOdds', 'AwayOdds', 'DrawOdds'] + betting_feats\n",
    "to_scale = ['HomeTeamELO', 'AwayTeamELO'] + betting_feats\n",
    "\n",
    "xTr.loc[:, to_scale] = std_scaler.fit_transform(xTr.loc[:, to_scale])\n",
    "xTe.loc[:, to_scale] = std_scaler.transform(xTe.loc[:, to_scale])\n",
    "# 保存到文件\n",
    "import time\n",
    "local_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
    "joblib.dump(std_scaler, f'std_scaler_{local_time}.pkl')  # 保存为 .pkl 文件"
   ],
   "execution_count": 248,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "76792bc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:52.962014Z",
     "start_time": "2025-02-02T10:51:52.902915Z"
    }
   },
   "source": [
    "xTr"
   ],
   "execution_count": 249,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4696b592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:53.020949Z",
     "start_time": "2025-02-02T10:51:52.963017Z"
    }
   },
   "source": [
    "xTe"
   ],
   "execution_count": 250,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8b0abe45",
   "metadata": {},
   "source": [
    "### 3.3 HomeWins Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "c8002824",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:53.056038Z",
     "start_time": "2025-02-02T10:51:53.021953Z"
    }
   },
   "source": [
    "from sklearn.metrics import accuracy_score"
   ],
   "execution_count": 251,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:53.091209Z",
     "start_time": "2025-02-02T10:51:53.057041Z"
    }
   },
   "cell_type": "code",
   "source": "xTr.shape",
   "id": "bdd8d4925bb687b9",
   "execution_count": 252,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:53.117606Z",
     "start_time": "2025-02-02T10:51:53.091727Z"
    }
   },
   "cell_type": "code",
   "source": "xTe.shape",
   "id": "53f767c6fd6ea837",
   "execution_count": 253,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f13115e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:53.146719Z",
     "start_time": "2025-02-02T10:51:53.118616Z"
    }
   },
   "source": [
    "# training score\n",
    "baseline_Tr = np.full((xTr.shape[0], 1), 2) \n",
    "accuracy_score(yTr.Result.values, baseline_Tr.ravel())"
   ],
   "execution_count": 254,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "16d2cf5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:51:53.174989Z",
     "start_time": "2025-02-02T10:51:53.147723Z"
    }
   },
   "source": [
    "# testing score\n",
    "baseline_preds_Te = np.full((xTe.shape[0]  , 1), 2) #predicts home wins all the time\n",
    "accuracy_score(yTe.Result.values, baseline_preds_Te.ravel())"
   ],
   "execution_count": 255,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dff87ca8",
   "metadata": {},
   "source": [
    "### 3.4 Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e03fdd6",
   "metadata": {},
   "source": [
    "**3.4.1** $l2$ Regularized"
   ]
  },
  {
   "cell_type": "code",
   "id": "6c5f52f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:52:47.016815Z",
     "start_time": "2025-02-02T10:51:53.175995Z"
    }
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "l2_lr = LogisticRegression(max_iter = 10000, n_jobs=-1).fit(xTr, yTr.values.ravel())"
   ],
   "execution_count": 256,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "54140ce0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:52:47.117144Z",
     "start_time": "2025-02-02T10:52:47.017965Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, l2_lr.predict(xTr))"
   ],
   "execution_count": 257,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8c59602e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:52:47.136076Z",
     "start_time": "2025-02-02T10:52:47.118149Z"
    }
   },
   "source": [
    "# testing score\n",
    "lr_preds = l2_lr.predict(xTe)\n",
    "accuracy_score(yTe.Result.values, lr_preds)"
   ],
   "execution_count": 258,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8adcb166",
   "metadata": {},
   "source": [
    "**3.4.1** $l2$ Penalty Tuning"
   ]
  },
  {
   "cell_type": "code",
   "id": "3009cb8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:53:59.218333Z",
     "start_time": "2025-02-02T10:52:47.137084Z"
    }
   },
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "logistic_params = {'C':[0.001,0.01,0.10]}\n",
    "\n",
    "# logistic_randsearch = RandomizedSearchCV(estimator=LogisticRegression(max_iter=10000),\n",
    "#                                          param_distributions=logistic_params,\n",
    "logistic_randsearch = GridSearchCV(estimator=LogisticRegression(max_iter=10000),\n",
    "                                         param_grid=logistic_params,\n",
    "                                         scoring='accuracy',\n",
    "                                         verbose=1,\n",
    "                                         cv=5,\n",
    "                                         n_jobs=-1)\n",
    "\n",
    "logistic_rand_results = logistic_randsearch.fit(xTr, yTr.values.ravel())\n",
    "print(\"Best: %f using %s\" % (logistic_rand_results.best_score_, logistic_rand_results.best_params_))"
   ],
   "execution_count": 259,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a9231529",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:53:59.233464Z",
     "start_time": "2025-02-02T10:53:59.218942Z"
    }
   },
   "source": [
    "l2_rs = logistic_rand_results.best_estimator_"
   ],
   "execution_count": 260,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9197a0f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:53:59.337270Z",
     "start_time": "2025-02-02T10:53:59.234533Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, l2_rs.predict(xTr))"
   ],
   "execution_count": 261,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5c45f431",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:53:59.355439Z",
     "start_time": "2025-02-02T10:53:59.338279Z"
    }
   },
   "source": [
    "# testing score\n",
    "accuracy_score(yTe.Result.values, l2_rs.predict(xTe))"
   ],
   "execution_count": 262,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "febcc06f",
   "metadata": {},
   "source": [
    "**3.4.4** $l1$ Regularized"
   ]
  },
  {
   "cell_type": "code",
   "id": "d9502351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:54:50.475425Z",
     "start_time": "2025-02-02T10:53:59.356443Z"
    }
   },
   "source": [
    "l1_lr = LogisticRegression(penalty='l1', solver='saga', max_iter = 10000, n_jobs=-1).fit(xTr, yTr.values.ravel())"
   ],
   "execution_count": 263,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "82d75648",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:54:50.576656Z",
     "start_time": "2025-02-02T10:54:50.476441Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, l1_lr.predict(xTr))"
   ],
   "execution_count": 264,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4faea9e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:54:50.596260Z",
     "start_time": "2025-02-02T10:54:50.579670Z"
    }
   },
   "source": [
    "# testing score\n",
    "l1_preds = l1_lr.predict(xTe)\n",
    "accuracy_score(yTe.Result.values, l1_preds)"
   ],
   "execution_count": 265,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "445dcdb4",
   "metadata": {},
   "source": [
    "**3.4.5** Penalty Tuning"
   ]
  },
  {
   "cell_type": "code",
   "id": "3314235e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:55:31.254368Z",
     "start_time": "2025-02-02T10:54:50.596260Z"
    }
   },
   "source": [
    "l1_params = {'C':[0.001,0.01,0.10]}\n",
    "\n",
    "# l1_randsearch = RandomizedSearchCV(estimator=LogisticRegression(penalty='l1',solver='saga', max_iter=10000),\n",
    "#                                          param_distributions=l1_params,\n",
    "l1_randsearch = GridSearchCV(estimator=LogisticRegression(penalty='l1',solver='saga', max_iter=10000),\n",
    "                                         param_grid=l1_params,\n",
    "                                         scoring='accuracy',\n",
    "                                         verbose=1,\n",
    "                                         n_jobs=-1,\n",
    "                                         cv=5)\n",
    "\n",
    "l1_rand_results = l1_randsearch.fit(xTr, yTr.values.ravel())\n",
    "print(\"Best: %f using %s\" % (l1_rand_results.best_score_, l1_rand_results.best_params_))"
   ],
   "execution_count": 266,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f4607e83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:55:31.263916Z",
     "start_time": "2025-02-02T10:55:31.254368Z"
    }
   },
   "source": [
    "l1_rs = l1_randsearch.best_estimator_ #LogisticRegression(C=0.10, solver='saga', max_iter=10000).fit(xTr, yTr.values.ravel())#"
   ],
   "execution_count": 267,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4895d05a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:55:31.395424Z",
     "start_time": "2025-02-02T10:55:31.264467Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, l1_rs.predict(xTr))"
   ],
   "execution_count": 268,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7016a84c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:55:31.413338Z",
     "start_time": "2025-02-02T10:55:31.395945Z"
    }
   },
   "source": [
    "# testing score\n",
    "accuracy_score(yTe.Result.values, l1_rs.predict(xTe))"
   ],
   "execution_count": 269,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8b988c26",
   "metadata": {},
   "source": [
    "### 3.5 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "id": "0f4908d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:55:43.631868Z",
     "start_time": "2025-02-02T10:55:31.413338Z"
    }
   },
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(max_iter=100000).fit(xTr, yTr.values.ravel())"
   ],
   "execution_count": 270,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "1a18a45a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:55:58.314436Z",
     "start_time": "2025-02-02T10:55:43.632872Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, svm.predict(xTr))"
   ],
   "execution_count": 271,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7311630d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:55:59.312732Z",
     "start_time": "2025-02-02T10:55:58.315783Z"
    }
   },
   "source": [
    "# testing score\n",
    "accuracy_score(yTe.Result.values, svm.predict(xTe))"
   ],
   "execution_count": 272,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:56:00.249463Z",
     "start_time": "2025-02-02T10:55:59.314245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predict_svm = svm.predict(xTe)\n",
    "series_svm = pd.Series(predict_svm, name='Predicted')\n",
    "compare_result = pd.concat([series_svm, yTe.reset_index()], axis=1)\n",
    "compare_result"
   ],
   "id": "63ff47239dab82c6",
   "execution_count": 273,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:56:00.669797Z",
     "start_time": "2025-02-02T10:56:00.250473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "result_subset = compare_result.tail(200)\n",
    "plt.figure(figsize=(10,10))\n",
    "result_subset.plot(x='index', y=['Predicted', 'Result'], kind='line')\n",
    "plt.title(\"Prediction vs Real\")\n",
    "plt.show()"
   ],
   "id": "b3202c7e1788a64e",
   "execution_count": 274,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:56:00.723639Z",
     "start_time": "2025-02-02T10:56:00.670317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "local_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
    "dump(svm, f'./sklearn_svm_{local_time}.joblib')"
   ],
   "id": "619560b8158e3d4e",
   "execution_count": 275,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b2f00bed",
   "metadata": {},
   "source": [
    "**3.5.2** Penalty Tuning"
   ]
  },
  {
   "cell_type": "code",
   "id": "5b998f7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:57:28.386668Z",
     "start_time": "2025-02-02T10:56:00.725153Z"
    }
   },
   "source": [
    "svm_params = {'C':[0.001,0.01,0.10]}\n",
    "\n",
    "# svm_randsearch = RandomizedSearchCV(estimator=SVC(max_iter=100000),\n",
    "#                                          param_distributions=svm_params,\n",
    "svm_randsearch = GridSearchCV(estimator=SVC(max_iter=100000),\n",
    "                                         param_grid=svm_params,\n",
    "                                         scoring='accuracy',\n",
    "                                         verbose=2,\n",
    "                                         cv=5,\n",
    "                                         n_jobs=-1)\n",
    "\n",
    "svm_rand_results = svm_randsearch.fit(xTr, yTr.values.ravel())\n",
    "print(\"Best: %f using %s\" % (svm_rand_results.best_score_, svm_rand_results.best_params_))"
   ],
   "execution_count": 276,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7f9627ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:57:28.395930Z",
     "start_time": "2025-02-02T10:57:28.387270Z"
    }
   },
   "source": [
    "svm_rs = svm_rand_results.best_estimator_"
   ],
   "execution_count": 277,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "e32e6caf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:57:42.978858Z",
     "start_time": "2025-02-02T10:57:28.397608Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, svm_rs.predict(xTr))"
   ],
   "execution_count": 278,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "e88837b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:57:43.922075Z",
     "start_time": "2025-02-02T10:57:42.979872Z"
    }
   },
   "source": [
    "# testing score\n",
    "accuracy_score(yTe.Result.values, svm_rs.predict(xTe))"
   ],
   "execution_count": 279,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:57:43.950012Z",
     "start_time": "2025-02-02T10:57:43.923171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "local_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
    "dump(svm_rs, f'./sklearn_svm_randsearch_{local_time}.joblib')"
   ],
   "id": "2be3ad74a6193c29",
   "execution_count": 280,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "edee8858",
   "metadata": {},
   "source": [
    "### 3.6 Simple Neural Network ####"
   ]
  },
  {
   "cell_type": "code",
   "id": "d0505f91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T12:15:14.022493Z",
     "start_time": "2025-02-02T10:57:43.950526Z"
    }
   },
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(512,128,32),\n",
    "#                     activation='relu',\n",
    "#                     batch_size=512,\n",
    "#                     max_iter=10000,\n",
    "#                     learning_rate_init=1e-4,\n",
    "#                     early_stopping=True,\n",
    "#                     alpha=1e-3,\n",
    "#                    ).fit(xTr, yTr.values.ravel())\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(128,64,32),\n",
    "                    activation='relu',\n",
    "                    batch_size=64,\n",
    "                    max_iter=10000,\n",
    "                    learning_rate_init=1e-4,\n",
    "                    early_stopping=True,\n",
    "                    alpha=1e-3,\n",
    "                    verbose=True,\n",
    "                    tol=1e-4,\n",
    "                    random_state=13,\n",
    "                    learning_rate='adaptive',\n",
    "                    n_iter_no_change=300\n",
    "                   ).fit(xTr, yTr.values.ravel())"
   ],
   "execution_count": 281,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5ca78c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T12:15:14.212093Z",
     "start_time": "2025-02-02T12:15:14.025501Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, mlp.predict(xTr))"
   ],
   "execution_count": 282,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "aa884592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T12:15:14.236669Z",
     "start_time": "2025-02-02T12:15:14.213105Z"
    }
   },
   "source": [
    "# testing score\n",
    "accuracy_score(yTe.Result.values, mlp.predict(xTe))"
   ],
   "execution_count": 283,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T12:15:14.265767Z",
     "start_time": "2025-02-02T12:15:14.237175Z"
    }
   },
   "cell_type": "code",
   "source": "xTr",
   "id": "f3b1aae2279970a7",
   "execution_count": 284,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T12:15:14.276914Z",
     "start_time": "2025-02-02T12:15:14.266784Z"
    }
   },
   "cell_type": "code",
   "source": "yTr",
   "id": "e4dd2c1c61d1079c",
   "execution_count": 285,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T12:15:14.296230Z",
     "start_time": "2025-02-02T12:15:14.277920Z"
    }
   },
   "cell_type": "code",
   "source": "xTe",
   "id": "b2b865f37b49cf07",
   "execution_count": 286,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T12:15:14.309269Z",
     "start_time": "2025-02-02T12:15:14.296750Z"
    }
   },
   "cell_type": "code",
   "source": "yTe",
   "id": "a695d991d7cb690f",
   "execution_count": 287,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T12:15:14.336787Z",
     "start_time": "2025-02-02T12:15:14.309809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predict_val = mlp.predict(xTe)\n",
    "series_pre = pd.Series(predict_val, name='Predicted')\n",
    "compare_result = pd.concat([series_pre, yTe.reset_index()], axis=1)\n",
    "compare_result"
   ],
   "id": "75000857933fee73",
   "execution_count": 288,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T12:15:14.572387Z",
     "start_time": "2025-02-02T12:15:14.337818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "result_subset = compare_result.tail(200)\n",
    "plt.figure(figsize=(10,10))\n",
    "result_subset.plot(x='index', y=['Predicted', 'Result'], kind='line')\n",
    "plt.title(\"Prediction vs Real\")\n",
    "plt.show()"
   ],
   "id": "bf60fe066e7da7b",
   "execution_count": 289,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T12:15:14.580154Z",
     "start_time": "2025-02-02T12:15:14.573492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# xTr = xTr.drop(columns=['pad1', 'pad2', 'pad3'])\n",
    "# xTe = xTe.drop(columns=['pad1', 'pad2', 'pad3'])"
   ],
   "id": "75c183890ef24f2d",
   "execution_count": 290,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 保存模型",
   "id": "de0d8edbd57b1b47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T12:15:14.625676Z",
     "start_time": "2025-02-02T12:15:14.581162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "local_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
    "dump(mlp, f'./sklearn_mlp_{local_time}.joblib')"
   ],
   "id": "3112f4279bcb97e0",
   "execution_count": 291,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T12:15:14.649359Z",
     "start_time": "2025-02-02T12:15:14.626685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ### 加载模型\n",
    "# # model_name = 'sklearn_mlp_' + local_time + '.joblib'\n",
    "# model_name = './' + 'sklearn_mlp_2025_01_21_22_41_15.joblib'\n",
    "# mlp = load(model_name)"
   ],
   "id": "b01de6170bcd5bc2",
   "execution_count": 292,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T02:04:39.981521Z",
     "start_time": "2025-02-03T02:04:39.707493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 获取每个样本的类别概率\n",
    "probs = mlp.predict_proba(xTe)\n",
    "\n",
    "# 统计 top 2 类别\n",
    "top2_indices = np.argsort(probs, axis=1)[:, -2:]  # 获取每个样本概率最高的两个类别的索引\n",
    "top2_probs = np.take_along_axis(probs, top2_indices, axis=1)  # 获取这两个类别的概率\n",
    "\n",
    "print(\"Top 2 Categories for each sample:\")\n",
    "for i in range(xTe.shape[0]):\n",
    "    print(f\"Sample {i+1}: Class {top2_indices[i, 1]} (prob={top2_probs[i, 1]:.4f}), Class {top2_indices[i, 0]} (prob={top2_probs[i, 0]:.4f})\")"
   ],
   "id": "e9d19bbb71f72bae",
   "execution_count": 299,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 随机森林",
   "id": "9123750ff60e57fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T13:10:02.872608Z",
     "start_time": "2025-02-02T13:10:01.606309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 创建随机森林分类器\n",
    "rf = RandomForestClassifier(n_estimators=300,  # 100棵树\n",
    "                            max_depth=None,    # 不限制树的最大深度\n",
    "                            min_samples_split=2,  # 每个节点最小样本数\n",
    "                            random_state=42,  # 随机种子，确保结果可复现\n",
    "                            n_jobs=-1)  # 使用所有CPU核心\n",
    "\n",
    "# 训练分类器\n",
    "rf.fit(xTr, yTr.values.ravel())\n",
    "\n",
    "# 预测\n",
    "y_pred_rf = rf.predict(xTe)\n",
    "\n",
    "# 评估准确率\n",
    "accuracy_rf = accuracy_score(yTe, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf:.4f}\")"
   ],
   "id": "33ff7eaa09bf4485",
   "execution_count": 294,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T13:12:54.733536Z",
     "start_time": "2025-02-02T13:10:04.058520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 定义超参数网格\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500, 1000],  # 不同的树数\n",
    "    'max_depth': [None, 10, 20, 30],  # 树的最大深度\n",
    "}\n",
    "\n",
    "# 随机森林分类器\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# 进行网格搜索\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# 拟合网格搜索\n",
    "grid_search.fit(xTr, yTr.values.ravel())\n",
    "\n",
    "# 输出最佳参数\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_:.4f}\")"
   ],
   "id": "1f22aee22194aae7",
   "execution_count": 295,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T01:58:09.356734Z",
     "start_time": "2025-02-03T01:55:01.063976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 记录不同树数下的交叉验证准确率\n",
    "n_estimators_range = [50, 100, 200, 300, 500, 1000]\n",
    "cv_scores = []\n",
    "\n",
    "for n in n_estimators_range:\n",
    "    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "    scores = cross_val_score(rf, xTr, yTr.values.ravel(), cv=5)\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# 绘制树数与交叉验证准确率的关系\n",
    "plt.plot(n_estimators_range, cv_scores)\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Cross-validation Accuracy')\n",
    "plt.title('Random Forest: Trees vs Accuracy')\n",
    "plt.show()"
   ],
   "id": "ebce87ea3b367a15",
   "execution_count": 296,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T01:58:09.426193Z",
     "start_time": "2025-02-03T01:58:09.358247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. 获取最佳模型\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# 2. 在测试集上进行预测\n",
    "y_pred = best_rf.predict(xTe)\n",
    "\n",
    "# 3. 计算准确率\n",
    "test_accuracy = accuracy_score(yTe, y_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ],
   "id": "fd08e7311781907b",
   "execution_count": 297,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T01:58:09.438373Z",
     "start_time": "2025-02-03T01:58:09.427199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(yTe, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ],
   "id": "9d91f61fff6a0c7",
   "execution_count": 298,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### pytorch MLP",
   "id": "79ed2eb7fe513eae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 假设 xTr, yTr 已经准备好，xTe, yTe 作为测试集\n",
    "\n",
    "# 转换为Tensor\n",
    "xTr_tensor = torch.tensor(xTr, dtype=torch.float32)\n",
    "yTr_tensor = torch.tensor(yTr.values.ravel(), dtype=torch.long)  # 使用 long 类型因为是分类问题\n",
    "xTe_tensor = torch.tensor(xTe, dtype=torch.float32)\n",
    "yTe_tensor = torch.tensor(yTe.values.ravel(), dtype=torch.long)\n",
    "\n",
    "# 创建DataLoader\n",
    "train_data = TensorDataset(xTr_tensor, yTr_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# 定义神经网络结构\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, output_size))  # 输出层\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 初始化模型\n",
    "input_size = xTr.shape[1]  # 特征数\n",
    "hidden_sizes = [512, 128, 32]  # 可以根据实际需求调整\n",
    "output_size = 3  # 三分类问题\n",
    "model = MLP(input_size, hidden_sizes, output_size)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()  # 交叉熵损失函数，适用于多分类问题\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()  # 清除梯度\n",
    "        outputs = model(inputs)  # 模型前向传播\n",
    "        loss = criterion(outputs, labels)  # 计算损失\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新参数\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# 测试模型\n",
    "model.eval()  # 设置为评估模式\n",
    "with torch.no_grad():\n",
    "    y_pred = model(xTe_tensor)\n",
    "    _, predicted = torch.max(y_pred, 1)  # 获取预测类别\n",
    "    accuracy = accuracy_score(yTe_tensor.numpy(), predicted.numpy())  # 计算准确率\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ],
   "id": "aaba72c5d30a73e6",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### xgboost",
   "id": "16f231cf38649db1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 创建DMatrix（XGBoost的输入格式）\n",
    "dtrain = xgb.DMatrix(xTr, label=yTr.values.ravel())\n",
    "dtest = xgb.DMatrix(xTe)\n",
    "\n",
    "# 设置XGBoost参数\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # 多分类任务\n",
    "    'num_class': 3,  # 类别数，假设是三分类\n",
    "    'max_depth': 6,  # 树的最大深度\n",
    "    'eta': 0.05,  # 学习率\n",
    "    'subsample': 0.8,  # 数据采样比率\n",
    "    'colsample_bytree': 0.8,  # 特征采样比率\n",
    "    'eval_metric': 'mlogloss',  # 多分类对数损失\n",
    "    'gamma': 1,  # 正则化的最小损失\n",
    "    'lambda': 1,  # L2 正则化\n",
    "    'alpha': 0.5,  # L1 正则化\n",
    "    'tree_method': 'hist',  # 使用直方图算法\n",
    "    'missing': np.nan  # 处理缺失值\n",
    "}\n",
    "\n",
    "# 训练模型\n",
    "num_round = 100  # 训练轮次\n",
    "bst = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# 预测\n",
    "y_pred_xgb = bst.predict(dtest)\n",
    "\n",
    "# 评估准确率\n",
    "accuracy_xgb = accuracy_score(yTe, y_pred_xgb)\n",
    "print(f\"XGBoost Accuracy: {accuracy_xgb:.4f}\")\n"
   ],
   "id": "e283bb1fd8d5e582",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bc4656a5",
   "metadata": {},
   "source": [
    "### 3.7 Stacked Classifier ###"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import SVC"
   ],
   "id": "b630a432dc50b319"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "stacked_clf = StackingClassifier(estimators=[('svm', SVC(max_iter=100000)), ('logistic', LogisticRegression(C=0.01, max_iter=10000))],\n",
    "                                final_estimator=LogisticRegression(max_iter=10000),\n",
    "                                n_jobs=-1).fit(xTr, yTr.values.ravel())"
   ],
   "id": "d1ff3a7ac5466e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, stacked_clf.predict(xTr))"
   ],
   "id": "89cbb239a3759038"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# testing score\n",
    "accuracy_score(yTe.Result.values, stacked_clf.predict(xTe))"
   ],
   "id": "a4ddde81e2f0c7be"
  },
  {
   "cell_type": "markdown",
   "id": "e90e8cae",
   "metadata": {},
   "source": [
    "## 4. Result Analysis ##"
   ]
  },
  {
   "cell_type": "code",
   "id": "0d4a4507",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T12:15:14.679960Z",
     "start_time": "2025-02-02T12:15:14.650369Z"
    }
   },
   "source": [
    "## TODO: breakdown results across divisions and/or teams; i.e., see how model performs individually at subgroups"
   ],
   "execution_count": 293,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a10a069c",
   "metadata": {},
   "source": [
    "## 5. Scrap Code ##"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "barcelona_df = learning_df[(learning_df['HomeTeam 17'] == 1) | (learning_df['AwayTeam 17'] == 1)]\n",
    "barcelona_df"
   ],
   "id": "6062bbe60dc13f71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "bxTr = xTr[(xTr['HomeTeam 17'] == 1) | (xTr['AwayTeam 17'] == 1)]\n",
    "bxTe = xTe[(xTe['HomeTeam 17'] == 1) | (xTe['AwayTeam 17'] == 1)]"
   ],
   "id": "ebf2f4232e36abc6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "byTr, byTe = yTr.loc[bxTr.index,:], yTe.loc[bxTe.index,:]",
   "id": "605452e5abb605ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# training score\n",
    "accuracy_score(byTr, l1_lr.predict(bxTr))"
   ],
   "id": "98151321f88aef9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# testing score\n",
    "accuracy_score(byTe, l1_lr.predict(bxTe))"
   ],
   "id": "7da0be8a78c64799"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# training score\n",
    "accuracy_score(byTr, l2_lr.predict(bxTr))"
   ],
   "id": "c8b3e93d0eb50d5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# testing score\n",
    "accuracy_score(byTe, l2_lr.predict(bxTe))"
   ],
   "id": "2f0a4b0253463085"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Pytorch MLP ##",
   "id": "1a9d44ed88b506d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "type(xTr)",
   "id": "e0ea07161a4dada9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "xTr.shape",
   "id": "eab34be724a70a74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ],
   "id": "b2cd03727fc3f4a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_weights = nn.Parameter(torch.randn(feature_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 应用注意力权重\n",
    "        weights = F.softmax(self.attention_weights, dim=0)\n",
    "        # 加权求和\n",
    "        x = x * weights\n",
    "        return x\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=xTr.shape[1], out_features=512)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=512)\n",
    "        self.attention = Attention(512)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=128)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=128)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=32)\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=32)\n",
    "        self.dropout3 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc4 = nn.Linear(in_features=32, out_features=3)  # 输出层改为3，对应三个类别\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(torch.relu(self.bn1(self.fc1(x))))\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout2(torch.relu(self.bn2(self.fc2(x))))\n",
    "        x = self.dropout3(torch.relu(self.bn3(self.fc3(x))))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# 数据预处理\n",
    "scaler = StandardScaler()\n",
    "xTr_scaled = scaler.fit_transform(xTr)\n",
    "xTr_tensor = torch.tensor(xTr_scaled, dtype=torch.float32).to(device)\n",
    "yTr_tensor = torch.tensor(yTr.values.ravel(), dtype=torch.long).to(device)\n",
    "\n",
    "# 创建数据加载器\n",
    "dataset = TensorDataset(xTr_tensor, yTr_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "# 创建模型实例\n",
    "model = MLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "train_start = time.time()\n",
    "# 训练模型\n",
    "model.train()\n",
    "for epoch in range(500):  # 假设训练200个epoch\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # 清除之前的梯度\n",
    "\n",
    "        outputs = model(inputs)  # 前向传播\n",
    "        loss = criterion(outputs, labels)  # 计算损失\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新参数\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)  # 累计损失\n",
    "        _, predicted = torch.max(outputs.data, 1)  # 获取预测结果\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total * 100  # 计算准确率\n",
    "\n",
    "    # 每个epoch结束后输出\n",
    "    print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "print(f'训练时长： {time.time() - train_start}s')"
   ],
   "id": "6e7e048c04345f63"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 假设 xTe 和 yTe 是 pandas DataFrame 或 Series\n",
    "# 数据预处理\n",
    "xTe_scaled = scaler.fit_transform(xTe)  # 使用与训练数据相同的标准化参数\n",
    "xTe_tensor = torch.tensor(xTe_scaled, dtype=torch.float32).to(device)\n",
    "yTe_tensor = torch.tensor(yTe.values.ravel(), dtype=torch.long).to(device)\n",
    "\n",
    "# 创建数据加载器\n",
    "test_dataset = TensorDataset(xTe_tensor, yTe_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 初始化用于计算准确率的变量\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 不计算梯度，因为在评估模式下不需要进行反向传播\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy on test set: {accuracy * 100:.2f}%')"
   ],
   "id": "5d7583df837e05c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Pytorch Transformer ##",
   "id": "cf72af33f38ca812"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_heads, num_layers, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.model_dim = input_dim  # 通常情况下，模型维度与输入维度相同\n",
    "\n",
    "        # Transformer Encoder Layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.model_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=512,  # 前馈网络的维度\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Transformer Encoder\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # 输出层\n",
    "        self.output_layer = nn.Linear(self.model_dim, self.num_classes)\n",
    "\n",
    "        # Batch Normalization\n",
    "        self.bn = nn.BatchNorm1d(self.model_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 增加一个假的序列维度\n",
    "        x = x.unsqueeze(1)\n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Batch Normalization\n",
    "        x = self.bn(x[:, 0, :])  # 取序列的第一个元素进行批量归一化\n",
    "\n",
    "        # 输出层\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# 补充维度\n",
    "n_samples_xTr = xTr.shape[0]\n",
    "n_samples_xTe = xTe.shape[0]\n",
    "for i in range(1, 4):  # 从 1 到 3，因为需要添加三列\n",
    "    xTr[f'pad{i}'] = 0  # 添加填充列，初始化为 0\n",
    "    xTe[f'pad{i}'] = 0  # 添加填充列，初始化为 0\n",
    "\n",
    "# 参数设置\n",
    "input_dim = xTr.shape[1]  # 输入特征的维度\n",
    "num_classes = 3  # 类别数\n",
    "num_heads = 10  # 注意力头的数量\n",
    "num_layers = 3  # Transformer层的数量\n",
    "dropout = 0.8  # Dropout比率\n",
    "\n",
    "# 创建模型\n",
    "model = TransformerModel(input_dim, num_classes, num_heads, num_layers, dropout).to(device)\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 数据加载\n",
    "# 假设 xTr 和 xTe 已经是适当的 torch.Tensor 对象\n",
    "xTr_values = xTr.values.astype(float)\n",
    "xTe_values = xTe.values.astype(float)\n",
    "\n",
    "xTr_tensor = torch.tensor(xTr_values, dtype=torch.float32).to(device)\n",
    "xTe_tensor = torch.tensor(xTe_values, dtype=torch.float32).to(device)\n",
    "yTr_tensor = torch.tensor(yTr.values, dtype=torch.long).to(device).squeeze(1)\n",
    "yTe_tensor = torch.tensor(yTe.values, dtype=torch.long).to(device).squeeze(1)\n",
    "# 转换为 one-hot 编码\n",
    "yTr_tensor = F.one_hot(yTr_tensor, num_classes=num_classes).float()\n",
    "yTe_tensor = F.one_hot(yTe_tensor, num_classes=num_classes).float()\n",
    "\n",
    "# 数据加载器\n",
    "train_dataset = TensorDataset(xTr_tensor, yTr_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(xTe_tensor, yTe_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "train_start = time.time()\n",
    "# 训练模型\n",
    "model.train()\n",
    "for epoch in range(500):  # 运行更多的 epoch 以获得更好的结果\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)  # 累计损失\n",
    "        _, predicted = torch.max(outputs.data, 1)  # 获取预测结果\n",
    "        _, truth = torch.max(labels.data, 1)\n",
    "        total += truth.size(0)\n",
    "        correct += (predicted == truth).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total * 100  # 计算准确率\n",
    "\n",
    "    # 每个epoch结束后输出\n",
    "    print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "print(f'训练时长： {time.time() - train_start}s')\n",
    "\n",
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 初始化用于计算准确率的变量\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 不计算梯度，因为在评估模式下不需要进行反向传播\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, truth = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == truth).sum().item()\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy on test set: {accuracy * 100:.2f}%')"
   ],
   "id": "7c086a748d381401"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
