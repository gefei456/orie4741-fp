{
 "cells": [
  {
   "cell_type": "code",
   "id": "6e504281",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:19.208891Z",
     "start_time": "2024-12-16T01:12:19.197736Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np"
   ],
   "execution_count": 270,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a341b52b",
   "metadata": {},
   "source": [
    "## 0. DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "id": "e9469f20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:21.203638Z",
     "start_time": "2024-12-16T01:12:21.102620Z"
    }
   },
   "source": [
    "football_df = pd.read_csv('data/all_data_with_elo.csv', low_memory = False)\n",
    "football_df"
   ],
   "execution_count": 271,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fcbbc3ce",
   "metadata": {},
   "source": [
    "## 1. Descriptive Statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b828a5",
   "metadata": {},
   "source": [
    "**1.1 DataFrame Shape**"
   ]
  },
  {
   "cell_type": "code",
   "id": "15427373",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:21.214949Z",
     "start_time": "2024-12-16T01:12:21.204643Z"
    }
   },
   "source": [
    "# no. rows and no. cols\n",
    "football_df.shape"
   ],
   "execution_count": 272,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7b649b14",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:21.313317Z",
     "start_time": "2024-12-16T01:12:21.302796Z"
    }
   },
   "source": [
    "# feature names\n",
    "print(football_df.columns.tolist())"
   ],
   "execution_count": 273,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fdb560ee",
   "metadata": {},
   "source": [
    "**1.2 NaN Values**"
   ]
  },
  {
   "cell_type": "code",
   "id": "d3377302",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:21.374139Z",
     "start_time": "2024-12-16T01:12:21.360966Z"
    }
   },
   "source": [
    "football_df.isnull().sum()"
   ],
   "execution_count": 274,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7eae5438",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:21.552923Z",
     "start_time": "2024-12-16T01:12:21.542906Z"
    }
   },
   "source": [
    "# total elements in \n",
    "football_df.size"
   ],
   "execution_count": 275,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2afbc469",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:21.674684Z",
     "start_time": "2024-12-16T01:12:21.662716Z"
    }
   },
   "source": [
    "# total number of NaN\n",
    "football_df.size - football_df.count().sum()"
   ],
   "execution_count": 276,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a9b3446d",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:22.011247Z",
     "start_time": "2024-12-16T01:12:21.998338Z"
    }
   },
   "source": [
    "# total number of NaN rows\n",
    "football_df.isnull().any(axis = 1).sum()"
   ],
   "execution_count": 277,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "12fcf2ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:22.111827Z",
     "start_time": "2024-12-16T01:12:22.098136Z"
    }
   },
   "source": [
    "# total number of NaN columns\n",
    "football_df.isnull().any(axis = 0).sum()"
   ],
   "execution_count": 278,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "da2853e2",
   "metadata": {},
   "source": [
    "## 2. Data Wrangling and Feature Transformation/Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cde42d8",
   "metadata": {},
   "source": [
    "### 2.1 NaN Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a34fc35",
   "metadata": {},
   "source": [
    "`TODO`: drop NaN values along columns: {Date, Home Team, Away Team, FTR} <br>\n",
    "`TODO`: identify betting odds w/ most available data"
   ]
  },
  {
   "cell_type": "code",
   "id": "7f75f7d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:22.218992Z",
     "start_time": "2024-12-16T01:12:22.203643Z"
    }
   },
   "source": [
    "# 当前方法仅提取这几个字段 分区 日期 主队 客队 full-time-result 三家机构的胜平负 主队ELO评分 客队ELO评分\n",
    "# nan_mask = ['Div', 'Date', 'HomeTeam', 'AwayTeam', 'FTR', 'B365H', 'B365D', 'B365A', \n",
    "#             'IWH', 'IWD', 'IWA', 'WHH', 'WHD', 'WHA', 'AHh', 'B365AHH', 'B365AHA', 'HomeTeamELO', 'AwayTeamELO']\n",
    "nan_mask = ['Div', 'Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'B365H', 'B365D', 'B365A', \n",
    "            'IWH', 'IWD', 'IWA', 'WHH', 'WHD', 'WHA', 'AHh', 'B365AHH', 'B365AHA', 'HomeTeamELO', 'AwayTeamELO']"
   ],
   "execution_count": 279,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:22.313552Z",
     "start_time": "2024-12-16T01:12:22.301268Z"
    }
   },
   "cell_type": "code",
   "source": "asia_mask = ['Div', 'Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'AHh', 'B365AHH', 'B365AHA', 'HomeTeamELO', 'AwayTeamELO']",
   "id": "e9cb444cd412e510",
   "execution_count": 280,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:22.490490Z",
     "start_time": "2024-12-16T01:12:22.468064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "asia_football_df = football_df[asia_mask]\n",
    "asia_football_df"
   ],
   "id": "c5f40ebae6d39e68",
   "execution_count": 281,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:22.585158Z",
     "start_time": "2024-12-16T01:12:22.565798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "asia_football_df['asia_final_result'] = asia_football_df['FTHG'] - asia_football_df['FTAG'] + asia_football_df['AHh']\n",
    "asia_football_df"
   ],
   "id": "11c29f5924e5d7a1",
   "execution_count": 282,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:22.734339Z",
     "start_time": "2024-12-16T01:12:22.712906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "asia_football_df_noNone = asia_football_df.dropna()\n",
    "asia_football_df_noNone"
   ],
   "id": "d1bdaa50359f4dd6",
   "execution_count": 283,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:22.850833Z",
     "start_time": "2024-12-16T01:12:22.835325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "asia_football_df_noNone.reset_index(inplace=True, drop=True)\n",
    "asia_football_df_noNone"
   ],
   "id": "fd25a49ce3655cb6",
   "execution_count": 284,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:23.169162Z",
     "start_time": "2024-12-16T01:12:23.145527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conditions = [\n",
    "    asia_football_df_noNone['asia_final_result'] < -0.25,\n",
    "    asia_football_df_noNone['asia_final_result'] == -0.25,\n",
    "    asia_football_df_noNone['asia_final_result'] == 0,\n",
    "    asia_football_df_noNone['asia_final_result'] == 0.25,\n",
    "    asia_football_df_noNone['asia_final_result'] > 0.25,\n",
    "]\n",
    "easy_conditions = [\n",
    "    asia_football_df_noNone['asia_final_result'] <= -0.25,\n",
    "    asia_football_df_noNone['asia_final_result'] == 0,\n",
    "    asia_football_df_noNone['asia_final_result'] >= 0.25,\n",
    "]\n",
    "labels = [-2, -1, 0, 1, 2]\n",
    "easy_labels = [-1, 0, 1]\n",
    "\n",
    "asia_football_df_noNone['label'] = np.select(conditions, labels)\n",
    "asia_football_df_noNone['easy_label'] = np.select(easy_conditions, easy_labels)\n",
    "asia_football_df_noNone"
   ],
   "id": "143b539d11ec3c73",
   "execution_count": 285,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:23.408298Z",
     "start_time": "2024-12-16T01:12:23.393895Z"
    }
   },
   "cell_type": "code",
   "source": "asia_football_df_noNone['label'].mean()",
   "id": "f300485c34204a70",
   "execution_count": 286,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:23.932738Z",
     "start_time": "2024-12-16T01:12:23.921225Z"
    }
   },
   "cell_type": "code",
   "source": "asia_football_df_noNone['easy_label'].mean()",
   "id": "de705b485595798e",
   "execution_count": 287,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:24.328353Z",
     "start_time": "2024-12-16T01:12:24.308970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "asia_football_df_noNone_E0 = asia_football_df_noNone[asia_football_df_noNone['Div'] == 'E0']\n",
    "asia_football_df_noNone_E0"
   ],
   "id": "345fdf32b8ae02f7",
   "execution_count": 288,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:24.603124Z",
     "start_time": "2024-12-16T01:12:24.548791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 队伍名唯一类别编码\n",
    "asia_football_df_noNone['div'] = pd.Categorical(asia_football_df_noNone['Div']).codes\n",
    "asia_football_df_noNone['home_team'] = pd.Categorical(asia_football_df_noNone['HomeTeam']).codes\n",
    "asia_football_df_noNone['away_team'] = pd.Categorical(asia_football_df_noNone['AwayTeam']).codes\n",
    "asia_football_df_noNone"
   ],
   "id": "5b08f93862fe7a03",
   "execution_count": 289,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### one-hot 编码",
   "id": "407aa62e9621f79a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:25.392910Z",
     "start_time": "2024-12-16T01:12:25.182281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "div_encoder = OneHotEncoder()\n",
    "home_encoder = OneHotEncoder()\n",
    "away_encoder = OneHotEncoder()\n",
    "onehot_div = div_encoder.fit_transform(asia_football_df_noNone.Div.values.reshape(-1, 1)).toarray().astype(int)\n",
    "onehot_div_df = pd.DataFrame(onehot_div, columns=[\"Div \" + str(int(i)) for i in range(onehot_div.shape[1])])\n",
    "\n",
    "onehot_home = home_encoder.fit_transform(asia_football_df_noNone.HomeTeam.values.reshape(-1, 1)).toarray().astype(int)\n",
    "onehot_home_df = pd.DataFrame(onehot_home, columns=['HomeTeam ' + str(int(i)) for i in np.arange(onehot_home.shape[1])])\n",
    "\n",
    "onehot_away = away_encoder.fit_transform(asia_football_df_noNone.AwayTeam.values.reshape(-1, 1)).toarray().astype(int)\n",
    "onehot_away_df = pd.DataFrame(onehot_away, columns=['AwayTeam ' + str(int(i)) for i in np.arange(onehot_away.shape[1])])\n",
    "asia_football_df_noNone_noDiv = pd.concat([asia_football_df_noNone, onehot_div_df, onehot_home_df, onehot_away_df], axis=1)\n",
    "asia_football_df_noNone_noDiv.drop(columns=['Div'], inplace=True)\n",
    "\n",
    "asia_football_df_noNone_noDiv"
   ],
   "id": "c84c36870e0d01c6",
   "execution_count": 290,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 日期时间 one-hot",
   "id": "2acc36881e9823bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:25.507005Z",
     "start_time": "2024-12-16T01:12:25.473240Z"
    }
   },
   "cell_type": "code",
   "source": "import copy",
   "id": "bbfcc8a420eaf8a6",
   "execution_count": 291,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:26.009705Z",
     "start_time": "2024-12-16T01:12:25.864213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "asia_football_df_noNone_noDiv_noDate = copy.deepcopy(asia_football_df_noNone_noDiv)\n",
    "\n",
    "asia_football_df_noNone_noDiv_noDate['Year'] = pd.DatetimeIndex(asia_football_df_noNone_noDiv.Date).year\n",
    "asia_football_df_noNone_noDiv_noDate['Month'] = pd.DatetimeIndex(asia_football_df_noNone_noDiv.Date).month\n",
    "asia_football_df_noNone_noDiv_noDate"
   ],
   "id": "1e11d3876326973d",
   "execution_count": 292,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:26.313144Z",
     "start_time": "2024-12-16T01:12:26.220303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "asia_football_df_noNone_noDiv_noDate['Sin_Month'] = np.sin(2*np.pi*asia_football_df_noNone_noDiv_noDate.Month/12)\n",
    "asia_football_df_noNone_noDiv_noDate['Cos_Month'] = np.cos(2*np.pi*asia_football_df_noNone_noDiv_noDate.Month/12)\n",
    "\n",
    "asia_football_df_noNone_noDiv_noDate['DayofYear'] = pd.DatetimeIndex(asia_football_df_noNone_noDiv_noDate.Date).dayofyear\n",
    "asia_football_df_noNone_noDiv_noDate['Sin_Day'] = np.sin(2*np.pi*asia_football_df_noNone_noDiv_noDate.DayofYear/365)\n",
    "asia_football_df_noNone_noDiv_noDate['Cos_Day'] = np.cos(2*np.pi*asia_football_df_noNone_noDiv_noDate.DayofYear/365)\n",
    "\n",
    "asia_football_df_noNone_noDiv_noDate.drop(columns = ['Date','Month'], inplace = True)\n",
    "# learning_df.drop(columns = ['Date'], inplace = True)\n",
    "asia_football_df_noNone_noDiv_noDate"
   ],
   "id": "4397a4035a150ec1",
   "execution_count": 293,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "明确输入：hometeam，awayteam--onhot，AHh,B365AHH,B365AHA,HomeTeamELO,AwayTeamELO,",
   "id": "31b96a5b39958153"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:28.560944Z",
     "start_time": "2024-12-16T01:12:26.709897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 保存数据\n",
    "import gzip\n",
    "with gzip.open('pkl/asia_football_df_noNone_noDiv.pkl.gz', 'wb') as f:\n",
    "    asia_football_df_noNone_noDiv_noDate.to_pickle(f)"
   ],
   "id": "791e68ecf7874aa7",
   "execution_count": 294,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:28.845784Z",
     "start_time": "2024-12-16T01:12:28.564251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with gzip.open('pkl/asia_football_df_noNone_noDiv.pkl.gz', 'rb') as f:\n",
    "    df_loaded = pd.read_pickle(f)"
   ],
   "id": "ed70b27c05460938",
   "execution_count": 295,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CNN 架构模型",
   "id": "d31f8a8dd0e3d401"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:28.915903Z",
     "start_time": "2024-12-16T01:12:28.846645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(CNNModel, self).__init__()\n",
    "        # 特征序列 A\n",
    "        self.convA = nn.Conv1d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.poolA = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # 特征序列 B\n",
    "        self.convB = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.poolB = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # 特征序列 C\n",
    "        self.convC = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.poolC = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(in_features=128, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, time_encoding, individual_encoding, featureA, featureB, featureC):\n",
    "        # 特征序列 A\n",
    "        xA = self.poolA(F.relu(self.convA(featureA.unsqueeze(1))))\n",
    "        xA = xA.view(xA.size(0), -1)\n",
    "        \n",
    "        # 特征序列 B\n",
    "        xB = self.poolB(F.relu(self.convB(featureB.unsqueeze(1))))\n",
    "        xB = xB.view(xB.size(0), -1)\n",
    "        \n",
    "        # 特征序列 C\n",
    "        xC = self.poolC(F.relu(self.convC(featureC.unsqueeze(1))))\n",
    "        xC = xC.view(xC.size(0), -1)\n",
    "        \n",
    "        # 融合特征\n",
    "        x = torch.cat((xA, xB, xC), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "        "
   ],
   "id": "f810115cc05cbc25",
   "execution_count": 296,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 假设你的数据如下\n",
    "# df = pd.DataFrame({\n",
    "#     'feature1': [...],\n",
    "#     'feature2': [...],\n",
    "#     'feature3': [...],\n",
    "#     'label': [...]\n",
    "# })\n",
    "index_row_start = 0\n",
    "index_row_end = -1\n",
    "# 1. 准备数据\n",
    "# 提取特征和标签\n",
    "X = asia_football_df_noNone_noDiv_noDate.iloc[index_row_start:index_row_end][[\n",
    "    'AHh', 'B365AHH', 'B365AHA', 'HomeTeamELO', 'AwayTeamELO', 'Div 0', 'Div 1', 'Div 2', 'Div 3', 'Div 4', 'Year', 'Sin_Month', 'Cos_Month', 'DayofYear', 'Sin_Day', 'Cos_Day', 'DayofYear', 'Sin_Day', 'Cos_Day'\n",
    "]]  # 特征\n",
    "y = asia_football_df_noNone_noDiv_noDate.iloc[index_row_start:index_row_end]['easy_label']  # 标签\n",
    "\n",
    "# 2. 数据标准化（重要）\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "# tsne = TSNE(\n",
    "#     n_components=2,      # 降维到2维\n",
    "#     perplexity=30,      # 困惑度，通常在5-50之间\n",
    "#     learning_rate=200,   # 学习率\n",
    "#     n_iter=1000,        # 迭代次数\n",
    "#     random_state=42     # 随机种子\n",
    "# )\n",
    "# 3. 使用TSNE降维到2维\n",
    "tsne = TSNE(n_components=2, perplexity=20, random_state=13)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "# ['grey', 'lightcoral', 'chocolate', 'darkorange',\n",
    "# 'gold', 'olivedrab', 'lawngreen', 'aquamarine',\n",
    "# 'darkcyan', 'deepskyblue', 'cornflowerblue', 'blue',\n",
    "# 'mediumslateblue', 'blueviolet', 'violet', 'deeppink']\n",
    "# color_dict={'E0':'grey', 'F1':'lightcoral', 'D1':'chocolate', ''}\n",
    "# 4. 可视化\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis')\n",
    "plt.colorbar(scatter)\n",
    "plt.title('TSNE Visualization')\n",
    "plt.xlabel('TSNE Component 1')\n",
    "plt.ylabel('TSNE Component 2')\n",
    "plt.show()\n",
    "\n",
    "# # 5. 如果想要保存TSNE结果\n",
    "# df_tsne = pd.DataFrame(X_tsne, columns=['TSNE1', 'TSNE2'])\n",
    "# df_tsne['label'] = y"
   ],
   "id": "87cac5a1e59cb96f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GRU 模型架构",
   "id": "80f11c91574f27d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:28.943062Z",
     "start_time": "2024-12-16T01:12:28.916911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(GRUModel, self).__init__()\n",
    "        # 特征序列 A\n",
    "        self.gruA = nn.GRU(input_size=1, hidden_size=16, batch_first=True)\n",
    "        # 特征序列 B\n",
    "        self.gruB = nn.GRU(input_size=1, hidden_size=16, batch_first=True)\n",
    "        # 特征序列 C\n",
    "        self.gruC = nn.GRU(input_size=1, hidden_size=16, batch_first=True)\n",
    "        \n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(in_features=16 * 3, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, time_encoding, individual_encoding, featureA, featureB, featureC):\n",
    "        # 特征序列 A\n",
    "        xA, _ = self.gruA(featureA.unsqueeze(2))\n",
    "        xA = xA[:, -1, :]\n",
    "        \n",
    "        # 特征序列 B\n",
    "        xB, _ = self.gruB(featureB.unsqueeze(2))\n",
    "        xB = xB[:, -1, :]\n",
    "        \n",
    "        # 特征序列 C\n",
    "        xC, _ = self.gruC(featureC.unsqueeze(2))\n",
    "        xC = xC[:, -1, :]\n",
    "        \n",
    "        # 融合特征\n",
    "        x = torch.cat((xA, xB, xC), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "            "
   ],
   "id": "34fc7efb2473ffe3",
   "execution_count": 297,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 改进的CNN 架构",
   "id": "b24ff172301a24c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:28.976517Z",
     "start_time": "2024-12-16T01:12:28.945070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModelWithAttention(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(CNNModelWithAttention, self).__init__()\n",
    "        # 特征序列 A\n",
    "        self.convA = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.poolA = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # 特征序列 B\n",
    "        self.convB = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.poolB = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # 特征序列 C\n",
    "        self.convC = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.poolC = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # 注意力层\n",
    "        self.attention = nn.Linear(48, 48)\n",
    "        \n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(in_features=48, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, time_encoding, individual_encoding, featureA, featureB, featureC):\n",
    "        # 特征序列 A\n",
    "        xA = self.poolA(F.relu(self.convA(featureA.unsqueeze(1))))\n",
    "        xA = xA.view(xA.size(0), -1)\n",
    "        \n",
    "        # 特征序列 B\n",
    "        xB = self.poolB(F.relu(self.convB(featureB.unsqueeze(1))))\n",
    "        xB = xB.view(xB.size(0), -1)\n",
    "        \n",
    "        # 特征序列 C\n",
    "        xC = self.poolC(F.relu(self.convC(featureC.unsqueeze(1))))\n",
    "        xC = xC.view(xC.size(0), -1)\n",
    "        \n",
    "        # 融合特征\n",
    "        x = torch.cat((xA, xB, xC), dim=1)\n",
    "        \n",
    "        # 注意力机制\n",
    "        attention_weights = F.softmax(self.attention(x), dim = 1)\n",
    "        x = x * attention_weights\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ],
   "id": "9252243d783e36dd",
   "execution_count": 298,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 改进的GRU架构",
   "id": "644e5b0756675c1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:29.013470Z",
     "start_time": "2024-12-16T01:12:28.977525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GRUModelWithAttention(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(GRUModelWithAttention, self).__init__()\n",
    "        # 特征序列 A\n",
    "        self.gruA = nn.GRU(input_size=1, hidden_size=16, batch_first=True)\n",
    "        # 特征序列 B\n",
    "        self.gruB = nn.GRU(input_size=1, hidden_size=16, batch_first=True)\n",
    "        # 特征序列 C\n",
    "        self.gruC = nn.GRU(input_size=1, hidden_size=16, batch_first=True)\n",
    "        # 注意力层\n",
    "        self.attention = nn.Linear(48, 48)\n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(in_features=16 * 3, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, time_encoding, individual_encoding, featureA, featureB, featureC):\n",
    "        # 特征序列 A\n",
    "        xA, _ = self.gruA(featureA.unsqueeze(2))\n",
    "        xA = xA[:, -1, :]\n",
    "        \n",
    "        # 特征序列 B\n",
    "        xB, _ = self.gruB(featureB.unsqueeze(2))\n",
    "        xB = xB[:, -1, :]\n",
    "        \n",
    "        # 特征序列 C\n",
    "        xC, _ = self.gruC(featureC.unsqueeze(2))\n",
    "        xC = xC[:, -1, :]\n",
    "        \n",
    "        # 融合特征\n",
    "        x = torch.cat((xA, xB, xC), dim=1)\n",
    "        \n",
    "        # 注意力机制\n",
    "        attention_weights = F.softmax(self.attention(x), dim = 1)\n",
    "        x = x * attention_weights\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ],
   "id": "3a840a77c6fcde33",
   "execution_count": 299,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 多特征处理模型架构",
   "id": "ae308967fbb70f24"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:29.031104Z",
     "start_time": "2024-12-16T01:12:29.014086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiFeatureModel(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(MultiFeatureModel, self).__init__()\n",
    "        \n",
    "        # 数值特征处理\n",
    "        self.fc_numerical = nn.Sequential(\n",
    "            nn.Linear(in_features=16 * 3, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64)\n",
    "        )\n",
    "        \n",
    "        # 序列特征处理\n",
    "        self.gru = nn.GRU(input_size=3, hidden_size=16, batch_first=True)\n",
    "        \n",
    "        # 类别特征处理（使用嵌入层）\n",
    "        self.embedding = nn.Embedding(num_embeddings=3, embedding_dim=128)\n",
    "        \n",
    "        # 输出层\n",
    "        self.fc_output = nn.Linear(64+64+64, num_classes)\n",
    "        \n",
    "    def forward(self, numerical_features, sequence_features, categorical_features):\n",
    "        # 处理数值特征\n",
    "        numerical_out = self.fc_numeral(numerical_features)\n",
    "        \n",
    "        # 处理序列特征\n",
    "        seq_out, _ = self.gru(sequence_features)\n",
    "        seq_out = seq_out[:, -1, :] # 取最后的时刻输出\n",
    "        \n",
    "        # 处理类别特征\n",
    "        categorical_out = self.embedding(categorical_features).view(categorical_features.size(0), -1)\n",
    "        \n",
    "        # 融合输出\n",
    "        combined = torch.cat((numerical_out, seq_out, categorical_out), dim=1)\n",
    "        output = self.fc_output(combined)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "        "
   ],
   "id": "1f33b813a2f6b656",
   "execution_count": 300,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 更新后的多特征处理模型\n",
    "\n",
    "* 交互层InteractionLayer使用了一个线性层来捕捉特征之间的交互作用。输入的特征通过张量乘法得到交互特征，然后通过线性层进行处理\n",
    "* 特征维度调整 在设计输出层时，考虑了来自数值特征、序列特征、类别特征以及交互特征的输出维度\n",
    "* 灵活性 可以根据实际特征的数量和类型调整输入维度和结构\n",
    "* 这种设计能够有效地捕捉特征之间的复杂交互，有助于提升模型的表现\n"
   ],
   "id": "6ac950db257698f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:29.048916Z",
     "start_time": "2024-12-16T01:12:29.032112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class InteractionLayer(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(InteractionLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=input_size, out_features=input_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        interactions = torch.bmm(x.unsqueeze(1), x.unsqueeze(2)).view(x.size(0), -1)\n",
    "        return F.relu(self.linear(interactions))\n",
    "    \n",
    "class MultiFeatureModel(nn.Module):\n",
    "    def __init__(self, num_classes, numerical_inputsize, categorical_inputsize, sequence_inputsize, embedding_size):\n",
    "        super(MultiFeatureModel, self).__init__()\n",
    "        # 数值特征处理\n",
    "        self.fc_numerical = nn.Sequential(\n",
    "            nn.Linear(in_features=numerical_inputsize, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64)\n",
    "        )\n",
    "        \n",
    "        # 序列特征处理\n",
    "        self.gru = nn.GRU(input_size=sequence_inputsize, hidden_size=16, batch_first=True)\n",
    "        \n",
    "        # 类别特征处理（使用嵌入层）\n",
    "        self.embedding = nn.Embedding(num_embeddings=categorical_inputsize, embedding_dim=embedding_size)\n",
    "        \n",
    "        # 交互层\n",
    "        self.interaction_layer = InteractionLayer(64*3)\n",
    "        \n",
    "        # 输出层\n",
    "        self.fc_output = nn.Linear(64+64+64, num_classes)\n",
    "        \n",
    "    def forward(self, numerical_features, sequence_features, categorical_features):\n",
    "        # 处理数值特征\n",
    "        numerical_out = self.fc_numeral(numerical_features)\n",
    "        \n",
    "        # 处理序列特征\n",
    "        seq_out, _ = self.gru(sequence_features)\n",
    "        seq_out = seq_out[:, -1, :] # 取最后的时刻输出\n",
    "        \n",
    "        # 处理类别特征\n",
    "        categorical_out = self.embedding(categorical_features).view(categorical_features.size(0), -1)\n",
    "        \n",
    "        # 融合输出\n",
    "        combined = torch.cat((numerical_out, seq_out, categorical_out), dim=1)\n",
    "        \n",
    "        combined_out = self.interaction_layer(combined)\n",
    "        final_out = torch.cat((numerical_out, seq_out, categorical_out, combined_out), dim=1)\n",
    "        \n",
    "        output = self.fc_output(final_out)\n",
    "        \n",
    "        return output"
   ],
   "id": "28259f9710562ce1",
   "execution_count": 301,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "1e6545f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:29.091176Z",
     "start_time": "2024-12-16T01:12:29.049925Z"
    }
   },
   "source": [
    "# 删除指定列中含有缺失值的行\n",
    "#football_df.FTR.replace('nan', np.nan, inplace=True)\n",
    "nan_football_df = football_df.dropna(subset = nan_mask)\n",
    "nan_football_df"
   ],
   "execution_count": 302,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ad42e459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:29.112156Z",
     "start_time": "2024-12-16T01:12:29.092179Z"
    }
   },
   "source": [
    "# resize shape\n",
    "football_df.shape[0] - nan_football_df.shape[0]"
   ],
   "execution_count": 303,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "42864535",
   "metadata": {},
   "source": [
    "### 2.2 Feature Encoding <br>\n",
    "* $\\phi(Date)$ $\\Rightarrow$ one column for *year*, second column for *month*, third column for *day of year*\n",
    "* One hot encode Division, Home and Away Teams\n",
    "* Label encode Full Time Result (Win/Draw/Loss)"
   ]
  },
  {
   "cell_type": "code",
   "id": "7b5bb61e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:29.134168Z",
     "start_time": "2024-12-16T01:12:29.113164Z"
    }
   },
   "source": [
    "feats = nan_mask"
   ],
   "execution_count": 304,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:29.188434Z",
     "start_time": "2024-12-16T01:12:29.138174Z"
    }
   },
   "cell_type": "code",
   "source": "nan_football_df",
   "id": "9919d74193a28304",
   "execution_count": 305,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:29.455390Z",
     "start_time": "2024-12-16T01:12:29.419952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_df = nan_football_df.copy()[feats]\n",
    "learning_df"
   ],
   "id": "5213f7b7293d7cc0",
   "execution_count": 306,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "fe16f1b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:29.809816Z",
     "start_time": "2024-12-16T01:12:29.792764Z"
    }
   },
   "source": [
    "learning_df.reset_index(inplace=True, drop=True)\n",
    "learning_df"
   ],
   "execution_count": 307,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "15884417",
   "metadata": {},
   "source": [
    "**2.2.1 Division and Home/Away Team Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "id": "913c0088",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:30.169995Z",
     "start_time": "2024-12-16T01:12:30.150419Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "div_encoder = OneHotEncoder()\n",
    "home_encoder = OneHotEncoder()\n",
    "away_encoder = OneHotEncoder()"
   ],
   "execution_count": 308,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "bc739d2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:30.591592Z",
     "start_time": "2024-12-16T01:12:30.528277Z"
    }
   },
   "source": [
    "onehot_div = div_encoder.fit_transform(learning_df.Div.values.reshape(-1,1)).toarray().astype(int)\n",
    "onehot_div_df = pd.DataFrame(onehot_div, columns = [\"Div \"+str(int(i)) for i in range(onehot_div.shape[1])])\n",
    "\n",
    "onehot_home = home_encoder.fit_transform(learning_df.HomeTeam.values.reshape(-1,1)).toarray().astype(int)\n",
    "onehot_home_df = pd.DataFrame(onehot_home, columns = ['HomeTeam ' + str(int(i)) for i in np.arange(onehot_home.shape[1])])\n",
    "\n",
    "onehot_away = away_encoder.fit_transform(learning_df.AwayTeam.values.reshape(-1,1)).toarray().astype(int)\n",
    "onehot_away_df = pd.DataFrame(onehot_away, columns = ['AwayTeam ' + str(int(i)) for i in np.arange(onehot_away.shape[1])])"
   ],
   "execution_count": 309,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f8444147",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:30.783170Z",
     "start_time": "2024-12-16T01:12:30.711055Z"
    }
   },
   "source": [
    "learning_df = pd.concat([learning_df, onehot_div_df, onehot_home_df, onehot_away_df], axis = 1)\n",
    "learning_df.drop(columns = ['Div'], inplace = True)"
   ],
   "execution_count": 310,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "321f2f32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:31.112717Z",
     "start_time": "2024-12-16T01:12:31.084089Z"
    }
   },
   "source": [
    "learning_df"
   ],
   "execution_count": 311,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1b0fefa3",
   "metadata": {},
   "source": [
    "**2.2.2 Full Time Result Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "id": "f9442a80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:31.260282Z",
     "start_time": "2024-12-16T01:12:31.241499Z"
    }
   },
   "source": [
    "target_encoder = LabelEncoder()\n",
    "learning_df['Result'] = target_encoder.fit_transform(learning_df.FTR) "
   ],
   "execution_count": 312,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "18c1cb41",
   "metadata": {},
   "source": [
    "**2.2.3 Date Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "id": "f5dfc529",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:31.411741Z",
     "start_time": "2024-12-16T01:12:31.334916Z"
    }
   },
   "source": [
    "learning_df['Year'] = pd.DatetimeIndex(learning_df.Date).year\n",
    "\n",
    "learning_df['Month'] = pd.DatetimeIndex(learning_df.Date).month\n",
    "learning_df['Sin_Month'] = np.sin(2*np.pi*learning_df.Month/12)\n",
    "learning_df['Cos_Month'] = np.cos(2*np.pi*learning_df.Month/12)\n",
    "\n",
    "learning_df['DayofYear'] = pd.DatetimeIndex(learning_df.Date).dayofyear\n",
    "learning_df['Sin_Day'] = np.sin(2*np.pi*learning_df.DayofYear/365)\n",
    "learning_df['Cos_Day'] = np.cos(2*np.pi*learning_df.DayofYear/365)\n",
    "\n",
    "learning_df.drop(columns = ['Date','Month'], inplace = True)\n",
    "# learning_df.drop(columns = ['Date'], inplace = True)"
   ],
   "execution_count": 313,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7e7bc241",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:31.594663Z",
     "start_time": "2024-12-16T01:12:31.567229Z"
    }
   },
   "source": [
    "learning_df"
   ],
   "execution_count": 314,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "# For Test\n",
   "id": "866fbac3337d0235",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b14da74e",
   "metadata": {},
   "source": [
    "### 2.3 Feature Engineering <br>\n",
    "* $\\phi(x)$ feature transformation $\\Rightarrow$ last match result, win/loss streak to date, wins to season date\n",
    "* $\\phi(x)$ feature engineering $\\Rightarrow$ average the home, away, and draw odds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843342cb",
   "metadata": {},
   "source": [
    "**2.3.1 Last Match Result** <br>\n",
    "Indicate the result from the last match played between both teams"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9871315",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:31.703575Z",
     "start_time": "2024-12-16T01:12:31.692980Z"
    }
   },
   "source": [
    "# 定义一个函数来计算两队之间上一场比赛的结果\n",
    "def compute_last_matches(df):\n",
    "    \n",
    "    unique_matchups = list(set((list(zip(df.HomeTeam, df.AwayTeam)))))\n",
    "    df['Last Match Result'] = np.nan\n",
    "    for home, away in unique_matchups:\n",
    "        matchup_df = df[(df.HomeTeam == home) & (df.AwayTeam == away)]\n",
    "        # 使用 shift(1) 方法将 FTR（全场比赛结果）列中的数据向下移动一行，这样每行的 last_match_result 将对应于这两队之前的一场比赛的结果。fill_value='Na' 确保了数据移动后空出的位置填充为 'Na'。\n",
    "        last_match_result = matchup_df.FTR.shift(1, fill_value='Na')\n",
    "        df.loc[matchup_df.index, 'Last Match Result'] = last_match_result\n",
    "        \n",
    "    lmr_encoder = LabelEncoder()\n",
    "    df['Last Match Result'] = lmr_encoder.fit_transform(df['Last Match Result'])\n",
    "    df.drop(columns = ['FTR'], inplace = True)\n",
    "    return df"
   ],
   "execution_count": 315,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:32.064694Z",
     "start_time": "2024-12-16T01:12:32.047991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_last_n_matches(df, n=5):\n",
    "    unique_matchups = list(set(zip(df.HomeTeam, df.AwayTeam)))\n",
    "    df['Last 5 Match Results'] = np.nan  # 新增一列用于存储过去 5 场比赛的结果\n",
    "    \n",
    "    for home, away in unique_matchups:\n",
    "        matchup_df = df[(df.HomeTeam == home) & (df.AwayTeam == away)]\n",
    "        \n",
    "        # 获取过去 n 场比赛的结果\n",
    "        last_n_results = [matchup_df.FTR.shift(i, fill_value='Na') for i in range(1, n+1)]\n",
    "        \n",
    "        # 将计算得到的过去 n 场比赛的结果合并为一个字符串或列表，取决于需求\n",
    "        # 这里使用字符串形式：'result1/result2/...'\n",
    "        matchup_df['Last 5 Match Results'] = pd.DataFrame(last_n_results).T.apply(lambda x: '/'.join(x), axis=1)\n",
    "        \n",
    "        # 将计算得到的结果更新回原始 df 中\n",
    "        df.loc[matchup_df.index, 'Last 5 Match Results'] = matchup_df['Last 5 Match Results']\n",
    "    \n",
    "    # 对 Last 5 Match Results 列进行标签编码\n",
    "    lmr_encoder = LabelEncoder()\n",
    "    df['Last 5 Match Results'] = lmr_encoder.fit_transform(df['Last 5 Match Results'])\n",
    "    \n",
    "    # 删除原始的 FTR 列\n",
    "    df.drop(columns=['FTR'], inplace=True)\n",
    "    \n",
    "    return df\n"
   ],
   "id": "bee90a274731b07e",
   "execution_count": 316,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "751bdc97",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:52.360544Z",
     "start_time": "2024-12-16T01:12:32.173839Z"
    }
   },
   "source": [
    "learning_df = compute_last_matches(learning_df)\n",
    "# learning_df.drop(columns = ['FTR'], inplace = True)"
   ],
   "execution_count": 317,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8ff554cf",
   "metadata": {},
   "source": [
    "**2.3.2 Home and Away Win/Loss Streak** <br>\n",
    "Important note about this feature: the win/loss streak is the teams *home* and *away* win streak, *not* its ***consecutive*** win/loss streak."
   ]
  },
  {
   "cell_type": "code",
   "id": "41f1fcc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:52.369836Z",
     "start_time": "2024-12-16T01:12:52.361642Z"
    }
   },
   "source": [
    "# https://stackoverflow.com/questions/52976336/compute-winning-streak-with-pandas\n",
    "# https://joshdevlin.com/blog/calculate-streaks-in-pandas/"
   ],
   "execution_count": 318,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4658caa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:12:52.390724Z",
     "start_time": "2024-12-16T01:12:52.370364Z"
    }
   },
   "source": [
    "def compute_winstreak(df):\n",
    "    \n",
    "    years = df.Year.unique()\n",
    "    df_lst = []    \n",
    "    for year in years:\n",
    "        \n",
    "        year_df = df[df.Year == year]\n",
    "        year_df['HomeWin'] = year_df.Result.replace([0, 1, 2], [0, 0, 1])\n",
    "        year_df['AwayWin'] = year_df.Result.replace([0, 1, 2], [1, 0, 0])\n",
    "        year_df['HomeWinStreak'] = None\n",
    "        year_df['AwayWinStreak'] = None\n",
    "        \n",
    "        hometeams = year_df.HomeTeam.unique()\n",
    "        awayteams = year_df.AwayTeam.unique()\n",
    "        \n",
    "        for team in hometeams:\n",
    "            team_df = year_df[(year_df.HomeTeam == team)]\n",
    "            team_df = team_df.sort_values(['Year', 'DayofYear'], ascending = (True, True))\n",
    "\n",
    "            team_grouper = (team_df.HomeWin != team_df.HomeWin.shift()).cumsum()\n",
    "            team_df['HomeWinStreak'] = team_df[['HomeWin']].groupby(team_grouper).cumsum()\n",
    "            team_df.loc[team_df.HomeWinStreak >0, 'HomeWinStreak'] -= 1\n",
    "            year_df.loc[team_df.index, 'HomeWinStreak'] = team_df.HomeWinStreak\n",
    "            \n",
    "        for team in awayteams:\n",
    "            team_df = year_df[(year_df.AwayTeam == team)]\n",
    "            team_df = team_df.sort_values(['Year', 'DayofYear'], ascending = (True, True))\n",
    "\n",
    "            team_grouper = (team_df.AwayWin != team_df.AwayWin.shift()).cumsum()\n",
    "            team_df['AwayWinStreak'] = team_df[['AwayWin']].groupby(team_grouper).cumsum()\n",
    "            team_df.loc[team_df.AwayWinStreak >0, 'AwayWinStreak'] -= 1\n",
    "            year_df.loc[team_df.index, 'AwayWinStreak'] = team_df.AwayWinStreak\n",
    "            \n",
    "        df_lst.append(year_df)\n",
    "        \n",
    "    return pd.concat(df_lst, axis = 0).drop(columns = ['HomeWin', 'AwayWin'])#,'DayofYear'])"
   ],
   "execution_count": 319,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "aca53647",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:02.819046Z",
     "start_time": "2024-12-16T01:12:52.391733Z"
    }
   },
   "source": [
    "learning_df = compute_winstreak(learning_df)"
   ],
   "execution_count": 320,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "94e1c235",
   "metadata": {},
   "source": [
    "**2.3.4 Season Home/Away Wins to Date** <br>\n",
    "Indicate the number of wins for a team as home and away to date within current season"
   ]
  },
  {
   "cell_type": "code",
   "id": "cb9c5f13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:02.838875Z",
     "start_time": "2024-12-16T01:13:02.820051Z"
    }
   },
   "source": [
    "toy = learning_df[(learning_df.Year == 2010) & (learning_df.HomeTeam == 'Barcelona')][['HomeTeam', 'AwayTeam', 'Result']]\n",
    "toy['HomeWin'] = toy.Result.replace([0, 1, 2], [0, 0, 1])\n",
    "toy['HomeWinsToDate'] = toy.HomeWin.cumsum()"
   ],
   "execution_count": 321,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "885dc94a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:02.850408Z",
     "start_time": "2024-12-16T01:13:02.839883Z"
    }
   },
   "source": [
    "def compute_winstodate(df):\n",
    "    \n",
    "    years = df.Year.unique()\n",
    "    df_lst = []    \n",
    "    for year in years:\n",
    "        \n",
    "        year_df = df[df.Year == year]\n",
    "        year_df['HomeWin'] = year_df.Result.replace([0, 1, 2], [0, 0, 1])\n",
    "        year_df['AwayWin'] = year_df.Result.replace([0, 1, 2], [1, 0, 0])\n",
    "        year_df['HomeWinsToDate'] = None\n",
    "        year_df['AwayWinsToDate'] = None\n",
    "        \n",
    "        hometeams = year_df.HomeTeam.unique()\n",
    "        awayteams = year_df.AwayTeam.unique()\n",
    "        \n",
    "        for team in hometeams:\n",
    "            team_df = year_df[(year_df.HomeTeam == team)]\n",
    "            team_df = team_df.sort_values(['Year', 'DayofYear'], ascending = (True, True))\n",
    "\n",
    "            team_df['HomeWinsToDate'] = team_df.HomeWin.cumsum()\n",
    "            year_df.loc[team_df.index, 'HomeWinsToDate'] = team_df.HomeWinsToDate\n",
    "            \n",
    "        for team in awayteams:\n",
    "            team_df = year_df[(year_df.AwayTeam == team)]\n",
    "            team_df = team_df.sort_values(['Year', 'DayofYear'], ascending = (True, True))\n",
    "            \n",
    "            team_df['AwayWinsToDate'] = team_df.AwayWin.cumsum()\n",
    "            year_df.loc[team_df.index, 'AwayWinsToDate'] = team_df.AwayWinsToDate\n",
    "            \n",
    "        df_lst.append(year_df)\n",
    "        \n",
    "    return pd.concat(df_lst, axis = 0).drop(columns = ['HomeWin', 'AwayWin','DayofYear'])"
   ],
   "execution_count": 322,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "b60fbbf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:08.257288Z",
     "start_time": "2024-12-16T01:13:02.851423Z"
    }
   },
   "source": [
    "learning_df = compute_winstodate(learning_df)\n",
    "learning_df.drop(columns = ['HomeTeam', 'AwayTeam'], inplace = True)"
   ],
   "execution_count": 323,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:08.285740Z",
     "start_time": "2024-12-16T01:13:08.258302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# learning_df\n",
    "learning_df"
   ],
   "id": "b23f5c1f7a05744b",
   "execution_count": 324,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.479516Z",
     "start_time": "2024-12-16T01:13:08.286752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 保存为pkl文件\n",
    "learning_df.to_pickle('E:/Data/PKL/learning_df.pkl')"
   ],
   "id": "749b75afd7a2bc77",
   "execution_count": 325,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "36c2496f",
   "metadata": {},
   "source": [
    "**2.3.5 Website Odds** <br>\n",
    "The `betting odds` recorded by various betting websites offer insight into sentiment surrounding the outcome of a particular game. "
   ]
  },
  {
   "cell_type": "code",
   "id": "1efc8290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.493672Z",
     "start_time": "2024-12-16T01:13:09.480031Z"
    }
   },
   "source": [
    "# betting_feats = ['B365H', 'B365D', 'B365A', 'IWH', 'IWD', 'IWA', 'WHH', 'WHD', 'WHA', \"AHh\", \"B365AHH\", \"B365AHA\"]\n",
    "betting_feats = ['B365H', 'B365D', 'B365A']\n",
    "betting_feats"
   ],
   "execution_count": 326,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "031548ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.506573Z",
     "start_time": "2024-12-16T01:13:09.495677Z"
    }
   },
   "source": [
    "def compute_meanodds(df, betting_feats):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    home_odds = []\n",
    "    away_odds = []\n",
    "    draw_odds = []\n",
    "    for odd in betting_feats:\n",
    "        odd_type = odd[-1]\n",
    "        if odd_type == 'H':\n",
    "            home_odds.append(odd)\n",
    "        elif odd_type == 'A':\n",
    "            away_odds.append(odd)\n",
    "        else:\n",
    "            draw_odds.append(odd)\n",
    "    avg_home_odds = df[home_odds].mean(axis=1)\n",
    "    avg_away_odds = df[away_odds].mean(axis=1)\n",
    "    avg_draw_odds = df[draw_odds].mean(axis=1)\n",
    "    \n",
    "    ordered_cols = ['HomeOdds', 'AwayOdds', 'DrawOdds'] + df.columns.tolist()\n",
    "    \n",
    "    df['HomeOdds'] = avg_home_odds\n",
    "    df['AwayOdds'] = avg_away_odds\n",
    "    df['DrawOdds'] = avg_draw_odds\n",
    "    \n",
    "    return df[ordered_cols]"
   ],
   "execution_count": 327,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "08e0a28b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.540696Z",
     "start_time": "2024-12-16T01:13:09.507581Z"
    }
   },
   "source": [
    "learning_df = compute_meanodds(learning_df, betting_feats)"
   ],
   "execution_count": 328,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0e4768c8",
   "metadata": {},
   "source": [
    "### 2.4 Peek @ Learning DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "id": "b54348f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.581743Z",
     "start_time": "2024-12-16T01:13:09.541700Z"
    }
   },
   "source": [
    "learning_df"
   ],
   "execution_count": 329,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.612704Z",
     "start_time": "2024-12-16T01:13:09.585259Z"
    }
   },
   "cell_type": "code",
   "source": "learning_df.drop(columns = ['B365H', 'B365D', 'B365A', 'IWH', 'IWD', 'IWA', 'WHH', 'WHD', 'WHA', 'HomeOdds', 'AwayOdds', 'DrawOdds'], inplace = True)",
   "id": "228a748b486ff190",
   "execution_count": 330,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7b46d936",
   "metadata": {},
   "source": [
    "# 3. Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e10788",
   "metadata": {},
   "source": [
    "* Establish a baseline Logistic Regression model fit over the entire learning dataframe without special regard to *division* and *team*. \n",
    "* Train model over 16 seasons, and predict for the remaining 3 seasons (approximate 80-20 split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ddd44e",
   "metadata": {},
   "source": [
    "### 3.1 Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "id": "a82f4a6b",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.621788Z",
     "start_time": "2024-12-16T01:13:09.613506Z"
    }
   },
   "source": [
    "split = 0.80\n",
    "no_seasons = 20\n",
    "\n",
    "print('No. seasons to train over: ' + str(round(split*no_seasons)))"
   ],
   "execution_count": 331,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f22ecde1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.653923Z",
     "start_time": "2024-12-16T01:13:09.622793Z"
    }
   },
   "source": [
    "X, y = learning_df.loc[:, learning_df.columns != 'Result'], learning_df[['Result']]"
   ],
   "execution_count": 332,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "60373664",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.663045Z",
     "start_time": "2024-12-16T01:13:09.654933Z"
    }
   },
   "source": [
    "# full_feat = ['HomeWinStreak','AwayWinStreak','HomeWinsToDate', 'AwayWinsToDate', 'Last Match Result',\n",
    "#              'HomeTeamELO', 'AwayTeamELO', 'HomeOdds', 'AwayOdds', 'DrawOdds'] + betting_feats\n",
    "\n",
    "# exclude_feats = ['HomeWinsToDate', 'AwayWinsToDate', 'Last Match Result'] "
   ],
   "execution_count": 333,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "1db1f4fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.673004Z",
     "start_time": "2024-12-16T01:13:09.663995Z"
    }
   },
   "source": [
    "# X = X[X.columns[~X.columns.isin(exclude_feats)]]\n",
    "# X"
   ],
   "execution_count": 334,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4c621fff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.705541Z",
     "start_time": "2024-12-16T01:13:09.673522Z"
    }
   },
   "source": [
    "X"
   ],
   "execution_count": 335,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.717890Z",
     "start_time": "2024-12-16T01:13:09.706545Z"
    }
   },
   "cell_type": "code",
   "source": "y",
   "id": "232473f400896cfa",
   "execution_count": 336,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.729496Z",
     "start_time": "2024-12-16T01:13:09.718895Z"
    }
   },
   "cell_type": "code",
   "source": "split_year = 2022",
   "id": "44dc55d09c257a7d",
   "execution_count": 337,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0e361977",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.773879Z",
     "start_time": "2024-12-16T01:13:09.730501Z"
    }
   },
   "source": [
    "# 切分训练集和测试集\n",
    "xTr, xTe = X[X.Year <= split_year], X[X.Year > split_year]\n",
    "yTr, yTe = y.loc[xTr.index, :], y.loc[xTe.index, :]"
   ],
   "execution_count": 338,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2 Normalization <br>\n",
    "Following our various feature transformations and development, we arrived to a sparse dataframe with the exception of a few features(*Year, DayofYear*). It will be important to *normalize* these features as they are in gross magnitudes compared to the remaining features. During model training, having dominating features (in scale relative to others) can be dangerous as the weight updates may mistakengly favor these larger-scale features because it will have the largest influence on the target output. "
   ],
   "id": "50636cdc8bb898d6"
  },
  {
   "cell_type": "code",
   "id": "a72d269a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.787110Z",
     "start_time": "2024-12-16T01:13:09.774893Z"
    }
   },
   "source": [
    "# minmax_scaler.fit_transform()：这个方法首先拟合数据，即计算数据的最小值和最大值，这些值用于后续的缩放。然后，它将这些参数用于转换数据，将原始数据缩放到0和1之间。\n",
    "# minmax_scaler.transform()：这个方法使用在训练数据上计算得到的最小值和最大值来转换测试数据。这确保了训练数据和测试数据使用相同的缩放标准。\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "xTr.loc[:, ['Year']] = minmax_scaler.fit_transform(xTr.loc[:, ['Year']])\n",
    "xTe.loc[:, ['Year']] = minmax_scaler.transform(xTe.loc[:, ['Year']])"
   ],
   "execution_count": 339,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "11ee9c78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.803140Z",
     "start_time": "2024-12-16T01:13:09.788115Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "# to_scale = ['HomeWinStreak','AwayWinStreak','HomeWinsToDate', 'AwayWinsToDate', 'HomeTeamELO', 'AwayTeamELO', 'HomeOdds', 'AwayOdds', 'DrawOdds'] + betting_feats\n",
    "to_scale = ['HomeTeamELO', 'AwayTeamELO'] + betting_feats\n",
    "\n",
    "xTr.loc[:, to_scale] = std_scaler.fit_transform(xTr.loc[:, to_scale])\n",
    "xTe.loc[:, to_scale] = std_scaler.transform(xTe.loc[:, to_scale])"
   ],
   "execution_count": 340,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "76792bc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.843214Z",
     "start_time": "2024-12-16T01:13:09.804146Z"
    }
   },
   "source": [
    "xTr"
   ],
   "execution_count": 341,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4696b592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.867369Z",
     "start_time": "2024-12-16T01:13:09.844219Z"
    }
   },
   "source": [
    "xTe"
   ],
   "execution_count": 342,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8b0abe45",
   "metadata": {},
   "source": [
    "### 3.3 HomeWins Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "c8002824",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.878496Z",
     "start_time": "2024-12-16T01:13:09.868376Z"
    }
   },
   "source": [
    "from sklearn.metrics import accuracy_score"
   ],
   "execution_count": 343,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.895027Z",
     "start_time": "2024-12-16T01:13:09.879505Z"
    }
   },
   "cell_type": "code",
   "source": "xTr.shape",
   "id": "bdd8d4925bb687b9",
   "execution_count": 344,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.908173Z",
     "start_time": "2024-12-16T01:13:09.897033Z"
    }
   },
   "cell_type": "code",
   "source": "xTe.shape",
   "id": "53f767c6fd6ea837",
   "execution_count": 345,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f13115e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.924031Z",
     "start_time": "2024-12-16T01:13:09.909179Z"
    }
   },
   "source": [
    "# training score\n",
    "baseline_Tr = np.full((xTr.shape[0], 1), 2) \n",
    "accuracy_score(yTr.Result.values, baseline_Tr.ravel())"
   ],
   "execution_count": 346,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "16d2cf5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:13:09.938492Z",
     "start_time": "2024-12-16T01:13:09.925042Z"
    }
   },
   "source": [
    "# testing score\n",
    "baseline_preds_Te = np.full((xTe.shape[0]  , 1), 2) #predicts home wins all the time\n",
    "accuracy_score(yTe.Result.values, baseline_preds_Te.ravel())"
   ],
   "execution_count": 347,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dff87ca8",
   "metadata": {},
   "source": [
    "### 3.4 Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e03fdd6",
   "metadata": {},
   "source": [
    "**3.4.1** $l2$ Regularized"
   ]
  },
  {
   "cell_type": "code",
   "id": "6c5f52f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:14:30.337180Z",
     "start_time": "2024-12-16T01:13:09.939504Z"
    }
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "l2_lr = LogisticRegression(max_iter = 10000, n_jobs=-1).fit(xTr, yTr.values.ravel())"
   ],
   "execution_count": 348,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "54140ce0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:14:30.803545Z",
     "start_time": "2024-12-16T01:14:30.338550Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, l2_lr.predict(xTr))"
   ],
   "execution_count": 349,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8c59602e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:14:30.842778Z",
     "start_time": "2024-12-16T01:14:30.804550Z"
    }
   },
   "source": [
    "# testing score\n",
    "lr_preds = l2_lr.predict(xTe)\n",
    "accuracy_score(yTe.Result.values, lr_preds)"
   ],
   "execution_count": 350,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8adcb166",
   "metadata": {},
   "source": [
    "**3.4.1** $l2$ Penalty Tuning"
   ]
  },
  {
   "cell_type": "code",
   "id": "3009cb8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:16:25.201012Z",
     "start_time": "2024-12-16T01:14:30.843785Z"
    }
   },
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "logistic_params = {'C':[0.001,0.01,0.10]}\n",
    "\n",
    "# logistic_randsearch = RandomizedSearchCV(estimator=LogisticRegression(max_iter=10000),\n",
    "#                                          param_distributions=logistic_params,\n",
    "logistic_randsearch = GridSearchCV(estimator=LogisticRegression(max_iter=10000),\n",
    "                                         param_grid=logistic_params,\n",
    "                                         scoring='accuracy',\n",
    "                                         verbose=1,\n",
    "                                         cv=5,\n",
    "                                         n_jobs=-1)\n",
    "\n",
    "logistic_rand_results = logistic_randsearch.fit(xTr, yTr.values.ravel())\n",
    "print(\"Best: %f using %s\" % (logistic_rand_results.best_score_, logistic_rand_results.best_params_))"
   ],
   "execution_count": 351,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a9231529",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:16:25.211333Z",
     "start_time": "2024-12-16T01:16:25.201012Z"
    }
   },
   "source": [
    "l2_rs = logistic_rand_results.best_estimator_"
   ],
   "execution_count": 352,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9197a0f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:16:25.635829Z",
     "start_time": "2024-12-16T01:16:25.211868Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, l2_rs.predict(xTr))"
   ],
   "execution_count": 353,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5c45f431",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:16:25.673182Z",
     "start_time": "2024-12-16T01:16:25.636840Z"
    }
   },
   "source": [
    "# testing score\n",
    "accuracy_score(yTe.Result.values, l2_rs.predict(xTe))"
   ],
   "execution_count": 354,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "febcc06f",
   "metadata": {},
   "source": [
    "**3.4.4** $l1$ Regularized"
   ]
  },
  {
   "cell_type": "code",
   "id": "d9502351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:27:20.952844Z",
     "start_time": "2024-12-16T01:16:25.674185Z"
    }
   },
   "source": [
    "l1_lr = LogisticRegression(penalty='l1', solver='saga', max_iter = 10000, n_jobs=-1).fit(xTr, yTr.values.ravel())"
   ],
   "execution_count": 355,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "82d75648",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:27:21.422894Z",
     "start_time": "2024-12-16T01:27:20.954323Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, l1_lr.predict(xTr))"
   ],
   "execution_count": 356,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4faea9e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:27:21.472811Z",
     "start_time": "2024-12-16T01:27:21.425887Z"
    }
   },
   "source": [
    "# testing score\n",
    "l1_preds = l1_lr.predict(xTe)\n",
    "accuracy_score(yTe.Result.values, l1_preds)"
   ],
   "execution_count": 357,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "445dcdb4",
   "metadata": {},
   "source": [
    "**3.4.5** Penalty Tuning"
   ]
  },
  {
   "cell_type": "code",
   "id": "3314235e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:33:00.614753Z",
     "start_time": "2024-12-16T01:27:21.473751Z"
    }
   },
   "source": [
    "l1_params = {'C':[0.001,0.01,0.10]}\n",
    "\n",
    "# l1_randsearch = RandomizedSearchCV(estimator=LogisticRegression(penalty='l1',solver='saga', max_iter=10000),\n",
    "#                                          param_distributions=l1_params,\n",
    "l1_randsearch = GridSearchCV(estimator=LogisticRegression(penalty='l1',solver='saga', max_iter=10000),\n",
    "                                         param_grid=l1_params,\n",
    "                                         scoring='accuracy',\n",
    "                                         verbose=1,\n",
    "                                         n_jobs=-1,\n",
    "                                         cv=5)\n",
    "\n",
    "l1_rand_results = l1_randsearch.fit(xTr, yTr.values.ravel())\n",
    "print(\"Best: %f using %s\" % (l1_rand_results.best_score_, l1_rand_results.best_params_))"
   ],
   "execution_count": 358,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f4607e83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:33:00.626769Z",
     "start_time": "2024-12-16T01:33:00.615757Z"
    }
   },
   "source": [
    "l1_rs = l1_randsearch.best_estimator_ #LogisticRegression(C=0.10, solver='saga', max_iter=10000).fit(xTr, yTr.values.ravel())#"
   ],
   "execution_count": 359,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4895d05a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:33:01.152297Z",
     "start_time": "2024-12-16T01:33:00.627773Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, l1_rs.predict(xTr))"
   ],
   "execution_count": 360,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7016a84c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:33:01.194506Z",
     "start_time": "2024-12-16T01:33:01.153308Z"
    }
   },
   "source": [
    "# testing score\n",
    "accuracy_score(yTe.Result.values, l1_rs.predict(xTe))"
   ],
   "execution_count": 361,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8b988c26",
   "metadata": {},
   "source": [
    "### 3.5 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "id": "0f4908d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:36:55.316114Z",
     "start_time": "2024-12-16T01:33:01.195511Z"
    }
   },
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(max_iter=100000).fit(xTr, yTr.values.ravel())"
   ],
   "execution_count": 362,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "1a18a45a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:40:35.765479Z",
     "start_time": "2024-12-16T01:36:55.317124Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, svm.predict(xTr))"
   ],
   "execution_count": 363,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7311630d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T01:40:48.777719Z",
     "start_time": "2024-12-16T01:40:35.766482Z"
    }
   },
   "source": [
    "# testing score\n",
    "accuracy_score(yTe.Result.values, svm.predict(xTe))"
   ],
   "execution_count": 364,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b2f00bed",
   "metadata": {},
   "source": [
    "**3.5.2** Penalty Tuning"
   ]
  },
  {
   "cell_type": "code",
   "id": "5b998f7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:06:41.703551Z",
     "start_time": "2024-12-16T01:40:48.779996Z"
    }
   },
   "source": [
    "svm_params = {'C':[0.001,0.01,0.10]}\n",
    "\n",
    "# svm_randsearch = RandomizedSearchCV(estimator=SVC(max_iter=100000),\n",
    "#                                          param_distributions=svm_params,\n",
    "svm_randsearch = GridSearchCV(estimator=SVC(max_iter=100000),\n",
    "                                         param_grid=svm_params,\n",
    "                                         scoring='accuracy',\n",
    "                                         verbose=2,\n",
    "                                         cv=5,\n",
    "                                         n_jobs=-1)\n",
    "\n",
    "svm_rand_results = svm_randsearch.fit(xTr, yTr.values.ravel())\n",
    "print(\"Best: %f using %s\" % (svm_rand_results.best_score_, svm_rand_results.best_params_))"
   ],
   "execution_count": 365,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7f9627ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:06:41.723740Z",
     "start_time": "2024-12-16T02:06:41.704558Z"
    }
   },
   "source": [
    "svm_rs = svm_rand_results.best_estimator_"
   ],
   "execution_count": 366,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "e32e6caf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:10:12.025171Z",
     "start_time": "2024-12-16T02:06:41.724749Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, svm_rs.predict(xTr))"
   ],
   "execution_count": 367,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "e88837b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:10:24.178192Z",
     "start_time": "2024-12-16T02:10:12.026175Z"
    }
   },
   "source": [
    "# testing score\n",
    "accuracy_score(yTe.Result.values, svm_rs.predict(xTe))"
   ],
   "execution_count": 368,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "edee8858",
   "metadata": {},
   "source": [
    "### 3.6 Simple Neural Network ####"
   ]
  },
  {
   "cell_type": "code",
   "id": "d0505f91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:20:39.017437Z",
     "start_time": "2024-12-16T02:10:24.179198Z"
    }
   },
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(512,128,32),\n",
    "                    activation='relu',\n",
    "                    batch_size=64,\n",
    "                    max_iter=200,\n",
    "                    learning_rate_init=1e-4,\n",
    "                    early_stopping=False,\n",
    "                    alpha=1e-3,\n",
    "                   ).fit(xTr, yTr.values.ravel())"
   ],
   "execution_count": 369,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5ca78c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:20:39.711542Z",
     "start_time": "2024-12-16T02:20:39.018454Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, mlp.predict(xTr))"
   ],
   "execution_count": 370,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "aa884592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:20:39.772353Z",
     "start_time": "2024-12-16T02:20:39.711542Z"
    }
   },
   "source": [
    "# testing score\n",
    "accuracy_score(yTe.Result.values, mlp.predict(xTe))"
   ],
   "execution_count": 371,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T09:16:21.852969Z",
     "start_time": "2024-12-25T09:16:19.457513Z"
    }
   },
   "cell_type": "code",
   "source": "xTr",
   "id": "f3b1aae2279970a7",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bc4656a5",
   "metadata": {},
   "source": [
    "### 3.7 Stacked Classifier ###"
   ]
  },
  {
   "cell_type": "code",
   "id": "9a2f0890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:20:39.779723Z",
     "start_time": "2024-12-16T02:20:39.773362Z"
    }
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ],
   "execution_count": 372,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "94e15eaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:31:10.458932Z",
     "start_time": "2024-12-16T02:20:39.779723Z"
    }
   },
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "stacked_clf = StackingClassifier(estimators=[('svm', SVC(max_iter=100000)), ('logistic', LogisticRegression(C=0.01, max_iter=10000))],\n",
    "                                final_estimator=LogisticRegression(max_iter=10000),\n",
    "                                n_jobs=-1).fit(xTr, yTr.values.ravel())"
   ],
   "execution_count": 373,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "b22e6db5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:34:24.125246Z",
     "start_time": "2024-12-16T02:31:10.459949Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(yTr.Result.values, stacked_clf.predict(xTr))"
   ],
   "execution_count": 374,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7a57d164",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:34:35.864400Z",
     "start_time": "2024-12-16T02:34:24.126251Z"
    }
   },
   "source": [
    "# testing score\n",
    "accuracy_score(yTe.Result.values, stacked_clf.predict(xTe))"
   ],
   "execution_count": 375,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e90e8cae",
   "metadata": {},
   "source": [
    "## 4. Result Analysis ##"
   ]
  },
  {
   "cell_type": "code",
   "id": "0d4a4507",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:34:35.875294Z",
     "start_time": "2024-12-16T02:34:35.865411Z"
    }
   },
   "source": [
    "## TODO: breakdown results across divisions and/or teams; i.e., see how model performs individually at subgroups"
   ],
   "execution_count": 376,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a10a069c",
   "metadata": {},
   "source": [
    "## 5. Scrap Code ##"
   ]
  },
  {
   "cell_type": "code",
   "id": "4d631c08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:34:35.925726Z",
     "start_time": "2024-12-16T02:34:35.876298Z"
    }
   },
   "source": [
    "barcelona_df = learning_df[(learning_df['HomeTeam 17'] == 1) | (learning_df['AwayTeam 17'] == 1)]\n",
    "barcelona_df"
   ],
   "execution_count": 377,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "38bb2e00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:34:35.945722Z",
     "start_time": "2024-12-16T02:34:35.926735Z"
    }
   },
   "source": [
    "bxTr = xTr[(xTr['HomeTeam 17'] == 1) | (xTr['AwayTeam 17'] == 1)]\n",
    "bxTe = xTe[(xTe['HomeTeam 17'] == 1) | (xTe['AwayTeam 17'] == 1)]"
   ],
   "execution_count": 378,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "154fa92b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:34:35.959483Z",
     "start_time": "2024-12-16T02:34:35.946728Z"
    }
   },
   "source": [
    "byTr, byTe = yTr.loc[bxTr.index,:], yTe.loc[bxTe.index,:]"
   ],
   "execution_count": 379,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a0cdf35f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:34:35.985403Z",
     "start_time": "2024-12-16T02:34:35.959483Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(byTr, l1_lr.predict(bxTr))"
   ],
   "execution_count": 380,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "b3c76c88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:34:36.002214Z",
     "start_time": "2024-12-16T02:34:35.986409Z"
    }
   },
   "source": [
    "# testing score\n",
    "accuracy_score(byTe, l1_lr.predict(bxTe))"
   ],
   "execution_count": 381,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "6c7dad65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:34:36.026658Z",
     "start_time": "2024-12-16T02:34:36.003221Z"
    }
   },
   "source": [
    "# training score\n",
    "accuracy_score(byTr, l2_lr.predict(bxTr))"
   ],
   "execution_count": 382,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "e393c3c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:34:36.045559Z",
     "start_time": "2024-12-16T02:34:36.028462Z"
    }
   },
   "source": [
    "# testing score\n",
    "accuracy_score(byTe, l2_lr.predict(bxTe))"
   ],
   "execution_count": 383,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Pytorch MLP ##",
   "id": "1a9d44ed88b506d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:34:36.055401Z",
     "start_time": "2024-12-16T02:34:36.046568Z"
    }
   },
   "cell_type": "code",
   "source": "type(xTr)",
   "id": "46c50f7a30542964",
   "execution_count": 384,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:34:36.073320Z",
     "start_time": "2024-12-16T02:34:36.056406Z"
    }
   },
   "cell_type": "code",
   "source": "xTr.shape",
   "id": "3d5d6acf5e103ccf",
   "execution_count": 385,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:34:36.086888Z",
     "start_time": "2024-12-16T02:34:36.074328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ],
   "id": "8974fd74dc8ccc05",
   "execution_count": 386,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:37:51.296168Z",
     "start_time": "2024-12-16T02:34:36.087900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_weights = nn.Parameter(torch.randn(feature_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 应用注意力权重\n",
    "        weights = F.softmax(self.attention_weights, dim=0)\n",
    "        # 加权求和\n",
    "        x = x * weights\n",
    "        return x\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=xTr.shape[1], out_features=512)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=512)\n",
    "        self.attention = Attention(512)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=128)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=128)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=32)\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=32)\n",
    "        self.dropout3 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc4 = nn.Linear(in_features=32, out_features=3)  # 输出层改为3，对应三个类别\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(torch.relu(self.bn1(self.fc1(x))))\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout2(torch.relu(self.bn2(self.fc2(x))))\n",
    "        x = self.dropout3(torch.relu(self.bn3(self.fc3(x))))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# 数据预处理\n",
    "scaler = StandardScaler()\n",
    "xTr_scaled = scaler.fit_transform(xTr)\n",
    "xTr_tensor = torch.tensor(xTr_scaled, dtype=torch.float32).to(device)\n",
    "yTr_tensor = torch.tensor(yTr.values.ravel(), dtype=torch.long).to(device)\n",
    "\n",
    "# 创建数据加载器\n",
    "dataset = TensorDataset(xTr_tensor, yTr_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "# 创建模型实例\n",
    "model = MLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "train_start = time.time()\n",
    "# 训练模型\n",
    "model.train()\n",
    "for epoch in range(500):  # 假设训练200个epoch\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # 清除之前的梯度\n",
    "\n",
    "        outputs = model(inputs)  # 前向传播\n",
    "        loss = criterion(outputs, labels)  # 计算损失\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新参数\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)  # 累计损失\n",
    "        _, predicted = torch.max(outputs.data, 1)  # 获取预测结果\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total * 100  # 计算准确率\n",
    "\n",
    "    # 每个epoch结束后输出\n",
    "    print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "print(f'训练时长： {time.time() - train_start}s')"
   ],
   "id": "b2df02ede7062703",
   "execution_count": 387,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:37:51.465730Z",
     "start_time": "2024-12-16T02:37:51.297174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 假设 xTe 和 yTe 是 pandas DataFrame 或 Series\n",
    "# 数据预处理\n",
    "xTe_scaled = scaler.fit_transform(xTe)  # 使用与训练数据相同的标准化参数\n",
    "xTe_tensor = torch.tensor(xTe_scaled, dtype=torch.float32).to(device)\n",
    "yTe_tensor = torch.tensor(yTe.values.ravel(), dtype=torch.long).to(device)\n",
    "\n",
    "# 创建数据加载器\n",
    "test_dataset = TensorDataset(xTe_tensor, yTe_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 初始化用于计算准确率的变量\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 不计算梯度，因为在评估模式下不需要进行反向传播\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy on test set: {accuracy * 100:.2f}%')"
   ],
   "id": "eaf39c6569e4f89d",
   "execution_count": 388,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Pytorch Transformer ##",
   "id": "cf72af33f38ca812"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T02:37:51.552596Z",
     "start_time": "2024-12-16T02:37:51.466737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_heads, num_layers, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.model_dim = input_dim  # 通常情况下，模型维度与输入维度相同\n",
    "\n",
    "        # Transformer Encoder Layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.model_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=512,  # 前馈网络的维度\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Transformer Encoder\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # 输出层\n",
    "        self.output_layer = nn.Linear(self.model_dim, self.num_classes)\n",
    "\n",
    "        # Batch Normalization\n",
    "        self.bn = nn.BatchNorm1d(self.model_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 增加一个假的序列维度\n",
    "        x = x.unsqueeze(1)\n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Batch Normalization\n",
    "        x = self.bn(x[:, 0, :])  # 取序列的第一个元素进行批量归一化\n",
    "\n",
    "        # 输出层\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# 补充维度\n",
    "n_samples_xTr = xTr.shape[0]\n",
    "n_samples_xTe = xTe.shape[0]\n",
    "for i in range(1, 4):  # 从 1 到 3，因为需要添加三列\n",
    "    xTr[f'pad{i}'] = 0  # 添加填充列，初始化为 0\n",
    "    xTe[f'pad{i}'] = 0  # 添加填充列，初始化为 0\n",
    "\n",
    "# 参数设置\n",
    "input_dim = xTr.shape[1]  # 输入特征的维度\n",
    "num_classes = 3  # 类别数\n",
    "num_heads = 10  # 注意力头的数量\n",
    "num_layers = 3  # Transformer层的数量\n",
    "dropout = 0.8  # Dropout比率\n",
    "\n",
    "# 创建模型\n",
    "model = TransformerModel(input_dim, num_classes, num_heads, num_layers, dropout).to(device)\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 数据加载\n",
    "# 假设 xTr 和 xTe 已经是适当的 torch.Tensor 对象\n",
    "xTr_values = xTr.values.astype(float)\n",
    "xTe_values = xTe.values.astype(float)\n",
    "\n",
    "xTr_tensor = torch.tensor(xTr_values, dtype=torch.float32).to(device)\n",
    "xTe_tensor = torch.tensor(xTe_values, dtype=torch.float32).to(device)\n",
    "yTr_tensor = torch.tensor(yTr.values, dtype=torch.long).to(device).squeeze(1)\n",
    "yTe_tensor = torch.tensor(yTe.values, dtype=torch.long).to(device).squeeze(1)\n",
    "# 转换为 one-hot 编码\n",
    "yTr_tensor = F.one_hot(yTr_tensor, num_classes=num_classes).float()\n",
    "yTe_tensor = F.one_hot(yTe_tensor, num_classes=num_classes).float()\n",
    "\n",
    "# 数据加载器\n",
    "train_dataset = TensorDataset(xTr_tensor, yTr_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(xTe_tensor, yTe_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "train_start = time.time()\n",
    "# 训练模型\n",
    "model.train()\n",
    "for epoch in range(500):  # 运行更多的 epoch 以获得更好的结果\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)  # 累计损失\n",
    "        _, predicted = torch.max(outputs.data, 1)  # 获取预测结果\n",
    "        _, truth = torch.max(labels.data, 1)\n",
    "        total += truth.size(0)\n",
    "        correct += (predicted == truth).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total * 100  # 计算准确率\n",
    "\n",
    "    # 每个epoch结束后输出\n",
    "    print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "print(f'训练时长： {time.time() - train_start}s')\n",
    "\n",
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 初始化用于计算准确率的变量\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 不计算梯度，因为在评估模式下不需要进行反向传播\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, truth = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == truth).sum().item()\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy on test set: {accuracy * 100:.2f}%')"
   ],
   "id": "b49f583ba5a1498",
   "execution_count": 389,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
